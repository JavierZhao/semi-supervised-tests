{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T20:37:32.806526Z",
     "start_time": "2022-09-15T20:37:31.727826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Thu Sep 15 20:37:32 2022\n",
      "Driver Version                            : 510.54\n",
      "CUDA Version                              : 11.6\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:08:00.0\n",
      "    Product Name                          : NVIDIA GeForce GTX 1080\n",
      "    Product Brand                         : GeForce\n",
      "    Product Architecture                  : Pascal\n",
      "    Display Mode                          : Disabled\n",
      "    Display Active                        : Disabled\n",
      "    Persistence Mode                      : Disabled\n",
      "    MIG Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Accounting Mode                       : Disabled\n",
      "    Accounting Mode Buffer Size           : 4000\n",
      "    Driver Model\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Serial Number                         : N/A\n",
      "    GPU UUID                              : GPU-045391d7-2495-c5b2-211c-c060e8255265\n",
      "    Minor Number                          : 2\n",
      "    VBIOS Version                         : 86.04.3B.00.05\n",
      "    MultiGPU Board                        : No\n",
      "    Board ID                              : 0x800\n",
      "    GPU Part Number                       : N/A\n",
      "    Module ID                             : 0\n",
      "    Inforom Version\n",
      "        Image Version                     : G001.0000.01.03\n",
      "        OEM Object                        : 1.1\n",
      "        ECC Object                        : N/A\n",
      "        Power Management Object           : N/A\n",
      "    GPU Operation Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    GSP Firmware Version                  : N/A\n",
      "    GPU Virtualization Mode\n",
      "        Virtualization Mode               : None\n",
      "        Host VGPU Mode                    : N/A\n",
      "    IBMNPU\n",
      "        Relaxed Ordering Mode             : N/A\n",
      "    PCI\n",
      "        Bus                               : 0x08\n",
      "        Device                            : 0x00\n",
      "        Domain                            : 0x0000\n",
      "        Device Id                         : 0x1B8010DE\n",
      "        Bus Id                            : 00000000:08:00.0\n",
      "        Sub System Id                     : 0x119E196E\n",
      "        GPU Link Info\n",
      "            PCIe Generation\n",
      "                Max                       : 3\n",
      "                Current                   : 1\n",
      "            Link Width\n",
      "                Max                       : 16x\n",
      "                Current                   : 16x\n",
      "        Bridge Chip\n",
      "            Type                          : N/A\n",
      "            Firmware                      : N/A\n",
      "        Replays Since Reset               : 0\n",
      "        Replay Number Rollovers           : 0\n",
      "        Tx Throughput                     : 0 KB/s\n",
      "        Rx Throughput                     : 0 KB/s\n",
      "    Fan Speed                             : 27 %\n",
      "    Performance State                     : P8\n",
      "    Clocks Throttle Reasons\n",
      "        Idle                              : Active\n",
      "        Applications Clocks Setting       : Not Active\n",
      "        SW Power Cap                      : Not Active\n",
      "        HW Slowdown                       : Not Active\n",
      "            HW Thermal Slowdown           : Not Active\n",
      "            HW Power Brake Slowdown       : Not Active\n",
      "        Sync Boost                        : Not Active\n",
      "        SW Thermal Slowdown               : Not Active\n",
      "        Display Clock Setting             : Not Active\n",
      "    FB Memory Usage\n",
      "        Total                             : 8192 MiB\n",
      "        Reserved                          : 72 MiB\n",
      "        Used                              : 0 MiB\n",
      "        Free                              : 8119 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 2 MiB\n",
      "        Free                              : 254 MiB\n",
      "    Compute Mode                          : Default\n",
      "    Utilization\n",
      "        Gpu                               : 0 %\n",
      "        Memory                            : 0 %\n",
      "        Encoder                           : 0 %\n",
      "        Decoder                           : 0 %\n",
      "    Encoder Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    FBC Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    Ecc Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    ECC Errors\n",
      "        Volatile\n",
      "            Single Bit            \n",
      "                Device Memory             : N/A\n",
      "                Register File             : N/A\n",
      "                L1 Cache                  : N/A\n",
      "                L2 Cache                  : N/A\n",
      "                Texture Memory            : N/A\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : N/A\n",
      "            Double Bit            \n",
      "                Device Memory             : N/A\n",
      "                Register File             : N/A\n",
      "                L1 Cache                  : N/A\n",
      "                L2 Cache                  : N/A\n",
      "                Texture Memory            : N/A\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : N/A\n",
      "        Aggregate\n",
      "            Single Bit            \n",
      "                Device Memory             : N/A\n",
      "                Register File             : N/A\n",
      "                L1 Cache                  : N/A\n",
      "                L2 Cache                  : N/A\n",
      "                Texture Memory            : N/A\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : N/A\n",
      "            Double Bit            \n",
      "                Device Memory             : N/A\n",
      "                Register File             : N/A\n",
      "                L1 Cache                  : N/A\n",
      "                L2 Cache                  : N/A\n",
      "                Texture Memory            : N/A\n",
      "                Texture Shared            : N/A\n",
      "                CBU                       : N/A\n",
      "                Total                     : N/A\n",
      "    Retired Pages\n",
      "        Single Bit ECC                    : N/A\n",
      "        Double Bit ECC                    : N/A\n",
      "        Pending Page Blacklist            : N/A\n",
      "    Remapped Rows                         : N/A\n",
      "    Temperature\n",
      "        GPU Current Temp                  : 20 C\n",
      "        GPU Shutdown Temp                 : 99 C\n",
      "        GPU Slowdown Temp                 : 96 C\n",
      "        GPU Max Operating Temp            : N/A\n",
      "        GPU Target Temperature            : 83 C\n",
      "        Memory Current Temp               : N/A\n",
      "        Memory Max Operating Temp         : N/A\n",
      "    Power Readings\n",
      "        Power Management                  : Supported\n",
      "        Power Draw                        : 6.68 W\n",
      "        Power Limit                       : 180.00 W\n",
      "        Default Power Limit               : 180.00 W\n",
      "        Enforced Power Limit              : 180.00 W\n",
      "        Min Power Limit                   : 90.00 W\n",
      "        Max Power Limit                   : 217.00 W\n",
      "    Clocks\n",
      "        Graphics                          : 139 MHz\n",
      "        SM                                : 139 MHz\n",
      "        Memory                            : 405 MHz\n",
      "        Video                             : 544 MHz\n",
      "    Applications Clocks\n",
      "        Graphics                          : N/A\n",
      "        Memory                            : N/A\n",
      "    Default Applications Clocks\n",
      "        Graphics                          : N/A\n",
      "        Memory                            : N/A\n",
      "    Max Clocks\n",
      "        Graphics                          : 1911 MHz\n",
      "        SM                                : 1911 MHz\n",
      "        Memory                            : 5005 MHz\n",
      "        Video                             : 1708 MHz\n",
      "    Max Customer Boost Clocks\n",
      "        Graphics                          : N/A\n",
      "    Clock Policy\n",
      "        Auto Boost                        : N/A\n",
      "        Auto Boost Default                : N/A\n",
      "    Voltage\n",
      "        Graphics                          : N/A\n",
      "    Processes                             : None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T20:39:48.191614Z",
     "start_time": "2022-09-15T20:39:35.693730Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1OAfKz8Ys-W6",
    "outputId": "81b6db96-9b92-4941-ff58-35f0be0d8260"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 40 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import logging\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataListLoader\n",
    "from torch_geometric.nn import MaskLabel, TransformerConv\n",
    "from torch_geometric.utils import index_to_mask\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')  #go up one directory\n",
    "from src.data.jetnet_graph import JetNetGraph\n",
    "from src.models.unimp_model import UniMP\n",
    "from custom_libraries.my_functions import *\n",
    "from focal_loss import FocalLoss\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T20:57:46.516202Z",
     "start_time": "2022-09-15T20:57:46.504254Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, label_rate=0.85, loss_fcn=F.cross_entropy):\n",
    "    model.train()\n",
    "\n",
    "    sum_loss = 0\n",
    "    sum_true = 0\n",
    "    sum_all = 0\n",
    "    for i, data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_mask = torch.ones_like(data.x[:, 0], dtype=torch.bool)\n",
    "        propagation_mask = MaskLabel.ratio_mask(train_mask, ratio=label_rate)\n",
    "        supervision_mask = train_mask ^ propagation_mask\n",
    "\n",
    "        data = data.cuda()\n",
    "        out = model(data.x, data.y, data.edge_index, propagation_mask)\n",
    "        loss = loss_fcn(out[supervision_mask], data.y[supervision_mask])\n",
    "        loss.backward()\n",
    "        sum_loss += float(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = out[supervision_mask].argmax(dim=-1)\n",
    "        sum_true += int((pred == data.y[supervision_mask]).sum())\n",
    "        sum_all += pred.size(0)\n",
    "        logging.info(f\"Batch: {i + 1:03d}, Train Loss: {sum_loss:.4f}\")\n",
    "\n",
    "    return float(sum_loss) / (i + 1), float(sum_true) / sum_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T21:14:26.484295Z",
     "start_time": "2022-09-15T21:14:26.469004Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, loader, label_rate=0.85, output_pred=False):\n",
    "    model.eval()\n",
    "\n",
    "    sum_true = 0\n",
    "    sum_all = 0\n",
    "    for data in loader:\n",
    "        data = data.cuda()\n",
    "        test_mask = torch.ones_like(data.x[:, 0], dtype=torch.bool)\n",
    "        propagation_mask = MaskLabel.ratio_mask(test_mask, ratio=label_rate)\n",
    "        supervision_mask = test_mask ^ propagation_mask\n",
    "\n",
    "        out = model(data.x, data.y, data.edge_index, propagation_mask)\n",
    "        pred = out[supervision_mask].argmax(dim=-1)\n",
    "        sum_true += int((pred == data.y[supervision_mask]).sum())\n",
    "        sum_all += pred.size(0)\n",
    "    if output_pred:\n",
    "        return out\n",
    "    else:\n",
    "        return float(sum_true) / sum_all\n",
    "\n",
    "\n",
    "def collate_fn(items):\n",
    "    sum_list = sum(items, [])\n",
    "    return Batch.from_data_list(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T20:49:49.566558Z",
     "start_time": "2022-09-15T20:49:49.530319Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_params(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            print(layer)\n",
    "            layer.reset_parameters()\n",
    "        else:\n",
    "            for sublayer in layer:\n",
    "                if hasattr(sublayer, 'reset_parameters'):\n",
    "                    print(sublayer)\n",
    "                    sublayer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T20:59:02.968080Z",
     "start_time": "2022-09-15T20:59:02.881819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model summary\n",
      "INFO:root:UniMP(\n",
      "  (label_emb): MaskLabel()\n",
      "  (convs): ModuleList(\n",
      "    (0): TransformerConv(3, 32, heads=2)\n",
      "    (1): TransformerConv(64, 32, heads=2)\n",
      "    (2): TransformerConv(64, 5, heads=2)\n",
      "  )\n",
      "  (norms): ModuleList(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# train_root = osp.join(osp.dirname(osp.realpath(__file__)), \"..\", \"..\", \"data\", \"train\")\n",
    "# val_root = osp.join(osp.dirname(osp.realpath(__file__)), \"..\", \"..\", \"data\", \"val\")\n",
    "train_root = \"/ssl-jet-vol/semi-supervised-tests/Jupyter/data/train\"\n",
    "val_root = \"/ssl-jet-vol/semi-supervised-tests/Jupyter/data/val\"\n",
    "max_jets = 1000\n",
    "train_dataset = JetNetGraph(train_root, max_jets=max_jets, file_start=0, file_stop=1)\n",
    "val_dataset = JetNetGraph(val_root, max_jets=max_jets, file_start=1, file_stop=2)\n",
    "batch_size = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "train_loader = DataListLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "train_loader.collate_fn = collate_fn\n",
    "val_loader = DataListLoader(val_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)\n",
    "val_loader.collate_fn = collate_fn\n",
    "\n",
    "model = UniMP(\n",
    "    in_channels=train_dataset.num_features,\n",
    "    num_classes=train_dataset.num_classes,\n",
    "    hidden_channels=64,\n",
    "    num_layers=3,\n",
    "    heads=2,\n",
    ").to(device)\n",
    "\n",
    "logging.info(\"Model summary\")\n",
    "logging.info(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T21:03:04.202172Z",
     "start_time": "2022-09-15T20:59:05.288457Z"
    },
    "id": "c_TB_JD3tH39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch: 001, Train Loss: 1.5593\n",
      "INFO:root:Epoch: 001, Train Loss: 1.5593, Train Acc: 0.3430, Val Acc: 0.4570\n",
      "INFO:root:Batch: 001, Train Loss: 1.4763\n",
      "INFO:root:Epoch: 002, Train Loss: 1.4763, Train Acc: 0.4674, Val Acc: 0.4647\n",
      "INFO:root:Batch: 001, Train Loss: 1.4119\n",
      "INFO:root:Epoch: 003, Train Loss: 1.4119, Train Acc: 0.4634, Val Acc: 0.4580\n",
      "INFO:root:Batch: 001, Train Loss: 1.3561\n",
      "INFO:root:Epoch: 004, Train Loss: 1.3561, Train Acc: 0.4669, Val Acc: 0.4662\n",
      "INFO:root:Batch: 001, Train Loss: 1.3101\n",
      "INFO:root:Epoch: 005, Train Loss: 1.3101, Train Acc: 0.4518, Val Acc: 0.4508\n",
      "INFO:root:Batch: 001, Train Loss: 1.2609\n",
      "INFO:root:Epoch: 006, Train Loss: 1.2609, Train Acc: 0.4579, Val Acc: 0.4585\n",
      "INFO:root:Batch: 001, Train Loss: 1.2199\n",
      "INFO:root:Epoch: 007, Train Loss: 1.2199, Train Acc: 0.4600, Val Acc: 0.4430\n",
      "INFO:root:Batch: 001, Train Loss: 1.1773\n",
      "INFO:root:Epoch: 008, Train Loss: 1.1773, Train Acc: 0.4612, Val Acc: 0.4571\n",
      "INFO:root:Batch: 001, Train Loss: 1.1427\n",
      "INFO:root:Epoch: 009, Train Loss: 1.1427, Train Acc: 0.4566, Val Acc: 0.4589\n",
      "INFO:root:Batch: 001, Train Loss: 1.1019\n",
      "INFO:root:Epoch: 010, Train Loss: 1.1019, Train Acc: 0.4599, Val Acc: 0.4592\n",
      "INFO:root:Batch: 001, Train Loss: 1.0853\n",
      "INFO:root:Epoch: 011, Train Loss: 1.0853, Train Acc: 0.4779, Val Acc: 0.4579\n",
      "INFO:root:Batch: 001, Train Loss: 1.0731\n",
      "INFO:root:Epoch: 012, Train Loss: 1.0731, Train Acc: 0.5006, Val Acc: 0.4686\n",
      "INFO:root:Batch: 001, Train Loss: 1.0526\n",
      "INFO:root:Epoch: 013, Train Loss: 1.0526, Train Acc: 0.5223, Val Acc: 0.5116\n",
      "INFO:root:Batch: 001, Train Loss: 1.0261\n",
      "INFO:root:Epoch: 014, Train Loss: 1.0261, Train Acc: 0.5566, Val Acc: 0.5227\n",
      "INFO:root:Batch: 001, Train Loss: 1.0216\n",
      "INFO:root:Epoch: 015, Train Loss: 1.0216, Train Acc: 0.5403, Val Acc: 0.5351\n",
      "INFO:root:Batch: 001, Train Loss: 1.0070\n",
      "INFO:root:Epoch: 016, Train Loss: 1.0070, Train Acc: 0.5363, Val Acc: 0.5374\n",
      "INFO:root:Batch: 001, Train Loss: 0.9907\n",
      "INFO:root:Epoch: 017, Train Loss: 0.9907, Train Acc: 0.5408, Val Acc: 0.5359\n",
      "INFO:root:Batch: 001, Train Loss: 0.9812\n",
      "INFO:root:Epoch: 018, Train Loss: 0.9812, Train Acc: 0.5492, Val Acc: 0.5360\n",
      "INFO:root:Batch: 001, Train Loss: 0.9750\n",
      "INFO:root:Epoch: 019, Train Loss: 0.9750, Train Acc: 0.5418, Val Acc: 0.5436\n",
      "INFO:root:Batch: 001, Train Loss: 0.9638\n",
      "INFO:root:Epoch: 020, Train Loss: 0.9638, Train Acc: 0.5378, Val Acc: 0.5485\n",
      "INFO:root:Batch: 001, Train Loss: 0.9526\n",
      "INFO:root:Epoch: 021, Train Loss: 0.9526, Train Acc: 0.5504, Val Acc: 0.5448\n",
      "INFO:root:Batch: 001, Train Loss: 0.9565\n",
      "INFO:root:Epoch: 022, Train Loss: 0.9565, Train Acc: 0.5454, Val Acc: 0.5450\n",
      "INFO:root:Batch: 001, Train Loss: 0.9500\n",
      "INFO:root:Epoch: 023, Train Loss: 0.9500, Train Acc: 0.5489, Val Acc: 0.5542\n",
      "INFO:root:Batch: 001, Train Loss: 0.9344\n",
      "INFO:root:Epoch: 024, Train Loss: 0.9344, Train Acc: 0.5554, Val Acc: 0.5527\n",
      "INFO:root:Batch: 001, Train Loss: 0.9436\n",
      "INFO:root:Epoch: 025, Train Loss: 0.9436, Train Acc: 0.5594, Val Acc: 0.5608\n",
      "INFO:root:Batch: 001, Train Loss: 0.9315\n",
      "INFO:root:Epoch: 026, Train Loss: 0.9315, Train Acc: 0.5566, Val Acc: 0.5686\n",
      "INFO:root:Batch: 001, Train Loss: 0.9310\n",
      "INFO:root:Epoch: 027, Train Loss: 0.9310, Train Acc: 0.5466, Val Acc: 0.5786\n",
      "INFO:root:Batch: 001, Train Loss: 0.9319\n",
      "INFO:root:Epoch: 028, Train Loss: 0.9319, Train Acc: 0.5546, Val Acc: 0.5857\n",
      "INFO:root:Batch: 001, Train Loss: 0.9352\n",
      "INFO:root:Epoch: 029, Train Loss: 0.9352, Train Acc: 0.5571, Val Acc: 0.5768\n",
      "INFO:root:Batch: 001, Train Loss: 0.9313\n",
      "INFO:root:Epoch: 030, Train Loss: 0.9313, Train Acc: 0.5731, Val Acc: 0.5828\n",
      "INFO:root:Batch: 001, Train Loss: 0.9179\n",
      "INFO:root:Epoch: 031, Train Loss: 0.9179, Train Acc: 0.5788, Val Acc: 0.5796\n",
      "INFO:root:Batch: 001, Train Loss: 0.9349\n",
      "INFO:root:Epoch: 032, Train Loss: 0.9349, Train Acc: 0.5626, Val Acc: 0.5854\n",
      "INFO:root:Batch: 001, Train Loss: 0.9104\n",
      "INFO:root:Epoch: 033, Train Loss: 0.9104, Train Acc: 0.5729, Val Acc: 0.5802\n",
      "INFO:root:Batch: 001, Train Loss: 0.9036\n",
      "INFO:root:Epoch: 034, Train Loss: 0.9036, Train Acc: 0.5689, Val Acc: 0.5877\n",
      "INFO:root:Batch: 001, Train Loss: 0.9155\n",
      "INFO:root:Epoch: 035, Train Loss: 0.9155, Train Acc: 0.5688, Val Acc: 0.5864\n",
      "INFO:root:Batch: 001, Train Loss: 0.9153\n",
      "INFO:root:Epoch: 036, Train Loss: 0.9153, Train Acc: 0.5783, Val Acc: 0.5737\n",
      "INFO:root:Batch: 001, Train Loss: 0.9026\n",
      "INFO:root:Epoch: 037, Train Loss: 0.9026, Train Acc: 0.5661, Val Acc: 0.5745\n",
      "INFO:root:Batch: 001, Train Loss: 0.9111\n",
      "INFO:root:Epoch: 038, Train Loss: 0.9111, Train Acc: 0.5625, Val Acc: 0.5823\n",
      "INFO:root:Batch: 001, Train Loss: 0.9140\n",
      "INFO:root:Epoch: 039, Train Loss: 0.9140, Train Acc: 0.5761, Val Acc: 0.5743\n",
      "INFO:root:Batch: 001, Train Loss: 0.9046\n",
      "INFO:root:Epoch: 040, Train Loss: 0.9046, Train Acc: 0.5770, Val Acc: 0.5821\n",
      "INFO:root:Batch: 001, Train Loss: 0.8955\n",
      "INFO:root:Epoch: 041, Train Loss: 0.8955, Train Acc: 0.5693, Val Acc: 0.5801\n",
      "INFO:root:Batch: 001, Train Loss: 0.8963\n",
      "INFO:root:Epoch: 042, Train Loss: 0.8963, Train Acc: 0.5729, Val Acc: 0.5932\n",
      "INFO:root:Batch: 001, Train Loss: 0.8736\n",
      "INFO:root:Epoch: 043, Train Loss: 0.8736, Train Acc: 0.5926, Val Acc: 0.5754\n",
      "INFO:root:Batch: 001, Train Loss: 0.9000\n",
      "INFO:root:Epoch: 044, Train Loss: 0.9000, Train Acc: 0.5704, Val Acc: 0.5713\n",
      "INFO:root:Batch: 001, Train Loss: 0.8849\n",
      "INFO:root:Epoch: 045, Train Loss: 0.8849, Train Acc: 0.5790, Val Acc: 0.5767\n",
      "INFO:root:Batch: 001, Train Loss: 0.8913\n",
      "INFO:root:Epoch: 046, Train Loss: 0.8913, Train Acc: 0.5853, Val Acc: 0.5938\n",
      "INFO:root:Batch: 001, Train Loss: 0.8834\n",
      "INFO:root:Epoch: 047, Train Loss: 0.8834, Train Acc: 0.5832, Val Acc: 0.5795\n",
      "INFO:root:Batch: 001, Train Loss: 0.8883\n",
      "INFO:root:Epoch: 048, Train Loss: 0.8883, Train Acc: 0.5819, Val Acc: 0.5753\n",
      "INFO:root:Batch: 001, Train Loss: 0.8815\n",
      "INFO:root:Epoch: 049, Train Loss: 0.8815, Train Acc: 0.5818, Val Acc: 0.5696\n",
      "INFO:root:Batch: 001, Train Loss: 0.8799\n",
      "INFO:root:Epoch: 050, Train Loss: 0.8799, Train Acc: 0.5796, Val Acc: 0.5732\n",
      "INFO:root:Batch: 001, Train Loss: 0.8802\n",
      "INFO:root:Epoch: 051, Train Loss: 0.8802, Train Acc: 0.5797, Val Acc: 0.5722\n",
      "INFO:root:Batch: 001, Train Loss: 0.8859\n",
      "INFO:root:Epoch: 052, Train Loss: 0.8859, Train Acc: 0.5787, Val Acc: 0.5951\n",
      "INFO:root:Batch: 001, Train Loss: 0.8873\n",
      "INFO:root:Epoch: 053, Train Loss: 0.8873, Train Acc: 0.5703, Val Acc: 0.5733\n",
      "INFO:root:Batch: 001, Train Loss: 0.8707\n",
      "INFO:root:Epoch: 054, Train Loss: 0.8707, Train Acc: 0.5852, Val Acc: 0.5681\n",
      "INFO:root:Batch: 001, Train Loss: 0.8749\n",
      "INFO:root:Epoch: 055, Train Loss: 0.8749, Train Acc: 0.5847, Val Acc: 0.5732\n",
      "INFO:root:Batch: 001, Train Loss: 0.8744\n",
      "INFO:root:Epoch: 056, Train Loss: 0.8744, Train Acc: 0.5782, Val Acc: 0.5830\n",
      "INFO:root:Batch: 001, Train Loss: 0.8696\n",
      "INFO:root:Epoch: 057, Train Loss: 0.8696, Train Acc: 0.5861, Val Acc: 0.5750\n",
      "INFO:root:Batch: 001, Train Loss: 0.8848\n",
      "INFO:root:Epoch: 058, Train Loss: 0.8848, Train Acc: 0.5809, Val Acc: 0.5810\n",
      "INFO:root:Batch: 001, Train Loss: 0.8748\n",
      "INFO:root:Epoch: 059, Train Loss: 0.8748, Train Acc: 0.5713, Val Acc: 0.5819\n",
      "INFO:root:Batch: 001, Train Loss: 0.8756\n",
      "INFO:root:Epoch: 060, Train Loss: 0.8756, Train Acc: 0.5809, Val Acc: 0.5781\n",
      "INFO:root:Batch: 001, Train Loss: 0.8787\n",
      "INFO:root:Epoch: 061, Train Loss: 0.8787, Train Acc: 0.5724, Val Acc: 0.5761\n",
      "INFO:root:Batch: 001, Train Loss: 0.8699\n",
      "INFO:root:Epoch: 062, Train Loss: 0.8699, Train Acc: 0.5827, Val Acc: 0.5830\n",
      "INFO:root:Batch: 001, Train Loss: 0.8705\n",
      "INFO:root:Epoch: 063, Train Loss: 0.8705, Train Acc: 0.5877, Val Acc: 0.5688\n",
      "INFO:root:Batch: 001, Train Loss: 0.8631\n",
      "INFO:root:Epoch: 064, Train Loss: 0.8631, Train Acc: 0.5864, Val Acc: 0.5735\n",
      "INFO:root:Batch: 001, Train Loss: 0.8620\n",
      "INFO:root:Epoch: 065, Train Loss: 0.8620, Train Acc: 0.5790, Val Acc: 0.5748\n",
      "INFO:root:Batch: 001, Train Loss: 0.8733\n",
      "INFO:root:Epoch: 066, Train Loss: 0.8733, Train Acc: 0.5847, Val Acc: 0.5921\n",
      "INFO:root:Batch: 001, Train Loss: 0.8604\n",
      "INFO:root:Epoch: 067, Train Loss: 0.8604, Train Acc: 0.5924, Val Acc: 0.5744\n",
      "INFO:root:Batch: 001, Train Loss: 0.8655\n",
      "INFO:root:Epoch: 068, Train Loss: 0.8655, Train Acc: 0.5811, Val Acc: 0.5817\n",
      "INFO:root:Batch: 001, Train Loss: 0.8676\n",
      "INFO:root:Epoch: 069, Train Loss: 0.8676, Train Acc: 0.5756, Val Acc: 0.5893\n",
      "INFO:root:Batch: 001, Train Loss: 0.8674\n",
      "INFO:root:Epoch: 070, Train Loss: 0.8674, Train Acc: 0.5795, Val Acc: 0.5681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch: 001, Train Loss: 0.8624\n",
      "INFO:root:Epoch: 071, Train Loss: 0.8624, Train Acc: 0.5931, Val Acc: 0.5721\n",
      "INFO:root:Batch: 001, Train Loss: 0.8581\n",
      "INFO:root:Epoch: 072, Train Loss: 0.8581, Train Acc: 0.5851, Val Acc: 0.5773\n",
      "INFO:root:Batch: 001, Train Loss: 0.8592\n",
      "INFO:root:Epoch: 073, Train Loss: 0.8592, Train Acc: 0.5816, Val Acc: 0.5796\n",
      "INFO:root:Batch: 001, Train Loss: 0.8702\n",
      "INFO:root:Epoch: 074, Train Loss: 0.8702, Train Acc: 0.5734, Val Acc: 0.5828\n",
      "INFO:root:Batch: 001, Train Loss: 0.8685\n",
      "INFO:root:Epoch: 075, Train Loss: 0.8685, Train Acc: 0.5746, Val Acc: 0.5751\n",
      "INFO:root:Batch: 001, Train Loss: 0.8757\n",
      "INFO:root:Epoch: 076, Train Loss: 0.8757, Train Acc: 0.5754, Val Acc: 0.5796\n",
      "INFO:root:Batch: 001, Train Loss: 0.8533\n",
      "INFO:root:Epoch: 077, Train Loss: 0.8533, Train Acc: 0.5844, Val Acc: 0.5796\n",
      "INFO:root:Batch: 001, Train Loss: 0.8638\n",
      "INFO:root:Epoch: 078, Train Loss: 0.8638, Train Acc: 0.5847, Val Acc: 0.5845\n",
      "INFO:root:Batch: 001, Train Loss: 0.8712\n",
      "INFO:root:Epoch: 079, Train Loss: 0.8712, Train Acc: 0.5808, Val Acc: 0.5813\n",
      "INFO:root:Batch: 001, Train Loss: 0.8612\n",
      "INFO:root:Epoch: 080, Train Loss: 0.8612, Train Acc: 0.5765, Val Acc: 0.5795\n",
      "INFO:root:Batch: 001, Train Loss: 0.8587\n",
      "INFO:root:Epoch: 081, Train Loss: 0.8587, Train Acc: 0.5757, Val Acc: 0.5734\n",
      "INFO:root:Batch: 001, Train Loss: 0.8563\n",
      "INFO:root:Epoch: 082, Train Loss: 0.8563, Train Acc: 0.5881, Val Acc: 0.5830\n",
      "INFO:root:Batch: 001, Train Loss: 0.8733\n",
      "INFO:root:Epoch: 083, Train Loss: 0.8733, Train Acc: 0.5757, Val Acc: 0.5842\n",
      "INFO:root:Batch: 001, Train Loss: 0.8617\n",
      "INFO:root:Epoch: 084, Train Loss: 0.8617, Train Acc: 0.5919, Val Acc: 0.5870\n",
      "INFO:root:Batch: 001, Train Loss: 0.8613\n",
      "INFO:root:Epoch: 085, Train Loss: 0.8613, Train Acc: 0.5714, Val Acc: 0.5788\n",
      "INFO:root:Batch: 001, Train Loss: 0.8585\n",
      "INFO:root:Epoch: 086, Train Loss: 0.8585, Train Acc: 0.5917, Val Acc: 0.5899\n",
      "INFO:root:Batch: 001, Train Loss: 0.8456\n",
      "INFO:root:Epoch: 087, Train Loss: 0.8456, Train Acc: 0.5870, Val Acc: 0.5874\n",
      "INFO:root:Batch: 001, Train Loss: 0.8585\n",
      "INFO:root:Epoch: 088, Train Loss: 0.8585, Train Acc: 0.5821, Val Acc: 0.5764\n",
      "INFO:root:Batch: 001, Train Loss: 0.8513\n",
      "INFO:root:Epoch: 089, Train Loss: 0.8513, Train Acc: 0.5824, Val Acc: 0.5813\n",
      "INFO:root:Batch: 001, Train Loss: 0.8556\n",
      "INFO:root:Epoch: 090, Train Loss: 0.8556, Train Acc: 0.5797, Val Acc: 0.5824\n",
      "INFO:root:Batch: 001, Train Loss: 0.8519\n",
      "INFO:root:Epoch: 091, Train Loss: 0.8519, Train Acc: 0.5814, Val Acc: 0.5868\n",
      "INFO:root:Batch: 001, Train Loss: 0.8665\n",
      "INFO:root:Epoch: 092, Train Loss: 0.8665, Train Acc: 0.5818, Val Acc: 0.5935\n",
      "INFO:root:Batch: 001, Train Loss: 0.8563\n",
      "INFO:root:Epoch: 093, Train Loss: 0.8563, Train Acc: 0.5810, Val Acc: 0.5850\n",
      "INFO:root:Batch: 001, Train Loss: 0.8566\n",
      "INFO:root:Epoch: 094, Train Loss: 0.8566, Train Acc: 0.5884, Val Acc: 0.5861\n",
      "INFO:root:Batch: 001, Train Loss: 0.8511\n",
      "INFO:root:Epoch: 095, Train Loss: 0.8511, Train Acc: 0.5846, Val Acc: 0.5845\n",
      "INFO:root:Batch: 001, Train Loss: 0.8551\n",
      "INFO:root:Epoch: 096, Train Loss: 0.8551, Train Acc: 0.5849, Val Acc: 0.5841\n",
      "INFO:root:Batch: 001, Train Loss: 0.8573\n",
      "INFO:root:Epoch: 097, Train Loss: 0.8573, Train Acc: 0.5791, Val Acc: 0.5854\n",
      "INFO:root:Batch: 001, Train Loss: 0.8534\n",
      "INFO:root:Epoch: 098, Train Loss: 0.8534, Train Acc: 0.5881, Val Acc: 0.5846\n",
      "INFO:root:Batch: 001, Train Loss: 0.8715\n",
      "INFO:root:Epoch: 099, Train Loss: 0.8715, Train Acc: 0.5768, Val Acc: 0.5867\n",
      "INFO:root:Batch: 001, Train Loss: 0.8535\n",
      "INFO:root:Epoch: 100, Train Loss: 0.8535, Train Acc: 0.5860, Val Acc: 0.5948\n"
     ]
    }
   ],
   "source": [
    "reset_params(model)\n",
    "num_epochs = 100\n",
    "train_loss_lst, val_acc_lst, train_acc_lst = [], [], []\n",
    "\n",
    "# Set up Focal loss\n",
    "gamma = 1\n",
    "floss = FocalLoss(gamma=gamma, reduction='mean')\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, loss_fcn=floss)\n",
    "    val_acc = test(model, val_loader)\n",
    "    logging.info(f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    train_loss_lst.append(train_loss)\n",
    "    train_acc_lst.append(train_acc)\n",
    "    val_acc_lst.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: why is this constant for any max_jets\n",
    "Need to delete the .pt files in the \"processed\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T21:09:33.013383Z",
     "start_time": "2022-09-15T21:09:32.574265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "marker_size = 3\n",
    "plt.plot(train_loss_lst, \"r.\", markersize=marker_size, label=\"training loss\")\n",
    "plt.plot(train_acc_lst, \"b.\", markersize=marker_size, label=\"training acc\")\n",
    "plt.plot(val_acc_lst, \"g.\",markersize=marker_size, label=\"val acc\")\n",
    "plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.ylabel(\"value of params\", rotation=90, fontsize=18)\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "# plt.xlim([200, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T21:15:46.112038Z",
     "start_time": "2022-09-15T21:15:44.997489Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 518.00 MiB (GPU 0; 7.93 GiB total capacity; 6.98 GiB already allocated; 226.50 MiB free; 7.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pred \u001b[38;5;241m=\u001b[39m out[supervision_mask]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((pred \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my[supervision_mask])\u001b[38;5;241m.\u001b[39msum()) \u001b[38;5;241m/\u001b[39m pred\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader, label_rate, output_pred)\u001b[0m\n\u001b[1;32m     10\u001b[0m propagation_mask \u001b[38;5;241m=\u001b[39m MaskLabel\u001b[38;5;241m.\u001b[39mratio_mask(test_mask, ratio\u001b[38;5;241m=\u001b[39mlabel_rate)\n\u001b[1;32m     11\u001b[0m supervision_mask \u001b[38;5;241m=\u001b[39m test_mask \u001b[38;5;241m^\u001b[39m propagation_mask\n\u001b[0;32m---> 13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpropagation_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m pred \u001b[38;5;241m=\u001b[39m out[supervision_mask]\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m sum_true \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((pred \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my[supervision_mask])\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/ssl-jet-vol/semi-supervised-tests/Jupyter/../src/models/unimp_model.py:45\u001b[0m, in \u001b[0;36mUniMP.forward\u001b[0;34m(self, x, y, edge_index, label_mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_emb(x, y, label_mask)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv, norm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms):\n\u001b[0;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m norm(\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x, edge_index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch_geometric/nn/conv/transformer_conv.py:176\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    173\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_value(x[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# propagate_type: (query: Tensor, key:Tensor, value: Tensor, edge_attr: OptTensor) # noqa\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                     \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:368\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[1;32m    366\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 368\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__collect__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__user_args__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mdistribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:262\u001b[0m, in \u001b[0;36mMessagePassing.__collect__\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_size__(size, dim, data)\n\u001b[0;32m--> 262\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__lift__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m         out[arg] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, Tensor):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:232\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, Tensor):\n\u001b[1;32m    231\u001b[0m     index \u001b[38;5;241m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, SparseTensor):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 518.00 MiB (GPU 0; 7.93 GiB total capacity; 6.98 GiB already allocated; 226.50 MiB free; 7.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "out = test(model, val_loader, output_pred=True)\n",
    "pred = out[supervision_mask].argmax(dim=-1)\n",
    "test_acc = int((pred == data.y[supervision_mask]).sum()) / pred.size(0)\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "out_norm = m(out)  # the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.092163Z",
     "start_time": "2022-09-07T22:13:13.092156Z"
    }
   },
   "outputs": [],
   "source": [
    "0 in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.092709Z",
     "start_time": "2022-09-07T22:13:13.092701Z"
    }
   },
   "outputs": [],
   "source": [
    "1 in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.093202Z",
     "start_time": "2022-09-07T22:13:13.093195Z"
    }
   },
   "outputs": [],
   "source": [
    "2 in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.093804Z",
     "start_time": "2022-09-07T22:13:13.093796Z"
    }
   },
   "outputs": [],
   "source": [
    "3 in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.094339Z",
     "start_time": "2022-09-07T22:13:13.094332Z"
    }
   },
   "outputs": [],
   "source": [
    "4 in pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 cells above shows that the model ONLY predicted classes 2 (photon) and classes 3 (charged_hadron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot class balance\n",
    "PDG_CLASSES = [\"electron\", \"muon\", \"photon\", \"charged_hadron\", \"neutral_hadron\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.095004Z",
     "start_time": "2022-09-07T22:13:13.094997Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = np.array([i for i in range(5)])\n",
    "PDG_CLASSES = [\"electron\", \"muon\", \"photon\", \"charged_hadron\", \"neutral_hadron\"]\n",
    "labels_training = data.y[train_mask].cpu().numpy()\n",
    "class_dict = plot_class_balance(classes, labels_training, PDG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.095902Z",
     "start_time": "2022-09-07T22:13:13.095894Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(list(class_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.096473Z",
     "start_time": "2022-09-07T22:13:13.096465Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_testing = data.y[test_mask].cpu().numpy()\n",
    "plot_class_balance_and_accuracy(class_dict, labels_testing, PDG_CLASSES, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve OvR for Muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.097021Z",
     "start_time": "2022-09-07T22:13:13.097014Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = [1]\n",
    "class_labels = [\"muon\"]\n",
    "roc_auc_ovr = plot_overlayed_roc_curve(classes, data.y[test_mask], out_norm[test_mask][:, classes], class_labels, ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.097613Z",
     "start_time": "2022-09-07T22:13:13.097606Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = np.array([i for i in range(5)])\n",
    "roc_auc_ovr = plot_overlayed_roc_curve(classes, data.y[test_mask], out_norm[test_mask][:, classes], PDG_CLASSES, ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.098147Z",
     "start_time": "2022-09-07T22:13:13.098141Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = [2, 3]\n",
    "class_labels = [\"photon\", \"charged_hadron\"]\n",
    "plot_overlayed_roc_curve(classes, data.y[test_mask], out_norm[test_mask][:, classes], class_labels, ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.098906Z",
     "start_time": "2022-09-07T22:13:13.098899Z"
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from scipy import stats\n",
    "# def plot_roc_curve(tpr, fpr, scatter = True, ax = None):\n",
    "#     '''\n",
    "#     Plots the ROC Curve by using the list of coordinates (tpr and fpr).\n",
    "    \n",
    "#     Args:\n",
    "#         tpr: The list of TPRs representing each coordinate.\n",
    "#         fpr: The list of FPRs representing each coordinate.\n",
    "#         scatter: When True, the points used on the calculation will be plotted with the line (default = True).\n",
    "#     '''\n",
    "#     if ax == None:\n",
    "#         plt.figure(figsize = (5, 5))\n",
    "#         ax = plt.axes()\n",
    "    \n",
    "#     if scatter:\n",
    "#         sns.scatterplot(x = fpr, y = tpr, ax = ax)\n",
    "#     sns.lineplot(x = fpr, y = tpr, ax = ax)\n",
    "#     sns.lineplot(x = [0, 1], y = [0, 1], color = 'green', ax = ax)\n",
    "#     plt.xlim(-0.05, 1.05)\n",
    "#     plt.ylim(-0.05, 1.05)\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.ylabel(\"True Positive Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.099483Z",
     "start_time": "2022-09-07T22:13:13.099475Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Plots the ROC Curves One vs Rest\n",
    "# plt.figure(figsize = (12, 3))\n",
    "# roc_auc_ovr = {}\n",
    "# predictions = out_norm[test_mask]\n",
    "# for i in range(len(classes)):  #for each of the classes\n",
    "#     # Gets the class\n",
    "#     c = classes[i]\n",
    "#     y_real = [1 if y == c else 0 for y in labels_testing]\n",
    "#     y_proba = predictions[:, i]\n",
    "#     tpr, fpr = get_all_roc_coordinates(y_real, y_proba.cpu())\n",
    "    \n",
    "#     # Calculates the ROC Coordinates and plots the ROC Curves\n",
    "#     ax_bottom = plt.subplot(1, 5, i+1)\n",
    "#     plot_roc_curve(tpr, fpr, scatter = False, ax = ax_bottom)\n",
    "#     ax_bottom.set_title(f\"ROC OvR {PDG_CLASSES[i]}\")\n",
    "    \n",
    "#     # Calculates the ROC AUC OvR\n",
    "#     roc_auc_ovr[c] = roc_auc_score(y_real, y_proba.detach().cpu())\n",
    "    \n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T22:13:13.099966Z",
     "start_time": "2022-09-07T22:13:13.099959Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_class_balance_and_AUC(class_dict, roc_auc_ovr, PDG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ssl-tests train_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

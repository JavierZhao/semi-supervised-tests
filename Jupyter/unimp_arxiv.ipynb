{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-23T17:46:34.172793Z",
     "start_time": "2022-08-23T17:46:33.542466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Tue Aug 23 17:46:33 2022\n",
      "Driver Version                            : 510.47.03\n",
      "CUDA Version                              : 11.6\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:A1:00.0\n",
      "    Product Name                          : NVIDIA A100-PCIE-40GB\n",
      "    Product Brand                         : NVIDIA\n",
      "    Product Architecture                  : Ampere\n",
      "    Display Mode                          : Enabled\n",
      "    Display Active                        : Disabled\n",
      "    Persistence Mode                      : Disabled\n",
      "    MIG Mode\n",
      "        Current                           : Enabled\n",
      "        Pending                           : Enabled\n",
      "    MIG Device\n",
      "        Index                             : 0\n",
      "        GPU Instance ID                   : 6\n",
      "        Compute Instance ID               : 0\n",
      "        Device Attributes\n",
      "            Shared\n",
      "                Multiprocessor count      : 28\n",
      "                Copy Engine count         : 2\n",
      "                Encoder count             : 0\n",
      "                Decoder count             : 1\n",
      "                OFA count                 : 0\n",
      "                JPG count                 : 0\n",
      "        ECC Errors\n",
      "            Volatile\n",
      "                SRAM Uncorrectable        : 0\n",
      "        FB Memory Usage\n",
      "            Total                         : 9856 MiB\n",
      "            Reserved                      : 0 MiB\n",
      "            Used                          : 9290 MiB\n",
      "            Free                          : 565 MiB\n",
      "        BAR1 Memory\n",
      "            Total                         : 16383 MiB\n",
      "            Used                          : 2 MiB\n",
      "            Free                          : 16381 MiB\n",
      "    Accounting Mode                       : Disabled\n",
      "    Accounting Mode Buffer Size           : 4000\n",
      "    Driver Model\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    Serial Number                         : 1323920032718\n",
      "    GPU UUID                              : GPU-c3a2513b-014b-e0f9-927d-c40d97e859aa\n",
      "    Minor Number                          : 1\n",
      "    VBIOS Version                         : 92.00.25.00.08\n",
      "    MultiGPU Board                        : No\n",
      "    Board ID                              : 0xa100\n",
      "    GPU Part Number                       : 900-21001-0000-000\n",
      "    Module ID                             : 0\n",
      "    Inforom Version\n",
      "        Image Version                     : 1001.0200.00.04\n",
      "        OEM Object                        : 2.0\n",
      "        ECC Object                        : 6.16\n",
      "        Power Management Object           : N/A\n",
      "    GPU Operation Mode\n",
      "        Current                           : N/A\n",
      "        Pending                           : N/A\n",
      "    GSP Firmware Version                  : 510.47.03\n",
      "    GPU Virtualization Mode\n",
      "        Virtualization Mode               : None\n",
      "        Host VGPU Mode                    : N/A\n",
      "    IBMNPU\n",
      "        Relaxed Ordering Mode             : N/A\n",
      "    PCI\n",
      "        Bus                               : 0xA1\n",
      "        Device                            : 0x00\n",
      "        Domain                            : 0x0000\n",
      "        Device Id                         : 0x20F110DE\n",
      "        Bus Id                            : 00000000:A1:00.0\n",
      "        Sub System Id                     : 0x145F10DE\n",
      "        GPU Link Info\n",
      "            PCIe Generation\n",
      "                Max                       : 4\n",
      "                Current                   : 4\n",
      "            Link Width\n",
      "                Max                       : 16x\n",
      "                Current                   : 16x\n",
      "        Bridge Chip\n",
      "            Type                          : N/A\n",
      "            Firmware                      : N/A\n",
      "        Replays Since Reset               : 0\n",
      "        Replay Number Rollovers           : 0\n",
      "        Tx Throughput                     : 2000 KB/s\n",
      "        Rx Throughput                     : 0 KB/s\n",
      "    Fan Speed                             : N/A\n",
      "    Performance State                     : P0\n",
      "    Clocks Throttle Reasons\n",
      "        Idle                              : Not Active\n",
      "        Applications Clocks Setting       : Not Active\n",
      "        SW Power Cap                      : Not Active\n",
      "        HW Slowdown                       : Not Active\n",
      "            HW Thermal Slowdown           : Not Active\n",
      "            HW Power Brake Slowdown       : Not Active\n",
      "        Sync Boost                        : Not Active\n",
      "        SW Thermal Slowdown               : Not Active\n",
      "        Display Clock Setting             : Not Active\n",
      "    FB Memory Usage\n",
      "        Total                             : Insufficient Permissions\n",
      "        Reserved                          : Insufficient Permissions\n",
      "        Used                              : Insufficient Permissions\n",
      "        Free                              : Insufficient Permissions\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : Insufficient Permissions\n",
      "        Used                              : Insufficient Permissions\n",
      "        Free                              : Insufficient Permissions\n",
      "    Compute Mode                          : Default\n",
      "    Utilization\n",
      "        Gpu                               : N/A\n",
      "        Memory                            : N/A\n",
      "        Encoder                           : N/A\n",
      "        Decoder                           : N/A\n",
      "    Encoder Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    FBC Stats\n",
      "        Active Sessions                   : 0\n",
      "        Average FPS                       : 0\n",
      "        Average Latency                   : 0\n",
      "    Ecc Mode\n",
      "        Current                           : Enabled\n",
      "        Pending                           : Enabled\n",
      "    ECC Errors\n",
      "        Volatile\n",
      "            SRAM Correctable              : N/A\n",
      "            SRAM Uncorrectable            : N/A\n",
      "            DRAM Correctable              : N/A\n",
      "            DRAM Uncorrectable            : N/A\n",
      "        Aggregate\n",
      "            SRAM Correctable              : 0\n",
      "            SRAM Uncorrectable            : 0\n",
      "            DRAM Correctable              : 0\n",
      "            DRAM Uncorrectable            : 0\n",
      "    Retired Pages\n",
      "        Single Bit ECC                    : N/A\n",
      "        Double Bit ECC                    : N/A\n",
      "        Pending Page Blacklist            : N/A\n",
      "    Remapped Rows                         : N/A\n",
      "    Temperature\n",
      "        GPU Current Temp                  : 24 C\n",
      "        GPU Shutdown Temp                 : 95 C\n",
      "        GPU Slowdown Temp                 : 92 C\n",
      "        GPU Max Operating Temp            : 85 C\n",
      "        GPU Target Temperature            : N/A\n",
      "        Memory Current Temp               : 24 C\n",
      "        Memory Max Operating Temp         : 95 C\n",
      "    Power Readings\n",
      "        Power Management                  : Supported\n",
      "        Power Draw                        : 36.88 W\n",
      "        Power Limit                       : 250.00 W\n",
      "        Default Power Limit               : 250.00 W\n",
      "        Enforced Power Limit              : 250.00 W\n",
      "        Min Power Limit                   : 150.00 W\n",
      "        Max Power Limit                   : 250.00 W\n",
      "    Clocks\n",
      "        Graphics                          : 765 MHz\n",
      "        SM                                : 765 MHz\n",
      "        Memory                            : 1215 MHz\n",
      "        Video                             : 720 MHz\n",
      "    Applications Clocks\n",
      "        Graphics                          : 765 MHz\n",
      "        Memory                            : 1215 MHz\n",
      "    Default Applications Clocks\n",
      "        Graphics                          : 765 MHz\n",
      "        Memory                            : 1215 MHz\n",
      "    Max Clocks\n",
      "        Graphics                          : 1410 MHz\n",
      "        SM                                : 1410 MHz\n",
      "        Memory                            : 1215 MHz\n",
      "        Video                             : 1290 MHz\n",
      "    Max Customer Boost Clocks\n",
      "        Graphics                          : 1410 MHz\n",
      "    Clock Policy\n",
      "        Auto Boost                        : N/A\n",
      "        Auto Boost Default                : N/A\n",
      "    Voltage\n",
      "        Graphics                          : 712.500 mV\n",
      "    Processes\n",
      "        GPU instance ID                   : 6\n",
      "        Compute instance ID               : 0\n",
      "        Process ID                        : 2195956\n",
      "            Type                          : C\n",
      "            Name                          : \n",
      "            Used GPU Memory               : 9271 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T23:18:03.339275Z",
     "start_time": "2022-08-12T23:17:50.056674Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SvhPQJwuouk-",
    "outputId": "cf8a2030-c3f4-4746-e325-8d34b78cdf5b"
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import MaskLabel, TransformerConv\n",
    "from torch_geometric.utils import index_to_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T23:17:37.015925Z",
     "start_time": "2022-08-12T23:17:36.677284Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/semi-supervised-tests/Jupyter\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T23:18:11.710513Z",
     "start_time": "2022-08-12T23:18:11.484742Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "8fI-8u88ojNj",
    "outputId": "08afed7f-7c95-48e1-b8ab-324286483f24"
   },
   "outputs": [],
   "source": [
    "root = \"/home/jovyan/work/semi-supervised-tests/Jupyter/data/OGB\"\n",
    "dataset = PygNodePropPredDataset('ogbn-arxiv', root, T.ToUndirected())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T19:47:37.295170Z",
     "start_time": "2022-08-08T19:47:37.276788Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "8fI-8u88ojNj",
    "outputId": "08afed7f-7c95-48e1-b8ab-324286483f24"
   },
   "outputs": [],
   "source": [
    "class UniMP(torch.nn.Module):   #UniMP stands for Unified Message Passing\n",
    "    def __init__(self, in_channels, num_classes, hidden_channels, num_layers,\n",
    "                 heads, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.label_emb = MaskLabel(num_classes, in_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()  # initialize self.convs\n",
    "        self.norms = torch.nn.ModuleList()  # initialize self.norms\n",
    "        for i in range(1, num_layers + 1):\n",
    "            if i < num_layers:\n",
    "                out_channels = hidden_channels // heads #what is a head?\n",
    "                concat = True   #what does concat do?\n",
    "            else:\n",
    "                out_channels = num_classes\n",
    "                concat = False\n",
    "            conv = TransformerConv(in_channels, out_channels, heads,\n",
    "                                   concat=concat, beta=True, dropout=dropout)\n",
    "            self.convs.append(conv)\n",
    "            in_channels = hidden_channels\n",
    "\n",
    "            if i < num_layers:\n",
    "                self.norms.append(torch.nn.LayerNorm(hidden_channels))\n",
    "                \n",
    "    def forward(self, x, y, edge_index, label_mask):\n",
    "        x = self.label_emb(x, y, label_mask)  #mask some of the input labels\n",
    "        for conv, norm in zip(self.convs, self.norms):\n",
    "            x = norm(conv(x, edge_index)).relu()\n",
    "        # conv -> norm -> ReLu -> conv -> norm -> Relu -> ... -> out\n",
    "        return self.convs[-1](x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFtGqDeHUc6Y"
   },
   "source": [
    "## Questions:  \n",
    "\n",
    "### 2. What does emb stand for?  \n",
    "Embedding\n",
    "\n",
    "### 3. What are normalization layers?  what do they do and why do we need them?\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html  \n",
    "Normalization? but why?\n",
    "\n",
    "### 3.1 What are heads?  \n",
    "\n",
    "### 4. What does concat do?  \n",
    "If set to False, the multi-head attentions are averaged instead of concatenated. (default: True)  \n",
    "\n",
    "### 4.1 What are attentions?  \n",
    "\n",
    "### 7. Why a separate if statement for self.norms?\n",
    "\n",
    "\n",
    "### 8. What does each parameter mean in this function?  \n",
    "x: input?  \n",
    "y: output?  \n",
    "edge_index: how the graph is connected  \n",
    "label_mask: \n",
    "\n",
    "### 8.1 What does the first line do?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "VFtGqDeHUc6Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef forward(self, x: Tensor, y: Tensor, mask: Tensor) -> Tensor:  \\n          \\n        if self.method == \"concat\":  \\n            out = x.new_zeros(y.size(0), self.emb.weight.size(-1))  \\n            out[mask] = self.emb(y[mask])  \\n            return torch.cat([x, out], dim=-1)  \\n        else:  \\n            x = torch.clone(x)  \\n            x[mask] += self.emb(y[mask])  \\n            return x\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def forward(self, x: Tensor, y: Tensor, mask: Tensor) -> Tensor:  \n",
    "        \"\"\"\"\"\"  \n",
    "        if self.method == \"concat\":  \n",
    "            out = x.new_zeros(y.size(0), self.emb.weight.size(-1))  \n",
    "            out[mask] = self.emb(y[mask])  \n",
    "            return torch.cat([x, out], dim=-1)  \n",
    "        else:  \n",
    "            x = torch.clone(x)  \n",
    "            x[mask] += self.emb(y[mask])  \n",
    "            return x\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I assume this applies the mask to the inputs x but why does this process involve y?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T19:47:43.098140Z",
     "start_time": "2022-08-08T19:47:42.458425Z"
    },
    "id": "gMlfPgRjr31t"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = dataset[0].to(device)\n",
    "data.y = data.y.view(-1)\n",
    "model = UniMP(dataset.num_features, dataset.num_classes, hidden_channels=64,\n",
    "              num_layers=3, heads=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)\n",
    " \n",
    "split_idx = dataset.get_idx_split()   # train/val/test split\n",
    "train_mask = index_to_mask(split_idx['train'], size=data.num_nodes)\n",
    "val_mask = index_to_mask(split_idx['valid'], size=data.num_nodes)\n",
    "test_mask = index_to_mask(split_idx['test'], size=data.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeJyK5jpVwxU"
   },
   "source": [
    "### 10. What is weight_decay?  \n",
    "L2 penalty  \n",
    "https://swamp-father-f58.notion.site/2-3-Lecture-Dealing-with-Overfitting-f1a77a6d12f445698e4d8cb340300d88\n",
    "\n",
    "### 12. What does the mask do?  \n",
    "They mask some of the labels, preventing the nodes from accessing those labels during training/validation/testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T20:12:27.868541Z",
     "start_time": "2022-08-08T20:12:27.853075Z"
    },
    "id": "K3nkFpQStWsW"
   },
   "outputs": [],
   "source": [
    "def train(label_rate=0.65):  # How many labels to use for propagation.\n",
    "    model.train()\n",
    "\n",
    "    propagation_mask = MaskLabel.ratio_mask(train_mask, ratio=label_rate)\n",
    "    # Randomly modifies mask by setting a certain ratio (label_rate) of True entries to False\n",
    "    supervision_mask = train_mask ^ propagation_mask  # Exclusive or: Sets each bit to 1 if only one of two bits is 1\n",
    "#     print(\"train_mask:\", train_mask)\n",
    "#     print(\"propagation_mask:\", propagation_mask)\n",
    "#     print(\"supervision_mask:\", supervision_mask)\n",
    "\n",
    "    # how to implement mini-batch gradient descent here? Right now going through the data set all at once, taking up too much memory\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.y, data.edge_index, propagation_mask)  #forward is called\n",
    "    loss = F.cross_entropy(out[supervision_mask], data.y[supervision_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    # evaluation\n",
    "    propagation_mask = train_mask  #no masked nodes while testing\n",
    "    out = model(data.x, data.y, data.edge_index, propagation_mask)  # forward is called\n",
    "    pred = out[val_mask].argmax(dim=-1)  #indices of maximum values across the last dimension\n",
    "    val_acc = int((pred == data.y[val_mask]).sum()) / pred.size(0)  # Validation accuracy\n",
    "#     print(\"out:\", out)\n",
    "#     print(\"out dim:\", out.size())\n",
    "#     print(\"out[val_mask] dim:\", out[val_mask].size())\n",
    "#     print(\"out[test_mask] size:\", out[test_mask].size())\n",
    "#     print(\"out[train_mask] size:\", out[train_mask].size())\n",
    "#     print(\"out summed over dimension 1\", torch.sum(out, dim=1))\n",
    "# #     print(\"out[val_mask]:\", out[val_mask])\n",
    "#     print(\"eval pred:\", pred)\n",
    "#     print(\"labels:\", data.y[val_mask])\n",
    "#     print(\"pred size:\",pred.size())\n",
    "#     print(\"data.y[val_mask] size:\", data.y[val_mask].size())\n",
    "\n",
    "    # testing\n",
    "    propagation_mask = train_mask | val_mask    #Or: Sets each bit to 1 if one of two bits is 1\n",
    "    out = model(data.x, data.y, data.edge_index, propagation_mask)\n",
    "    pred = out[test_mask].argmax(dim=-1)\n",
    "    test_acc = int((pred == data.y[test_mask]).sum()) / pred.size(0)\n",
    "#     print(\"test pred\", pred)\n",
    "    \n",
    "#     print(\"labels:\", data.y[test_mask])\n",
    "\n",
    "    return val_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "out is a torch tensor of dimension [169343, 40].  \n",
    "out[val_mask] is a torch tensor of dimension [29799, 40].  \n",
    "This ratio 0.1759683 should be one of the three numbers in the train/val/test split.  \n",
    "This is confirmed by   \n",
    "out[val_mask] dim: torch.Size([29799, 40])  \n",
    "out[test_mask] size: torch.Size([48603, 40])  \n",
    "out[train_mask] size: torch.Size([90941, 40])  \n",
    "(The three numbers add up to out.size(0))  \n",
    "\n",
    "(during evaluation) pred is a torch tensor of dimension [29799]  \n",
    "It is the indices of maximum values across the second dimension of out[val_mask]\n",
    "\n",
    "I am assuming each of the 40 numbers is some sort of probability of classifying a data point into a certain category. **But they are negative and do not sum to 1. How to interpret these?**\n",
    "This way, pred then consists of the predictions of the model (which category does a certain data point fall into)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9i8Jh2wWHqc"
   },
   "source": [
    "### 13. What is propagation_mask?  \n",
    "decides which nodes to use for message passing\n",
    "### 13.1 What is supervision_mask?  \n",
    "decides which nodes to use for verifying predictions  \n",
    "(propagation + supervision = training)\n",
    "\n",
    "## 14. What is argmax(dim=-1)\n",
    "Returns the indices of the maximum values of a tensor across a dimension.\n",
    "https://pytorch.org/docs/stable/generated/torch.argmax.html\n",
    "\n",
    "finding the argument that gives the max value across the last dimension, which is 40.  \n",
    "But now what does the 40 represent?  \n",
    "40 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_params(model):\n",
    "    for layer in model.children():\n",
    "       if hasattr(layer, 'reset_parameters'):\n",
    "           layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T20:35:57.333224Z",
     "start_time": "2022-08-08T20:12:31.556674Z"
    },
    "id": "5phe-4Fur8I4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.5047, Val: 0.4969, Test: 0.4625\n",
      "Epoch: 002, Loss: 1.7416, Val: 0.5549, Test: 0.5309\n",
      "Epoch: 003, Loss: 1.5370, Val: 0.5991, Test: 0.5861\n",
      "Epoch: 004, Loss: 1.4237, Val: 0.6197, Test: 0.6118\n",
      "Epoch: 005, Loss: 1.3226, Val: 0.6444, Test: 0.6386\n",
      "Epoch: 006, Loss: 1.2427, Val: 0.6611, Test: 0.6536\n",
      "Epoch: 007, Loss: 1.2156, Val: 0.6663, Test: 0.6530\n",
      "Epoch: 008, Loss: 1.1755, Val: 0.6601, Test: 0.6425\n",
      "Epoch: 009, Loss: 1.1755, Val: 0.6619, Test: 0.6443\n",
      "Epoch: 010, Loss: 1.1485, Val: 0.6748, Test: 0.6621\n",
      "Epoch: 011, Loss: 1.1181, Val: 0.6928, Test: 0.6819\n",
      "Epoch: 012, Loss: 1.0841, Val: 0.6985, Test: 0.6942\n",
      "Epoch: 013, Loss: 1.0716, Val: 0.6985, Test: 0.6958\n",
      "Epoch: 014, Loss: 1.0584, Val: 0.6973, Test: 0.6932\n",
      "Epoch: 015, Loss: 1.0497, Val: 0.6994, Test: 0.6939\n",
      "Epoch: 016, Loss: 1.0430, Val: 0.7015, Test: 0.6959\n",
      "Epoch: 017, Loss: 1.0301, Val: 0.7044, Test: 0.6993\n",
      "Epoch: 018, Loss: 1.0162, Val: 0.7078, Test: 0.7033\n",
      "Epoch: 019, Loss: 1.0056, Val: 0.7087, Test: 0.7053\n",
      "Epoch: 020, Loss: 0.9978, Val: 0.7093, Test: 0.7033\n",
      "Epoch: 021, Loss: 0.9993, Val: 0.7102, Test: 0.7011\n",
      "Epoch: 022, Loss: 0.9988, Val: 0.7104, Test: 0.7022\n",
      "Epoch: 023, Loss: 0.9845, Val: 0.7111, Test: 0.7042\n",
      "Epoch: 024, Loss: 0.9772, Val: 0.7127, Test: 0.7066\n",
      "Epoch: 025, Loss: 0.9827, Val: 0.7133, Test: 0.7070\n",
      "Epoch: 026, Loss: 0.9744, Val: 0.7144, Test: 0.7067\n",
      "Epoch: 027, Loss: 0.9693, Val: 0.7165, Test: 0.7088\n",
      "Epoch: 028, Loss: 0.9618, Val: 0.7155, Test: 0.7098\n",
      "Epoch: 029, Loss: 0.9634, Val: 0.7144, Test: 0.7082\n",
      "Epoch: 030, Loss: 0.9590, Val: 0.7138, Test: 0.7070\n",
      "Epoch: 031, Loss: 0.9599, Val: 0.7140, Test: 0.7075\n",
      "Epoch: 032, Loss: 0.9542, Val: 0.7161, Test: 0.7096\n",
      "Epoch: 033, Loss: 0.9456, Val: 0.7165, Test: 0.7111\n",
      "Epoch: 034, Loss: 0.9402, Val: 0.7167, Test: 0.7117\n",
      "Epoch: 035, Loss: 0.9426, Val: 0.7185, Test: 0.7107\n",
      "Epoch: 036, Loss: 0.9348, Val: 0.7174, Test: 0.7094\n",
      "Epoch: 037, Loss: 0.9352, Val: 0.7183, Test: 0.7103\n",
      "Epoch: 038, Loss: 0.9367, Val: 0.7182, Test: 0.7118\n",
      "Epoch: 039, Loss: 0.9237, Val: 0.7188, Test: 0.7136\n",
      "Epoch: 040, Loss: 0.9312, Val: 0.7192, Test: 0.7137\n",
      "Epoch: 041, Loss: 0.9319, Val: 0.7199, Test: 0.7139\n",
      "Epoch: 042, Loss: 0.9259, Val: 0.7192, Test: 0.7134\n",
      "Epoch: 043, Loss: 0.9143, Val: 0.7195, Test: 0.7126\n",
      "Epoch: 044, Loss: 0.9258, Val: 0.7200, Test: 0.7134\n",
      "Epoch: 045, Loss: 0.9220, Val: 0.7215, Test: 0.7152\n",
      "Epoch: 046, Loss: 0.9241, Val: 0.7213, Test: 0.7160\n",
      "Epoch: 047, Loss: 0.9096, Val: 0.7219, Test: 0.7165\n",
      "Epoch: 048, Loss: 0.9129, Val: 0.7213, Test: 0.7154\n",
      "Epoch: 049, Loss: 0.9190, Val: 0.7218, Test: 0.7153\n",
      "Epoch: 050, Loss: 0.9095, Val: 0.7228, Test: 0.7158\n",
      "Epoch: 051, Loss: 0.9038, Val: 0.7226, Test: 0.7157\n",
      "Epoch: 052, Loss: 0.9118, Val: 0.7221, Test: 0.7147\n",
      "Epoch: 053, Loss: 0.9073, Val: 0.7223, Test: 0.7139\n",
      "Epoch: 054, Loss: 0.9081, Val: 0.7229, Test: 0.7146\n",
      "Epoch: 055, Loss: 0.9079, Val: 0.7238, Test: 0.7154\n",
      "Epoch: 056, Loss: 0.9065, Val: 0.7240, Test: 0.7163\n",
      "Epoch: 057, Loss: 0.9064, Val: 0.7246, Test: 0.7165\n",
      "Epoch: 058, Loss: 0.8959, Val: 0.7247, Test: 0.7166\n",
      "Epoch: 059, Loss: 0.9032, Val: 0.7248, Test: 0.7168\n",
      "Epoch: 060, Loss: 0.8997, Val: 0.7241, Test: 0.7165\n",
      "Epoch: 061, Loss: 0.8954, Val: 0.7251, Test: 0.7162\n",
      "Epoch: 062, Loss: 0.8977, Val: 0.7245, Test: 0.7153\n",
      "Epoch: 063, Loss: 0.9018, Val: 0.7253, Test: 0.7154\n",
      "Epoch: 064, Loss: 0.8977, Val: 0.7257, Test: 0.7180\n",
      "Epoch: 065, Loss: 0.8902, Val: 0.7264, Test: 0.7194\n",
      "Epoch: 066, Loss: 0.8957, Val: 0.7261, Test: 0.7182\n",
      "Epoch: 067, Loss: 0.8983, Val: 0.7251, Test: 0.7162\n",
      "Epoch: 068, Loss: 0.8964, Val: 0.7248, Test: 0.7148\n",
      "Epoch: 069, Loss: 0.8987, Val: 0.7257, Test: 0.7160\n",
      "Epoch: 070, Loss: 0.8881, Val: 0.7265, Test: 0.7180\n",
      "Epoch: 071, Loss: 0.8820, Val: 0.7263, Test: 0.7179\n",
      "Epoch: 072, Loss: 0.8882, Val: 0.7263, Test: 0.7186\n",
      "Epoch: 073, Loss: 0.8846, Val: 0.7254, Test: 0.7165\n",
      "Epoch: 074, Loss: 0.8919, Val: 0.7254, Test: 0.7152\n",
      "Epoch: 075, Loss: 0.8878, Val: 0.7260, Test: 0.7156\n",
      "Epoch: 076, Loss: 0.8924, Val: 0.7273, Test: 0.7171\n",
      "Epoch: 077, Loss: 0.8865, Val: 0.7277, Test: 0.7185\n",
      "Epoch: 078, Loss: 0.8799, Val: 0.7268, Test: 0.7180\n",
      "Epoch: 079, Loss: 0.8844, Val: 0.7259, Test: 0.7173\n",
      "Epoch: 080, Loss: 0.8830, Val: 0.7261, Test: 0.7166\n",
      "Epoch: 081, Loss: 0.8893, Val: 0.7255, Test: 0.7163\n",
      "Epoch: 082, Loss: 0.8794, Val: 0.7264, Test: 0.7175\n",
      "Epoch: 083, Loss: 0.8758, Val: 0.7266, Test: 0.7186\n",
      "Epoch: 084, Loss: 0.8870, Val: 0.7275, Test: 0.7209\n",
      "Epoch: 085, Loss: 0.8779, Val: 0.7276, Test: 0.7196\n",
      "Epoch: 086, Loss: 0.8733, Val: 0.7270, Test: 0.7177\n",
      "Epoch: 087, Loss: 0.8802, Val: 0.7267, Test: 0.7161\n",
      "Epoch: 088, Loss: 0.8800, Val: 0.7261, Test: 0.7159\n",
      "Epoch: 089, Loss: 0.8788, Val: 0.7268, Test: 0.7178\n",
      "Epoch: 090, Loss: 0.8755, Val: 0.7279, Test: 0.7192\n",
      "Epoch: 091, Loss: 0.8813, Val: 0.7272, Test: 0.7186\n",
      "Epoch: 092, Loss: 0.8754, Val: 0.7270, Test: 0.7170\n",
      "Epoch: 093, Loss: 0.8741, Val: 0.7263, Test: 0.7160\n",
      "Epoch: 094, Loss: 0.8711, Val: 0.7263, Test: 0.7157\n",
      "Epoch: 095, Loss: 0.8824, Val: 0.7266, Test: 0.7169\n",
      "Epoch: 096, Loss: 0.8789, Val: 0.7276, Test: 0.7195\n",
      "Epoch: 097, Loss: 0.8721, Val: 0.7283, Test: 0.7212\n",
      "Epoch: 098, Loss: 0.8640, Val: 0.7282, Test: 0.7206\n",
      "Epoch: 099, Loss: 0.8676, Val: 0.7266, Test: 0.7178\n",
      "Epoch: 100, Loss: 0.8769, Val: 0.7255, Test: 0.7161\n",
      "Epoch: 101, Loss: 0.8695, Val: 0.7264, Test: 0.7174\n",
      "Epoch: 102, Loss: 0.8723, Val: 0.7283, Test: 0.7219\n",
      "Epoch: 103, Loss: 0.8707, Val: 0.7286, Test: 0.7241\n",
      "Epoch: 104, Loss: 0.8642, Val: 0.7280, Test: 0.7212\n",
      "Epoch: 105, Loss: 0.8790, Val: 0.7269, Test: 0.7177\n",
      "Epoch: 106, Loss: 0.8735, Val: 0.7268, Test: 0.7166\n",
      "Epoch: 107, Loss: 0.8708, Val: 0.7280, Test: 0.7192\n",
      "Epoch: 108, Loss: 0.8651, Val: 0.7284, Test: 0.7216\n",
      "Epoch: 109, Loss: 0.8639, Val: 0.7281, Test: 0.7215\n",
      "Epoch: 110, Loss: 0.8689, Val: 0.7264, Test: 0.7179\n",
      "Epoch: 111, Loss: 0.8682, Val: 0.7256, Test: 0.7140\n",
      "Epoch: 112, Loss: 0.8619, Val: 0.7265, Test: 0.7145\n",
      "Epoch: 113, Loss: 0.8661, Val: 0.7289, Test: 0.7191\n",
      "Epoch: 114, Loss: 0.8686, Val: 0.7298, Test: 0.7227\n",
      "Epoch: 115, Loss: 0.8649, Val: 0.7290, Test: 0.7221\n",
      "Epoch: 116, Loss: 0.8682, Val: 0.7274, Test: 0.7177\n",
      "Epoch: 117, Loss: 0.8588, Val: 0.7265, Test: 0.7164\n",
      "Epoch: 118, Loss: 0.8590, Val: 0.7272, Test: 0.7189\n",
      "Epoch: 119, Loss: 0.8727, Val: 0.7285, Test: 0.7222\n",
      "Epoch: 120, Loss: 0.8630, Val: 0.7295, Test: 0.7229\n",
      "Epoch: 121, Loss: 0.8610, Val: 0.7286, Test: 0.7210\n",
      "Epoch: 122, Loss: 0.8723, Val: 0.7282, Test: 0.7189\n",
      "Epoch: 123, Loss: 0.8680, Val: 0.7281, Test: 0.7191\n",
      "Epoch: 124, Loss: 0.8535, Val: 0.7290, Test: 0.7194\n",
      "Epoch: 125, Loss: 0.8655, Val: 0.7288, Test: 0.7204\n",
      "Epoch: 126, Loss: 0.8691, Val: 0.7272, Test: 0.7193\n",
      "Epoch: 127, Loss: 0.8557, Val: 0.7260, Test: 0.7183\n",
      "Epoch: 128, Loss: 0.8626, Val: 0.7272, Test: 0.7189\n",
      "Epoch: 129, Loss: 0.8635, Val: 0.7287, Test: 0.7210\n",
      "Epoch: 130, Loss: 0.8527, Val: 0.7292, Test: 0.7215\n",
      "Epoch: 131, Loss: 0.8569, Val: 0.7294, Test: 0.7221\n",
      "Epoch: 132, Loss: 0.8636, Val: 0.7287, Test: 0.7209\n",
      "Epoch: 133, Loss: 0.8474, Val: 0.7276, Test: 0.7195\n",
      "Epoch: 134, Loss: 0.8549, Val: 0.7280, Test: 0.7204\n",
      "Epoch: 135, Loss: 0.8620, Val: 0.7285, Test: 0.7213\n",
      "Epoch: 136, Loss: 0.8629, Val: 0.7287, Test: 0.7217\n",
      "Epoch: 137, Loss: 0.8602, Val: 0.7287, Test: 0.7210\n",
      "Epoch: 138, Loss: 0.8552, Val: 0.7291, Test: 0.7208\n",
      "Epoch: 139, Loss: 0.8556, Val: 0.7292, Test: 0.7216\n",
      "Epoch: 140, Loss: 0.8627, Val: 0.7296, Test: 0.7225\n",
      "Epoch: 141, Loss: 0.8537, Val: 0.7296, Test: 0.7222\n",
      "Epoch: 142, Loss: 0.8590, Val: 0.7291, Test: 0.7199\n",
      "Epoch: 143, Loss: 0.8626, Val: 0.7290, Test: 0.7187\n",
      "Epoch: 144, Loss: 0.8620, Val: 0.7292, Test: 0.7194\n",
      "Epoch: 145, Loss: 0.8522, Val: 0.7289, Test: 0.7224\n",
      "Epoch: 146, Loss: 0.8567, Val: 0.7285, Test: 0.7212\n",
      "Epoch: 147, Loss: 0.8615, Val: 0.7290, Test: 0.7199\n",
      "Epoch: 148, Loss: 0.8557, Val: 0.7291, Test: 0.7184\n",
      "Epoch: 149, Loss: 0.8603, Val: 0.7290, Test: 0.7197\n",
      "Epoch: 150, Loss: 0.8560, Val: 0.7295, Test: 0.7215\n",
      "Epoch: 151, Loss: 0.8486, Val: 0.7301, Test: 0.7225\n",
      "Epoch: 152, Loss: 0.8598, Val: 0.7280, Test: 0.7205\n",
      "Epoch: 153, Loss: 0.8628, Val: 0.7273, Test: 0.7166\n",
      "Epoch: 154, Loss: 0.8565, Val: 0.7281, Test: 0.7182\n",
      "Epoch: 155, Loss: 0.8548, Val: 0.7296, Test: 0.7218\n",
      "Epoch: 156, Loss: 0.8578, Val: 0.7295, Test: 0.7230\n",
      "Epoch: 157, Loss: 0.8570, Val: 0.7287, Test: 0.7198\n",
      "Epoch: 158, Loss: 0.8536, Val: 0.7268, Test: 0.7152\n",
      "Epoch: 159, Loss: 0.8523, Val: 0.7268, Test: 0.7148\n",
      "Epoch: 160, Loss: 0.8587, Val: 0.7290, Test: 0.7191\n",
      "Epoch: 161, Loss: 0.8430, Val: 0.7302, Test: 0.7221\n",
      "Epoch: 162, Loss: 0.8550, Val: 0.7297, Test: 0.7211\n",
      "Epoch: 163, Loss: 0.8608, Val: 0.7281, Test: 0.7175\n",
      "Epoch: 164, Loss: 0.8514, Val: 0.7268, Test: 0.7156\n",
      "Epoch: 165, Loss: 0.8533, Val: 0.7281, Test: 0.7179\n",
      "Epoch: 166, Loss: 0.8367, Val: 0.7291, Test: 0.7200\n",
      "Epoch: 167, Loss: 0.8460, Val: 0.7299, Test: 0.7200\n",
      "Epoch: 168, Loss: 0.8549, Val: 0.7291, Test: 0.7187\n",
      "Epoch: 169, Loss: 0.8475, Val: 0.7294, Test: 0.7174\n",
      "Epoch: 170, Loss: 0.8473, Val: 0.7292, Test: 0.7170\n",
      "Epoch: 171, Loss: 0.8527, Val: 0.7287, Test: 0.7164\n",
      "Epoch: 172, Loss: 0.8485, Val: 0.7287, Test: 0.7170\n",
      "Epoch: 173, Loss: 0.8506, Val: 0.7292, Test: 0.7173\n",
      "Epoch: 174, Loss: 0.8489, Val: 0.7293, Test: 0.7183\n",
      "Epoch: 175, Loss: 0.8494, Val: 0.7282, Test: 0.7184\n",
      "Epoch: 176, Loss: 0.8445, Val: 0.7293, Test: 0.7191\n",
      "Epoch: 177, Loss: 0.8457, Val: 0.7290, Test: 0.7193\n",
      "Epoch: 178, Loss: 0.8379, Val: 0.7293, Test: 0.7188\n",
      "Epoch: 179, Loss: 0.8478, Val: 0.7292, Test: 0.7189\n",
      "Epoch: 180, Loss: 0.8444, Val: 0.7296, Test: 0.7203\n",
      "Epoch: 181, Loss: 0.8511, Val: 0.7295, Test: 0.7215\n",
      "Epoch: 182, Loss: 0.8385, Val: 0.7290, Test: 0.7203\n",
      "Epoch: 183, Loss: 0.8416, Val: 0.7282, Test: 0.7169\n",
      "Epoch: 184, Loss: 0.8390, Val: 0.7274, Test: 0.7170\n",
      "Epoch: 185, Loss: 0.8467, Val: 0.7281, Test: 0.7183\n",
      "Epoch: 186, Loss: 0.8479, Val: 0.7293, Test: 0.7204\n",
      "Epoch: 187, Loss: 0.8504, Val: 0.7297, Test: 0.7201\n",
      "Epoch: 188, Loss: 0.8418, Val: 0.7290, Test: 0.7178\n",
      "Epoch: 189, Loss: 0.8450, Val: 0.7283, Test: 0.7150\n",
      "Epoch: 190, Loss: 0.8443, Val: 0.7281, Test: 0.7159\n",
      "Epoch: 191, Loss: 0.8495, Val: 0.7287, Test: 0.7196\n",
      "Epoch: 192, Loss: 0.8476, Val: 0.7290, Test: 0.7219\n",
      "Epoch: 193, Loss: 0.8483, Val: 0.7290, Test: 0.7202\n",
      "Epoch: 194, Loss: 0.8438, Val: 0.7279, Test: 0.7161\n",
      "Epoch: 195, Loss: 0.8459, Val: 0.7274, Test: 0.7156\n",
      "Epoch: 196, Loss: 0.8446, Val: 0.7288, Test: 0.7186\n",
      "Epoch: 197, Loss: 0.8435, Val: 0.7292, Test: 0.7207\n",
      "Epoch: 198, Loss: 0.8376, Val: 0.7294, Test: 0.7213\n",
      "Epoch: 199, Loss: 0.8405, Val: 0.7295, Test: 0.7224\n",
      "Epoch: 200, Loss: 0.8417, Val: 0.7292, Test: 0.7223\n",
      "Epoch: 201, Loss: 0.8456, Val: 0.7288, Test: 0.7207\n",
      "Epoch: 202, Loss: 0.8442, Val: 0.7285, Test: 0.7182\n",
      "Epoch: 203, Loss: 0.8386, Val: 0.7281, Test: 0.7170\n",
      "Epoch: 204, Loss: 0.8335, Val: 0.7283, Test: 0.7184\n",
      "Epoch: 205, Loss: 0.8346, Val: 0.7282, Test: 0.7213\n",
      "Epoch: 206, Loss: 0.8401, Val: 0.7298, Test: 0.7233\n",
      "Epoch: 207, Loss: 0.8349, Val: 0.7300, Test: 0.7226\n",
      "Epoch: 208, Loss: 0.8458, Val: 0.7293, Test: 0.7185\n",
      "Epoch: 209, Loss: 0.8413, Val: 0.7281, Test: 0.7168\n",
      "Epoch: 210, Loss: 0.8425, Val: 0.7277, Test: 0.7189\n",
      "Epoch: 211, Loss: 0.8307, Val: 0.7284, Test: 0.7200\n",
      "Epoch: 212, Loss: 0.8386, Val: 0.7275, Test: 0.7189\n",
      "Epoch: 213, Loss: 0.8424, Val: 0.7281, Test: 0.7202\n",
      "Epoch: 214, Loss: 0.8423, Val: 0.7284, Test: 0.7198\n",
      "Epoch: 215, Loss: 0.8444, Val: 0.7278, Test: 0.7196\n",
      "Epoch: 216, Loss: 0.8494, Val: 0.7279, Test: 0.7198\n",
      "Epoch: 217, Loss: 0.8418, Val: 0.7282, Test: 0.7195\n",
      "Epoch: 218, Loss: 0.8389, Val: 0.7288, Test: 0.7219\n",
      "Epoch: 219, Loss: 0.8318, Val: 0.7292, Test: 0.7227\n",
      "Epoch: 220, Loss: 0.8439, Val: 0.7290, Test: 0.7202\n",
      "Epoch: 221, Loss: 0.8368, Val: 0.7274, Test: 0.7160\n",
      "Epoch: 222, Loss: 0.8394, Val: 0.7273, Test: 0.7177\n",
      "Epoch: 223, Loss: 0.8441, Val: 0.7289, Test: 0.7207\n",
      "Epoch: 224, Loss: 0.8488, Val: 0.7301, Test: 0.7214\n",
      "Epoch: 225, Loss: 0.8381, Val: 0.7278, Test: 0.7176\n",
      "Epoch: 226, Loss: 0.8359, Val: 0.7264, Test: 0.7135\n",
      "Epoch: 227, Loss: 0.8460, Val: 0.7284, Test: 0.7187\n",
      "Epoch: 228, Loss: 0.8425, Val: 0.7300, Test: 0.7237\n",
      "Epoch: 229, Loss: 0.8440, Val: 0.7298, Test: 0.7238\n",
      "Epoch: 230, Loss: 0.8423, Val: 0.7290, Test: 0.7187\n",
      "Epoch: 231, Loss: 0.8460, Val: 0.7272, Test: 0.7157\n",
      "Epoch: 232, Loss: 0.8341, Val: 0.7284, Test: 0.7176\n",
      "Epoch: 233, Loss: 0.8307, Val: 0.7309, Test: 0.7224\n",
      "Epoch: 234, Loss: 0.8350, Val: 0.7304, Test: 0.7226\n",
      "Epoch: 235, Loss: 0.8364, Val: 0.7283, Test: 0.7170\n",
      "Epoch: 236, Loss: 0.8409, Val: 0.7271, Test: 0.7163\n",
      "Epoch: 237, Loss: 0.8368, Val: 0.7283, Test: 0.7189\n",
      "Epoch: 238, Loss: 0.8394, Val: 0.7300, Test: 0.7208\n",
      "Epoch: 239, Loss: 0.8364, Val: 0.7306, Test: 0.7206\n",
      "Epoch: 240, Loss: 0.8478, Val: 0.7300, Test: 0.7188\n",
      "Epoch: 241, Loss: 0.8265, Val: 0.7297, Test: 0.7177\n",
      "Epoch: 242, Loss: 0.8414, Val: 0.7293, Test: 0.7190\n",
      "Epoch: 243, Loss: 0.8357, Val: 0.7290, Test: 0.7193\n",
      "Epoch: 244, Loss: 0.8297, Val: 0.7283, Test: 0.7203\n",
      "Epoch: 245, Loss: 0.8394, Val: 0.7288, Test: 0.7193\n",
      "Epoch: 246, Loss: 0.8310, Val: 0.7301, Test: 0.7217\n",
      "Epoch: 247, Loss: 0.8423, Val: 0.7305, Test: 0.7216\n",
      "Epoch: 248, Loss: 0.8244, Val: 0.7295, Test: 0.7211\n",
      "Epoch: 249, Loss: 0.8371, Val: 0.7286, Test: 0.7201\n",
      "Epoch: 250, Loss: 0.8380, Val: 0.7284, Test: 0.7196\n",
      "Epoch: 251, Loss: 0.8350, Val: 0.7289, Test: 0.7205\n",
      "Epoch: 252, Loss: 0.8311, Val: 0.7291, Test: 0.7205\n",
      "Epoch: 253, Loss: 0.8328, Val: 0.7290, Test: 0.7193\n",
      "Epoch: 254, Loss: 0.8224, Val: 0.7299, Test: 0.7204\n",
      "Epoch: 255, Loss: 0.8326, Val: 0.7295, Test: 0.7211\n",
      "Epoch: 256, Loss: 0.8372, Val: 0.7296, Test: 0.7218\n",
      "Epoch: 257, Loss: 0.8325, Val: 0.7294, Test: 0.7217\n",
      "Epoch: 258, Loss: 0.8317, Val: 0.7279, Test: 0.7190\n",
      "Epoch: 259, Loss: 0.8333, Val: 0.7293, Test: 0.7181\n",
      "Epoch: 260, Loss: 0.8291, Val: 0.7296, Test: 0.7200\n",
      "Epoch: 261, Loss: 0.8209, Val: 0.7299, Test: 0.7208\n",
      "Epoch: 262, Loss: 0.8376, Val: 0.7290, Test: 0.7203\n",
      "Epoch: 263, Loss: 0.8288, Val: 0.7284, Test: 0.7199\n",
      "Epoch: 264, Loss: 0.8281, Val: 0.7292, Test: 0.7217\n",
      "Epoch: 265, Loss: 0.8313, Val: 0.7296, Test: 0.7223\n",
      "Epoch: 266, Loss: 0.8392, Val: 0.7294, Test: 0.7215\n",
      "Epoch: 267, Loss: 0.8298, Val: 0.7265, Test: 0.7163\n",
      "Epoch: 268, Loss: 0.8297, Val: 0.7283, Test: 0.7166\n",
      "Epoch: 269, Loss: 0.8300, Val: 0.7310, Test: 0.7229\n",
      "Epoch: 270, Loss: 0.8296, Val: 0.7322, Test: 0.7248\n",
      "Epoch: 271, Loss: 0.8387, Val: 0.7295, Test: 0.7191\n",
      "Epoch: 272, Loss: 0.8283, Val: 0.7275, Test: 0.7161\n",
      "Epoch: 273, Loss: 0.8257, Val: 0.7282, Test: 0.7188\n",
      "Epoch: 274, Loss: 0.8321, Val: 0.7289, Test: 0.7203\n",
      "Epoch: 275, Loss: 0.8284, Val: 0.7295, Test: 0.7201\n",
      "Epoch: 276, Loss: 0.8313, Val: 0.7293, Test: 0.7182\n",
      "Epoch: 277, Loss: 0.8383, Val: 0.7307, Test: 0.7213\n",
      "Epoch: 278, Loss: 0.8242, Val: 0.7300, Test: 0.7231\n",
      "Epoch: 279, Loss: 0.8325, Val: 0.7289, Test: 0.7188\n",
      "Epoch: 280, Loss: 0.8275, Val: 0.7288, Test: 0.7178\n",
      "Epoch: 281, Loss: 0.8220, Val: 0.7300, Test: 0.7188\n",
      "Epoch: 282, Loss: 0.8256, Val: 0.7316, Test: 0.7247\n",
      "Epoch: 283, Loss: 0.8269, Val: 0.7306, Test: 0.7235\n",
      "Epoch: 284, Loss: 0.8324, Val: 0.7276, Test: 0.7163\n",
      "Epoch: 285, Loss: 0.8345, Val: 0.7269, Test: 0.7127\n",
      "Epoch: 286, Loss: 0.8338, Val: 0.7296, Test: 0.7193\n",
      "Epoch: 287, Loss: 0.8279, Val: 0.7298, Test: 0.7237\n",
      "Epoch: 288, Loss: 0.8254, Val: 0.7293, Test: 0.7226\n",
      "Epoch: 289, Loss: 0.8203, Val: 0.7281, Test: 0.7167\n",
      "Epoch: 290, Loss: 0.8190, Val: 0.7286, Test: 0.7184\n",
      "Epoch: 291, Loss: 0.8271, Val: 0.7299, Test: 0.7216\n",
      "Epoch: 292, Loss: 0.8188, Val: 0.7308, Test: 0.7242\n",
      "Epoch: 293, Loss: 0.8297, Val: 0.7294, Test: 0.7222\n",
      "Epoch: 294, Loss: 0.8229, Val: 0.7284, Test: 0.7194\n",
      "Epoch: 295, Loss: 0.8188, Val: 0.7292, Test: 0.7200\n",
      "Epoch: 296, Loss: 0.8185, Val: 0.7299, Test: 0.7230\n",
      "Epoch: 297, Loss: 0.8244, Val: 0.7299, Test: 0.7234\n",
      "Epoch: 298, Loss: 0.8293, Val: 0.7295, Test: 0.7212\n",
      "Epoch: 299, Loss: 0.8283, Val: 0.7300, Test: 0.7221\n",
      "Epoch: 300, Loss: 0.8336, Val: 0.7306, Test: 0.7238\n"
     ]
    }
   ],
   "source": [
    "reset_params(model)\n",
    "\n",
    "label_rate = 0.55\n",
    "num_epochs = 300\n",
    "loss_lst, val_acc_lst, test_acc_lst = [], [], []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    loss = train(label_rate = label_rate)\n",
    "    val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, '\n",
    "          f'Test: {test_acc:.4f}')\n",
    "    loss_lst.append(loss)\n",
    "    val_acc_lst.append(val_acc)\n",
    "    test_acc_lst.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T19:56:50.173585Z",
     "start_time": "2022-08-08T19:56:48.108274Z"
    }
   },
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T20:57:43.030561Z",
     "start_time": "2022-08-08T20:57:42.872368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "marker_size = 1\n",
    "plt.plot(loss_lst, \"r.\", markersize=marker_size, label=\"loss\")\n",
    "plt.plot(val_acc_lst, \"b.\", markersize=marker_size, label=\"val acc\")\n",
    "plt.plot(test_acc_lst, \"g.\",markersize=marker_size, label=\"test acc\")\n",
    "plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.ylabel(\"value of params\", rotation=90, fontsize=18)\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "# plt.xlim([200, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T20:58:27.249923Z",
     "start_time": "2022-08-08T20:58:27.147265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "marker_size = 1\n",
    "plt.plot(val_acc_lst, \"b.\", markersize=marker_size, label=\"val acc\")\n",
    "plt.plot(test_acc_lst, \"g.\",markersize=marker_size, label=\"test acc\")\n",
    "plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.ylabel(\"value of params\", rotation=90, fontsize=18)\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "# plt.xlim([200, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val epoch: tensor(269)\n",
      "best test epoch: tensor(269)\n",
      "best val acc: tensor(0.7322)\n",
      "best test acc: tensor(0.7248)\n"
     ]
    }
   ],
   "source": [
    "print(\"best val epoch:\", torch.Tensor(val_acc_lst).argmax())\n",
    "print(\"best test epoch:\", torch.Tensor(test_acc_lst).argmax())\n",
    "print(\"best val acc:\", torch.Tensor(val_acc_lst).max())\n",
    "print(\"best test acc:\", torch.Tensor(test_acc_lst).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize label_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label_rate_arr = np.linspace(0.0, 1.0, num=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_loop(label_rate):\n",
    "    reset_params(model)\n",
    "    num_epochs = 300\n",
    "    loss_lst, val_acc_lst, test_acc_lst = [], [], []\n",
    "\n",
    "    print(f'Label_rate: {label_rate}')\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        loss = train(label_rate = label_rate)\n",
    "        val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, '\n",
    "              f'Test: {test_acc:.4f}')\n",
    "        loss_lst.append(loss)\n",
    "        val_acc_lst.append(val_acc)\n",
    "        test_acc_lst.append(test_acc)\n",
    "    return loss_lst, val_acc_lst, test_acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Label_rate: 0.0\n",
      "Epoch: 001, Loss: 0.8324, Val: 0.2086, Test: 0.1214\n",
      "Epoch: 002, Loss: 0.8330, Val: 0.2100, Test: 0.1232\n",
      "Epoch: 003, Loss: 0.8325, Val: 0.2086, Test: 0.1211\n",
      "Epoch: 004, Loss: 0.8305, Val: 0.2085, Test: 0.1211\n",
      "Epoch: 005, Loss: 0.8321, Val: 0.2059, Test: 0.1215\n",
      "Epoch: 006, Loss: 0.8314, Val: 0.2088, Test: 0.1212\n",
      "Epoch: 007, Loss: 0.8320, Val: 0.2118, Test: 0.1248\n",
      "Epoch: 008, Loss: 0.8301, Val: 0.2057, Test: 0.1198\n",
      "Epoch: 009, Loss: 0.8317, Val: 0.2148, Test: 0.1279\n",
      "Epoch: 010, Loss: 0.8316, Val: 0.2049, Test: 0.1206\n",
      "Epoch: 011, Loss: 0.8304, Val: 0.2136, Test: 0.1258\n",
      "Epoch: 012, Loss: 0.8304, Val: 0.2095, Test: 0.1241\n",
      "Epoch: 013, Loss: 0.8318, Val: 0.2092, Test: 0.1218\n",
      "Epoch: 014, Loss: 0.8312, Val: 0.2114, Test: 0.1258\n",
      "Epoch: 015, Loss: 0.8317, Val: 0.2025, Test: 0.1176\n",
      "Epoch: 016, Loss: 0.8308, Val: 0.2122, Test: 0.1241\n",
      "Epoch: 017, Loss: 0.8294, Val: 0.2043, Test: 0.1189\n",
      "Epoch: 018, Loss: 0.8306, Val: 0.2099, Test: 0.1217\n",
      "Epoch: 019, Loss: 0.8335, Val: 0.2027, Test: 0.1184\n",
      "Epoch: 020, Loss: 0.8313, Val: 0.2107, Test: 0.1233\n",
      "Epoch: 021, Loss: 0.8302, Val: 0.2049, Test: 0.1188\n",
      "Epoch: 022, Loss: 0.8311, Val: 0.2100, Test: 0.1221\n",
      "Epoch: 023, Loss: 0.8302, Val: 0.2096, Test: 0.1223\n",
      "Epoch: 024, Loss: 0.8274, Val: 0.2026, Test: 0.1169\n",
      "Epoch: 025, Loss: 0.8341, Val: 0.2124, Test: 0.1245\n",
      "Epoch: 026, Loss: 0.8326, Val: 0.2001, Test: 0.1146\n",
      "Epoch: 027, Loss: 0.8322, Val: 0.2143, Test: 0.1257\n",
      "Epoch: 028, Loss: 0.8325, Val: 0.2019, Test: 0.1159\n",
      "Epoch: 029, Loss: 0.8290, Val: 0.2113, Test: 0.1246\n",
      "Epoch: 030, Loss: 0.8309, Val: 0.2061, Test: 0.1194\n",
      "Epoch: 031, Loss: 0.8313, Val: 0.2101, Test: 0.1237\n",
      "Epoch: 032, Loss: 0.8336, Val: 0.2108, Test: 0.1230\n",
      "Epoch: 033, Loss: 0.8364, Val: 0.2146, Test: 0.1257\n",
      "Epoch: 034, Loss: 0.8354, Val: 0.2065, Test: 0.1200\n",
      "Epoch: 035, Loss: 0.8337, Val: 0.2161, Test: 0.1272\n",
      "Epoch: 036, Loss: 0.8299, Val: 0.2059, Test: 0.1189\n",
      "Epoch: 037, Loss: 0.8265, Val: 0.2166, Test: 0.1279\n",
      "Epoch: 038, Loss: 0.8286, Val: 0.2061, Test: 0.1199\n",
      "Epoch: 039, Loss: 0.8289, Val: 0.2158, Test: 0.1269\n",
      "Epoch: 040, Loss: 0.8288, Val: 0.2078, Test: 0.1228\n",
      "Epoch: 041, Loss: 0.8285, Val: 0.2079, Test: 0.1207\n",
      "Epoch: 042, Loss: 0.8292, Val: 0.2182, Test: 0.1288\n",
      "Epoch: 043, Loss: 0.8299, Val: 0.2056, Test: 0.1196\n",
      "Epoch: 044, Loss: 0.8305, Val: 0.2161, Test: 0.1273\n",
      "Epoch: 045, Loss: 0.8271, Val: 0.2080, Test: 0.1211\n",
      "Epoch: 046, Loss: 0.8258, Val: 0.2125, Test: 0.1249\n",
      "Epoch: 047, Loss: 0.8296, Val: 0.2112, Test: 0.1246\n",
      "Epoch: 048, Loss: 0.8297, Val: 0.2080, Test: 0.1215\n",
      "Epoch: 049, Loss: 0.8293, Val: 0.2181, Test: 0.1293\n",
      "Epoch: 050, Loss: 0.8311, Val: 0.2025, Test: 0.1177\n",
      "Epoch: 051, Loss: 0.8321, Val: 0.2212, Test: 0.1301\n",
      "Epoch: 052, Loss: 0.8292, Val: 0.2011, Test: 0.1174\n",
      "Epoch: 053, Loss: 0.8276, Val: 0.2156, Test: 0.1260\n",
      "Epoch: 054, Loss: 0.8261, Val: 0.2079, Test: 0.1209\n",
      "Epoch: 055, Loss: 0.8282, Val: 0.2143, Test: 0.1248\n",
      "Epoch: 056, Loss: 0.8259, Val: 0.2125, Test: 0.1251\n",
      "Epoch: 057, Loss: 0.8295, Val: 0.2117, Test: 0.1231\n",
      "Epoch: 058, Loss: 0.8299, Val: 0.2145, Test: 0.1263\n",
      "Epoch: 059, Loss: 0.8300, Val: 0.2101, Test: 0.1210\n",
      "Epoch: 060, Loss: 0.8289, Val: 0.2141, Test: 0.1249\n",
      "Epoch: 061, Loss: 0.8309, Val: 0.2113, Test: 0.1201\n",
      "Epoch: 062, Loss: 0.8316, Val: 0.2081, Test: 0.1223\n",
      "Epoch: 063, Loss: 0.8288, Val: 0.2114, Test: 0.1205\n",
      "Epoch: 064, Loss: 0.8274, Val: 0.2076, Test: 0.1201\n",
      "Epoch: 065, Loss: 0.8296, Val: 0.2198, Test: 0.1286\n",
      "Epoch: 066, Loss: 0.8281, Val: 0.2031, Test: 0.1164\n",
      "Epoch: 067, Loss: 0.8270, Val: 0.2222, Test: 0.1302\n",
      "Epoch: 068, Loss: 0.8277, Val: 0.2042, Test: 0.1171\n",
      "Epoch: 069, Loss: 0.8272, Val: 0.2146, Test: 0.1254\n",
      "Epoch: 070, Loss: 0.8262, Val: 0.2145, Test: 0.1246\n",
      "Epoch: 071, Loss: 0.8240, Val: 0.2062, Test: 0.1185\n",
      "Epoch: 072, Loss: 0.8262, Val: 0.2196, Test: 0.1291\n",
      "Epoch: 073, Loss: 0.8243, Val: 0.2092, Test: 0.1204\n",
      "Epoch: 074, Loss: 0.8268, Val: 0.2154, Test: 0.1269\n",
      "Epoch: 075, Loss: 0.8254, Val: 0.2166, Test: 0.1259\n",
      "Epoch: 076, Loss: 0.8257, Val: 0.2100, Test: 0.1216\n",
      "Epoch: 077, Loss: 0.8263, Val: 0.2192, Test: 0.1294\n",
      "Epoch: 078, Loss: 0.8239, Val: 0.2061, Test: 0.1191\n",
      "Epoch: 079, Loss: 0.8271, Val: 0.2161, Test: 0.1255\n",
      "Epoch: 080, Loss: 0.8251, Val: 0.2077, Test: 0.1197\n",
      "Epoch: 081, Loss: 0.8239, Val: 0.2157, Test: 0.1246\n",
      "Epoch: 082, Loss: 0.8233, Val: 0.2094, Test: 0.1204\n",
      "Epoch: 083, Loss: 0.8231, Val: 0.2127, Test: 0.1236\n",
      "Epoch: 084, Loss: 0.8220, Val: 0.2112, Test: 0.1207\n",
      "Epoch: 085, Loss: 0.8267, Val: 0.2093, Test: 0.1222\n",
      "Epoch: 086, Loss: 0.8264, Val: 0.2133, Test: 0.1219\n",
      "Epoch: 087, Loss: 0.8254, Val: 0.2058, Test: 0.1195\n",
      "Epoch: 088, Loss: 0.8270, Val: 0.2180, Test: 0.1248\n",
      "Epoch: 089, Loss: 0.8297, Val: 0.2070, Test: 0.1198\n",
      "Epoch: 090, Loss: 0.8308, Val: 0.2141, Test: 0.1217\n",
      "Epoch: 091, Loss: 0.8320, Val: 0.2102, Test: 0.1225\n",
      "Epoch: 092, Loss: 0.8332, Val: 0.2088, Test: 0.1162\n",
      "Epoch: 093, Loss: 0.8347, Val: 0.2124, Test: 0.1238\n",
      "Epoch: 094, Loss: 0.8291, Val: 0.2082, Test: 0.1184\n",
      "Epoch: 095, Loss: 0.8263, Val: 0.2062, Test: 0.1153\n",
      "Epoch: 096, Loss: 0.8253, Val: 0.2135, Test: 0.1236\n",
      "Epoch: 097, Loss: 0.8296, Val: 0.2066, Test: 0.1157\n",
      "Epoch: 098, Loss: 0.8297, Val: 0.2104, Test: 0.1225\n",
      "Epoch: 099, Loss: 0.8302, Val: 0.2163, Test: 0.1228\n",
      "Epoch: 100, Loss: 0.8265, Val: 0.2099, Test: 0.1204\n",
      "Epoch: 101, Loss: 0.8240, Val: 0.2156, Test: 0.1254\n",
      "Epoch: 102, Loss: 0.8243, Val: 0.2148, Test: 0.1230\n",
      "Epoch: 103, Loss: 0.8291, Val: 0.2120, Test: 0.1239\n",
      "Epoch: 104, Loss: 0.8291, Val: 0.2215, Test: 0.1282\n",
      "Epoch: 105, Loss: 0.8248, Val: 0.2102, Test: 0.1207\n",
      "Epoch: 106, Loss: 0.8216, Val: 0.2166, Test: 0.1265\n",
      "Epoch: 107, Loss: 0.8257, Val: 0.2143, Test: 0.1227\n",
      "Epoch: 108, Loss: 0.8252, Val: 0.2126, Test: 0.1223\n",
      "Epoch: 109, Loss: 0.8235, Val: 0.2157, Test: 0.1241\n",
      "Epoch: 110, Loss: 0.8235, Val: 0.2090, Test: 0.1192\n",
      "Epoch: 111, Loss: 0.8221, Val: 0.2137, Test: 0.1231\n",
      "Epoch: 112, Loss: 0.8238, Val: 0.2050, Test: 0.1149\n",
      "Epoch: 113, Loss: 0.8234, Val: 0.2109, Test: 0.1211\n",
      "Epoch: 114, Loss: 0.8222, Val: 0.2075, Test: 0.1179\n",
      "Epoch: 115, Loss: 0.8215, Val: 0.2094, Test: 0.1176\n",
      "Epoch: 116, Loss: 0.8224, Val: 0.2075, Test: 0.1189\n",
      "Epoch: 117, Loss: 0.8231, Val: 0.2116, Test: 0.1198\n",
      "Epoch: 118, Loss: 0.8257, Val: 0.2071, Test: 0.1171\n",
      "Epoch: 119, Loss: 0.8195, Val: 0.2128, Test: 0.1214\n",
      "Epoch: 120, Loss: 0.8219, Val: 0.2109, Test: 0.1191\n",
      "Epoch: 121, Loss: 0.8243, Val: 0.2092, Test: 0.1204\n",
      "Epoch: 122, Loss: 0.8237, Val: 0.2139, Test: 0.1210\n",
      "Epoch: 123, Loss: 0.8213, Val: 0.2116, Test: 0.1207\n",
      "Epoch: 124, Loss: 0.8182, Val: 0.2067, Test: 0.1180\n",
      "Epoch: 125, Loss: 0.8219, Val: 0.2164, Test: 0.1223\n",
      "Epoch: 126, Loss: 0.8204, Val: 0.2043, Test: 0.1158\n",
      "Epoch: 127, Loss: 0.8238, Val: 0.2202, Test: 0.1252\n",
      "Epoch: 128, Loss: 0.8237, Val: 0.2029, Test: 0.1152\n",
      "Epoch: 129, Loss: 0.8222, Val: 0.2211, Test: 0.1262\n",
      "Epoch: 130, Loss: 0.8192, Val: 0.2078, Test: 0.1170\n",
      "Epoch: 131, Loss: 0.8203, Val: 0.2123, Test: 0.1205\n",
      "Epoch: 132, Loss: 0.8202, Val: 0.2141, Test: 0.1210\n",
      "Epoch: 133, Loss: 0.8188, Val: 0.2050, Test: 0.1162\n",
      "Epoch: 134, Loss: 0.8199, Val: 0.2212, Test: 0.1261\n",
      "Epoch: 135, Loss: 0.8222, Val: 0.2059, Test: 0.1174\n",
      "Epoch: 136, Loss: 0.8222, Val: 0.2124, Test: 0.1208\n",
      "Epoch: 137, Loss: 0.8206, Val: 0.2115, Test: 0.1197\n",
      "Epoch: 138, Loss: 0.8183, Val: 0.2096, Test: 0.1187\n",
      "Epoch: 139, Loss: 0.8184, Val: 0.2220, Test: 0.1266\n",
      "Epoch: 140, Loss: 0.8200, Val: 0.2059, Test: 0.1167\n",
      "Epoch: 141, Loss: 0.8213, Val: 0.2223, Test: 0.1282\n",
      "Epoch: 142, Loss: 0.8189, Val: 0.2068, Test: 0.1174\n",
      "Epoch: 143, Loss: 0.8170, Val: 0.2174, Test: 0.1244\n",
      "Epoch: 144, Loss: 0.8216, Val: 0.2109, Test: 0.1192\n",
      "Epoch: 145, Loss: 0.8245, Val: 0.2213, Test: 0.1267\n",
      "Epoch: 146, Loss: 0.8284, Val: 0.2027, Test: 0.1144\n",
      "Epoch: 147, Loss: 0.8358, Val: 0.2294, Test: 0.1319\n",
      "Epoch: 148, Loss: 0.8345, Val: 0.2045, Test: 0.1149\n",
      "Epoch: 149, Loss: 0.8281, Val: 0.2161, Test: 0.1217\n",
      "Epoch: 150, Loss: 0.8193, Val: 0.2128, Test: 0.1191\n",
      "Epoch: 151, Loss: 0.8223, Val: 0.2030, Test: 0.1149\n",
      "Epoch: 152, Loss: 0.8284, Val: 0.2238, Test: 0.1274\n",
      "Epoch: 153, Loss: 0.8259, Val: 0.2067, Test: 0.1172\n",
      "Epoch: 154, Loss: 0.8191, Val: 0.2206, Test: 0.1273\n",
      "Epoch: 155, Loss: 0.8216, Val: 0.2161, Test: 0.1220\n",
      "Epoch: 156, Loss: 0.8228, Val: 0.2156, Test: 0.1231\n",
      "Epoch: 157, Loss: 0.8198, Val: 0.2195, Test: 0.1269\n",
      "Epoch: 158, Loss: 0.8189, Val: 0.2154, Test: 0.1219\n",
      "Epoch: 159, Loss: 0.8226, Val: 0.2187, Test: 0.1262\n",
      "Epoch: 160, Loss: 0.8223, Val: 0.2166, Test: 0.1237\n",
      "Epoch: 161, Loss: 0.8189, Val: 0.2121, Test: 0.1208\n",
      "Epoch: 162, Loss: 0.8163, Val: 0.2242, Test: 0.1293\n",
      "Epoch: 163, Loss: 0.8213, Val: 0.2103, Test: 0.1197\n",
      "Epoch: 164, Loss: 0.8172, Val: 0.2170, Test: 0.1243\n",
      "Epoch: 165, Loss: 0.8160, Val: 0.2201, Test: 0.1262\n",
      "Epoch: 166, Loss: 0.8196, Val: 0.2134, Test: 0.1222\n",
      "Epoch: 167, Loss: 0.8191, Val: 0.2227, Test: 0.1286\n",
      "Epoch: 168, Loss: 0.8189, Val: 0.2188, Test: 0.1264\n",
      "Epoch: 169, Loss: 0.8179, Val: 0.2166, Test: 0.1237\n",
      "Epoch: 170, Loss: 0.8176, Val: 0.2198, Test: 0.1272\n",
      "Epoch: 171, Loss: 0.8177, Val: 0.2171, Test: 0.1234\n",
      "Epoch: 172, Loss: 0.8179, Val: 0.2137, Test: 0.1230\n",
      "Epoch: 173, Loss: 0.8159, Val: 0.2160, Test: 0.1232\n",
      "Epoch: 174, Loss: 0.8164, Val: 0.2149, Test: 0.1221\n",
      "Epoch: 175, Loss: 0.8162, Val: 0.2142, Test: 0.1224\n",
      "Epoch: 176, Loss: 0.8163, Val: 0.2158, Test: 0.1241\n",
      "Epoch: 177, Loss: 0.8171, Val: 0.2140, Test: 0.1223\n",
      "Epoch: 178, Loss: 0.8168, Val: 0.2154, Test: 0.1225\n",
      "Epoch: 179, Loss: 0.8162, Val: 0.2129, Test: 0.1224\n",
      "Epoch: 180, Loss: 0.8182, Val: 0.2106, Test: 0.1187\n",
      "Epoch: 181, Loss: 0.8155, Val: 0.2133, Test: 0.1209\n",
      "Epoch: 182, Loss: 0.8141, Val: 0.2162, Test: 0.1235\n",
      "Epoch: 183, Loss: 0.8164, Val: 0.2065, Test: 0.1169\n",
      "Epoch: 184, Loss: 0.8146, Val: 0.2197, Test: 0.1254\n",
      "Epoch: 185, Loss: 0.8152, Val: 0.2054, Test: 0.1168\n",
      "Epoch: 186, Loss: 0.8152, Val: 0.2105, Test: 0.1189\n",
      "Epoch: 187, Loss: 0.8141, Val: 0.2169, Test: 0.1234\n",
      "Epoch: 188, Loss: 0.8159, Val: 0.2063, Test: 0.1174\n",
      "Epoch: 189, Loss: 0.8138, Val: 0.2177, Test: 0.1245\n",
      "Epoch: 190, Loss: 0.8169, Val: 0.2110, Test: 0.1198\n",
      "Epoch: 191, Loss: 0.8142, Val: 0.2135, Test: 0.1231\n",
      "Epoch: 192, Loss: 0.8152, Val: 0.2159, Test: 0.1229\n",
      "Epoch: 193, Loss: 0.8149, Val: 0.2151, Test: 0.1228\n",
      "Epoch: 194, Loss: 0.8156, Val: 0.2163, Test: 0.1232\n",
      "Epoch: 195, Loss: 0.8146, Val: 0.2111, Test: 0.1215\n",
      "Epoch: 196, Loss: 0.8164, Val: 0.2195, Test: 0.1243\n",
      "Epoch: 197, Loss: 0.8157, Val: 0.2113, Test: 0.1208\n",
      "Epoch: 198, Loss: 0.8167, Val: 0.2181, Test: 0.1244\n",
      "Epoch: 199, Loss: 0.8168, Val: 0.2138, Test: 0.1221\n",
      "Epoch: 200, Loss: 0.8137, Val: 0.2138, Test: 0.1216\n",
      "Epoch: 201, Loss: 0.8149, Val: 0.2220, Test: 0.1273\n",
      "Epoch: 202, Loss: 0.8130, Val: 0.2089, Test: 0.1201\n",
      "Epoch: 203, Loss: 0.8122, Val: 0.2263, Test: 0.1298\n",
      "Epoch: 204, Loss: 0.8150, Val: 0.2097, Test: 0.1209\n",
      "Epoch: 205, Loss: 0.8166, Val: 0.2248, Test: 0.1277\n",
      "Epoch: 206, Loss: 0.8151, Val: 0.2154, Test: 0.1239\n",
      "Epoch: 207, Loss: 0.8145, Val: 0.2187, Test: 0.1247\n",
      "Epoch: 208, Loss: 0.8136, Val: 0.2185, Test: 0.1248\n",
      "Epoch: 209, Loss: 0.8146, Val: 0.2157, Test: 0.1221\n",
      "Epoch: 210, Loss: 0.8155, Val: 0.2204, Test: 0.1269\n",
      "Epoch: 211, Loss: 0.8138, Val: 0.2165, Test: 0.1229\n",
      "Epoch: 212, Loss: 0.8129, Val: 0.2236, Test: 0.1285\n",
      "Epoch: 213, Loss: 0.8130, Val: 0.2124, Test: 0.1205\n",
      "Epoch: 214, Loss: 0.8155, Val: 0.2232, Test: 0.1293\n",
      "Epoch: 215, Loss: 0.8121, Val: 0.2180, Test: 0.1238\n",
      "Epoch: 216, Loss: 0.8133, Val: 0.2227, Test: 0.1278\n",
      "Epoch: 217, Loss: 0.8146, Val: 0.2248, Test: 0.1288\n",
      "Epoch: 218, Loss: 0.8146, Val: 0.2166, Test: 0.1237\n",
      "Epoch: 219, Loss: 0.8133, Val: 0.2248, Test: 0.1287\n",
      "Epoch: 220, Loss: 0.8164, Val: 0.2217, Test: 0.1282\n",
      "Epoch: 221, Loss: 0.8185, Val: 0.2167, Test: 0.1218\n",
      "Epoch: 222, Loss: 0.8216, Val: 0.2270, Test: 0.1296\n",
      "Epoch: 223, Loss: 0.8228, Val: 0.2116, Test: 0.1185\n",
      "Epoch: 224, Loss: 0.8238, Val: 0.2326, Test: 0.1331\n",
      "Epoch: 225, Loss: 0.8181, Val: 0.2115, Test: 0.1195\n",
      "Epoch: 226, Loss: 0.8148, Val: 0.2215, Test: 0.1271\n",
      "Epoch: 227, Loss: 0.8158, Val: 0.2212, Test: 0.1255\n",
      "Epoch: 228, Loss: 0.8221, Val: 0.2188, Test: 0.1255\n",
      "Epoch: 229, Loss: 0.8229, Val: 0.2295, Test: 0.1317\n",
      "Epoch: 230, Loss: 0.8210, Val: 0.2216, Test: 0.1266\n",
      "Epoch: 231, Loss: 0.8145, Val: 0.2262, Test: 0.1283\n",
      "Epoch: 232, Loss: 0.8138, Val: 0.2202, Test: 0.1276\n",
      "Epoch: 233, Loss: 0.8162, Val: 0.2260, Test: 0.1284\n",
      "Epoch: 234, Loss: 0.8177, Val: 0.2277, Test: 0.1316\n",
      "Epoch: 235, Loss: 0.8169, Val: 0.2190, Test: 0.1250\n",
      "Epoch: 236, Loss: 0.8141, Val: 0.2242, Test: 0.1294\n",
      "Epoch: 237, Loss: 0.8133, Val: 0.2260, Test: 0.1285\n",
      "Epoch: 238, Loss: 0.8165, Val: 0.2225, Test: 0.1303\n",
      "Epoch: 239, Loss: 0.8218, Val: 0.2276, Test: 0.1297\n",
      "Epoch: 240, Loss: 0.8175, Val: 0.2209, Test: 0.1280\n",
      "Epoch: 241, Loss: 0.8142, Val: 0.2185, Test: 0.1251\n",
      "Epoch: 242, Loss: 0.8132, Val: 0.2212, Test: 0.1267\n",
      "Epoch: 243, Loss: 0.8139, Val: 0.2216, Test: 0.1280\n",
      "Epoch: 244, Loss: 0.8171, Val: 0.2195, Test: 0.1248\n",
      "Epoch: 245, Loss: 0.8137, Val: 0.2200, Test: 0.1268\n",
      "Epoch: 246, Loss: 0.8116, Val: 0.2223, Test: 0.1265\n",
      "Epoch: 247, Loss: 0.8129, Val: 0.2167, Test: 0.1237\n",
      "Epoch: 248, Loss: 0.8157, Val: 0.2230, Test: 0.1275\n",
      "Epoch: 249, Loss: 0.8103, Val: 0.2200, Test: 0.1254\n",
      "Epoch: 250, Loss: 0.8098, Val: 0.2176, Test: 0.1243\n",
      "Epoch: 251, Loss: 0.8111, Val: 0.2176, Test: 0.1244\n",
      "Epoch: 252, Loss: 0.8123, Val: 0.2207, Test: 0.1264\n",
      "Epoch: 253, Loss: 0.8113, Val: 0.2199, Test: 0.1243\n",
      "Epoch: 254, Loss: 0.8106, Val: 0.2199, Test: 0.1263\n",
      "Epoch: 255, Loss: 0.8117, Val: 0.2198, Test: 0.1227\n",
      "Epoch: 256, Loss: 0.8088, Val: 0.2207, Test: 0.1263\n",
      "Epoch: 257, Loss: 0.8118, Val: 0.2177, Test: 0.1223\n",
      "Epoch: 258, Loss: 0.8129, Val: 0.2186, Test: 0.1242\n",
      "Epoch: 259, Loss: 0.8090, Val: 0.2221, Test: 0.1263\n",
      "Epoch: 260, Loss: 0.8112, Val: 0.2152, Test: 0.1211\n",
      "Epoch: 261, Loss: 0.8106, Val: 0.2227, Test: 0.1271\n",
      "Epoch: 262, Loss: 0.8097, Val: 0.2197, Test: 0.1241\n",
      "Epoch: 263, Loss: 0.8073, Val: 0.2195, Test: 0.1247\n",
      "Epoch: 264, Loss: 0.8077, Val: 0.2189, Test: 0.1244\n",
      "Epoch: 265, Loss: 0.8072, Val: 0.2245, Test: 0.1276\n",
      "Epoch: 266, Loss: 0.8085, Val: 0.2182, Test: 0.1238\n",
      "Epoch: 267, Loss: 0.8088, Val: 0.2263, Test: 0.1291\n",
      "Epoch: 268, Loss: 0.8088, Val: 0.2189, Test: 0.1249\n",
      "Epoch: 269, Loss: 0.8101, Val: 0.2232, Test: 0.1272\n",
      "Epoch: 270, Loss: 0.8086, Val: 0.2237, Test: 0.1275\n",
      "Epoch: 271, Loss: 0.8082, Val: 0.2200, Test: 0.1251\n",
      "Epoch: 272, Loss: 0.8073, Val: 0.2274, Test: 0.1303\n",
      "Epoch: 273, Loss: 0.8079, Val: 0.2172, Test: 0.1235\n",
      "Epoch: 274, Loss: 0.8086, Val: 0.2284, Test: 0.1320\n",
      "Epoch: 275, Loss: 0.8108, Val: 0.2210, Test: 0.1243\n",
      "Epoch: 276, Loss: 0.8121, Val: 0.2248, Test: 0.1304\n",
      "Epoch: 277, Loss: 0.8112, Val: 0.2210, Test: 0.1245\n",
      "Epoch: 278, Loss: 0.8135, Val: 0.2262, Test: 0.1315\n",
      "Epoch: 279, Loss: 0.8160, Val: 0.2205, Test: 0.1224\n",
      "Epoch: 280, Loss: 0.8165, Val: 0.2250, Test: 0.1309\n",
      "Epoch: 281, Loss: 0.8164, Val: 0.2201, Test: 0.1226\n",
      "Epoch: 282, Loss: 0.8144, Val: 0.2220, Test: 0.1275\n",
      "Epoch: 283, Loss: 0.8121, Val: 0.2311, Test: 0.1310\n",
      "Epoch: 284, Loss: 0.8118, Val: 0.2129, Test: 0.1204\n",
      "Epoch: 285, Loss: 0.8097, Val: 0.2383, Test: 0.1372\n",
      "Epoch: 286, Loss: 0.8133, Val: 0.2150, Test: 0.1213\n",
      "Epoch: 287, Loss: 0.8137, Val: 0.2284, Test: 0.1324\n",
      "Epoch: 288, Loss: 0.8118, Val: 0.2247, Test: 0.1278\n",
      "Epoch: 289, Loss: 0.8081, Val: 0.2163, Test: 0.1233\n",
      "Epoch: 290, Loss: 0.8083, Val: 0.2340, Test: 0.1350\n",
      "Epoch: 291, Loss: 0.8079, Val: 0.2101, Test: 0.1199\n",
      "Epoch: 292, Loss: 0.8118, Val: 0.2287, Test: 0.1317\n",
      "Epoch: 293, Loss: 0.8094, Val: 0.2187, Test: 0.1244\n",
      "Epoch: 294, Loss: 0.8093, Val: 0.2228, Test: 0.1266\n",
      "Epoch: 295, Loss: 0.8063, Val: 0.2286, Test: 0.1310\n",
      "Epoch: 296, Loss: 0.8056, Val: 0.2137, Test: 0.1227\n",
      "Epoch: 297, Loss: 0.8071, Val: 0.2359, Test: 0.1348\n",
      "Epoch: 298, Loss: 0.8072, Val: 0.2163, Test: 0.1243\n",
      "Epoch: 299, Loss: 0.8083, Val: 0.2316, Test: 0.1320\n",
      "Epoch: 300, Loss: 0.8084, Val: 0.2242, Test: 0.1299\n",
      "0.05\n",
      "Label_rate: 0.05\n",
      "Epoch: 001, Loss: 0.9298, Val: 0.1757, Test: 0.1018\n",
      "Epoch: 002, Loss: 0.9286, Val: 0.1883, Test: 0.1093\n",
      "Epoch: 003, Loss: 0.9251, Val: 0.2228, Test: 0.1333\n",
      "Epoch: 004, Loss: 0.9155, Val: 0.2493, Test: 0.1440\n",
      "Epoch: 005, Loss: 0.8952, Val: 0.3203, Test: 0.1773\n",
      "Epoch: 006, Loss: 0.8836, Val: 0.4026, Test: 0.2285\n",
      "Epoch: 007, Loss: 0.8811, Val: 0.4509, Test: 0.2720\n",
      "Epoch: 008, Loss: 0.8744, Val: 0.4982, Test: 0.3390\n",
      "Epoch: 009, Loss: 0.8673, Val: 0.5488, Test: 0.4515\n",
      "Epoch: 010, Loss: 0.8613, Val: 0.5719, Test: 0.5044\n",
      "Epoch: 011, Loss: 0.8613, Val: 0.5970, Test: 0.5416\n",
      "Epoch: 012, Loss: 0.8552, Val: 0.6192, Test: 0.5856\n",
      "Epoch: 013, Loss: 0.8506, Val: 0.6272, Test: 0.6047\n",
      "Epoch: 014, Loss: 0.8427, Val: 0.6437, Test: 0.6297\n",
      "Epoch: 015, Loss: 0.8441, Val: 0.6522, Test: 0.6436\n",
      "Epoch: 016, Loss: 0.8440, Val: 0.6517, Test: 0.6464\n",
      "Epoch: 017, Loss: 0.8398, Val: 0.6620, Test: 0.6571\n",
      "Epoch: 018, Loss: 0.8376, Val: 0.6715, Test: 0.6657\n",
      "Epoch: 019, Loss: 0.8378, Val: 0.6707, Test: 0.6665\n",
      "Epoch: 020, Loss: 0.8410, Val: 0.6723, Test: 0.6702\n",
      "Epoch: 021, Loss: 0.8319, Val: 0.6782, Test: 0.6756\n",
      "Epoch: 022, Loss: 0.8340, Val: 0.6794, Test: 0.6757\n",
      "Epoch: 023, Loss: 0.8347, Val: 0.6809, Test: 0.6786\n",
      "Epoch: 024, Loss: 0.8307, Val: 0.6824, Test: 0.6802\n",
      "Epoch: 025, Loss: 0.8293, Val: 0.6835, Test: 0.6803\n",
      "Epoch: 026, Loss: 0.8300, Val: 0.6860, Test: 0.6814\n",
      "Epoch: 027, Loss: 0.8337, Val: 0.6866, Test: 0.6823\n",
      "Epoch: 028, Loss: 0.8273, Val: 0.6869, Test: 0.6825\n",
      "Epoch: 029, Loss: 0.8287, Val: 0.6881, Test: 0.6836\n",
      "Epoch: 030, Loss: 0.8286, Val: 0.6869, Test: 0.6814\n",
      "Epoch: 031, Loss: 0.8313, Val: 0.6884, Test: 0.6842\n",
      "Epoch: 032, Loss: 0.8259, Val: 0.6899, Test: 0.6861\n",
      "Epoch: 033, Loss: 0.8258, Val: 0.6888, Test: 0.6828\n",
      "Epoch: 034, Loss: 0.8228, Val: 0.6901, Test: 0.6859\n",
      "Epoch: 035, Loss: 0.8218, Val: 0.6907, Test: 0.6845\n",
      "Epoch: 036, Loss: 0.8235, Val: 0.6911, Test: 0.6833\n",
      "Epoch: 037, Loss: 0.8222, Val: 0.6927, Test: 0.6857\n",
      "Epoch: 038, Loss: 0.8243, Val: 0.6918, Test: 0.6848\n",
      "Epoch: 039, Loss: 0.8219, Val: 0.6920, Test: 0.6835\n",
      "Epoch: 040, Loss: 0.8210, Val: 0.6938, Test: 0.6852\n",
      "Epoch: 041, Loss: 0.8227, Val: 0.6928, Test: 0.6848\n",
      "Epoch: 042, Loss: 0.8228, Val: 0.6925, Test: 0.6857\n",
      "Epoch: 043, Loss: 0.8200, Val: 0.6925, Test: 0.6836\n",
      "Epoch: 044, Loss: 0.8215, Val: 0.6924, Test: 0.6830\n",
      "Epoch: 045, Loss: 0.8212, Val: 0.6943, Test: 0.6859\n",
      "Epoch: 046, Loss: 0.8213, Val: 0.6939, Test: 0.6842\n",
      "Epoch: 047, Loss: 0.8191, Val: 0.6929, Test: 0.6833\n",
      "Epoch: 048, Loss: 0.8170, Val: 0.6937, Test: 0.6844\n",
      "Epoch: 049, Loss: 0.8203, Val: 0.6929, Test: 0.6823\n",
      "Epoch: 050, Loss: 0.8199, Val: 0.6925, Test: 0.6847\n",
      "Epoch: 051, Loss: 0.8189, Val: 0.6939, Test: 0.6826\n",
      "Epoch: 052, Loss: 0.8196, Val: 0.6933, Test: 0.6838\n",
      "Epoch: 053, Loss: 0.8214, Val: 0.6948, Test: 0.6869\n",
      "Epoch: 054, Loss: 0.8193, Val: 0.6935, Test: 0.6820\n",
      "Epoch: 055, Loss: 0.8192, Val: 0.6934, Test: 0.6849\n",
      "Epoch: 056, Loss: 0.8166, Val: 0.6946, Test: 0.6839\n",
      "Epoch: 057, Loss: 0.8192, Val: 0.6923, Test: 0.6818\n",
      "Epoch: 058, Loss: 0.8178, Val: 0.6936, Test: 0.6846\n",
      "Epoch: 059, Loss: 0.8192, Val: 0.6923, Test: 0.6824\n",
      "Epoch: 060, Loss: 0.8183, Val: 0.6918, Test: 0.6829\n",
      "Epoch: 061, Loss: 0.8182, Val: 0.6944, Test: 0.6836\n",
      "Epoch: 062, Loss: 0.8173, Val: 0.6927, Test: 0.6827\n",
      "Epoch: 063, Loss: 0.8194, Val: 0.6914, Test: 0.6816\n",
      "Epoch: 064, Loss: 0.8134, Val: 0.6940, Test: 0.6837\n",
      "Epoch: 065, Loss: 0.8183, Val: 0.6942, Test: 0.6860\n",
      "Epoch: 066, Loss: 0.8162, Val: 0.6917, Test: 0.6814\n",
      "Epoch: 067, Loss: 0.8188, Val: 0.6931, Test: 0.6831\n",
      "Epoch: 068, Loss: 0.8175, Val: 0.6944, Test: 0.6851\n",
      "Epoch: 069, Loss: 0.8202, Val: 0.6915, Test: 0.6810\n",
      "Epoch: 070, Loss: 0.8197, Val: 0.6938, Test: 0.6846\n",
      "Epoch: 071, Loss: 0.8175, Val: 0.6948, Test: 0.6841\n",
      "Epoch: 072, Loss: 0.8166, Val: 0.6926, Test: 0.6825\n",
      "Epoch: 073, Loss: 0.8156, Val: 0.6954, Test: 0.6848\n",
      "Epoch: 074, Loss: 0.8142, Val: 0.6955, Test: 0.6848\n",
      "Epoch: 075, Loss: 0.8157, Val: 0.6921, Test: 0.6815\n",
      "Epoch: 076, Loss: 0.8157, Val: 0.6944, Test: 0.6850\n",
      "Epoch: 077, Loss: 0.8167, Val: 0.6953, Test: 0.6857\n",
      "Epoch: 078, Loss: 0.8143, Val: 0.6930, Test: 0.6806\n",
      "Epoch: 079, Loss: 0.8162, Val: 0.6956, Test: 0.6873\n",
      "Epoch: 080, Loss: 0.8166, Val: 0.6952, Test: 0.6849\n",
      "Epoch: 081, Loss: 0.8133, Val: 0.6935, Test: 0.6814\n",
      "Epoch: 082, Loss: 0.8163, Val: 0.6961, Test: 0.6865\n",
      "Epoch: 083, Loss: 0.8149, Val: 0.6935, Test: 0.6827\n",
      "Epoch: 084, Loss: 0.8130, Val: 0.6930, Test: 0.6801\n",
      "Epoch: 085, Loss: 0.8147, Val: 0.6965, Test: 0.6863\n",
      "Epoch: 086, Loss: 0.8159, Val: 0.6933, Test: 0.6810\n",
      "Epoch: 087, Loss: 0.8158, Val: 0.6955, Test: 0.6852\n",
      "Epoch: 088, Loss: 0.8197, Val: 0.6950, Test: 0.6843\n",
      "Epoch: 089, Loss: 0.8176, Val: 0.6941, Test: 0.6853\n",
      "Epoch: 090, Loss: 0.8127, Val: 0.6953, Test: 0.6855\n",
      "Epoch: 091, Loss: 0.8160, Val: 0.6939, Test: 0.6823\n",
      "Epoch: 092, Loss: 0.8125, Val: 0.6942, Test: 0.6844\n",
      "Epoch: 093, Loss: 0.8149, Val: 0.6958, Test: 0.6850\n",
      "Epoch: 094, Loss: 0.8169, Val: 0.6946, Test: 0.6841\n",
      "Epoch: 095, Loss: 0.8133, Val: 0.6959, Test: 0.6849\n",
      "Epoch: 096, Loss: 0.8115, Val: 0.6952, Test: 0.6833\n",
      "Epoch: 097, Loss: 0.8166, Val: 0.6949, Test: 0.6853\n",
      "Epoch: 098, Loss: 0.8169, Val: 0.6946, Test: 0.6814\n",
      "Epoch: 099, Loss: 0.8132, Val: 0.6960, Test: 0.6869\n",
      "Epoch: 100, Loss: 0.8172, Val: 0.6952, Test: 0.6815\n",
      "Epoch: 101, Loss: 0.8149, Val: 0.6934, Test: 0.6805\n",
      "Epoch: 102, Loss: 0.8139, Val: 0.6969, Test: 0.6862\n",
      "Epoch: 103, Loss: 0.8116, Val: 0.6929, Test: 0.6780\n",
      "Epoch: 104, Loss: 0.8119, Val: 0.6950, Test: 0.6839\n",
      "Epoch: 105, Loss: 0.8119, Val: 0.6958, Test: 0.6832\n",
      "Epoch: 106, Loss: 0.8124, Val: 0.6912, Test: 0.6784\n",
      "Epoch: 107, Loss: 0.8147, Val: 0.6965, Test: 0.6818\n",
      "Epoch: 108, Loss: 0.8103, Val: 0.6968, Test: 0.6819\n",
      "Epoch: 109, Loss: 0.8098, Val: 0.6930, Test: 0.6793\n",
      "Epoch: 110, Loss: 0.8102, Val: 0.6950, Test: 0.6807\n",
      "Epoch: 111, Loss: 0.8145, Val: 0.6934, Test: 0.6808\n",
      "Epoch: 112, Loss: 0.8152, Val: 0.6956, Test: 0.6815\n",
      "Epoch: 113, Loss: 0.8110, Val: 0.6962, Test: 0.6840\n",
      "Epoch: 114, Loss: 0.8080, Val: 0.6951, Test: 0.6808\n",
      "Epoch: 115, Loss: 0.8115, Val: 0.6962, Test: 0.6833\n",
      "Epoch: 116, Loss: 0.8092, Val: 0.6945, Test: 0.6802\n",
      "Epoch: 117, Loss: 0.8104, Val: 0.6951, Test: 0.6796\n",
      "Epoch: 118, Loss: 0.8092, Val: 0.6975, Test: 0.6827\n",
      "Epoch: 119, Loss: 0.8147, Val: 0.6939, Test: 0.6790\n",
      "Epoch: 120, Loss: 0.8067, Val: 0.6962, Test: 0.6830\n",
      "Epoch: 121, Loss: 0.8110, Val: 0.6940, Test: 0.6783\n",
      "Epoch: 122, Loss: 0.8105, Val: 0.6947, Test: 0.6799\n",
      "Epoch: 123, Loss: 0.8086, Val: 0.6967, Test: 0.6827\n",
      "Epoch: 124, Loss: 0.8100, Val: 0.6951, Test: 0.6802\n",
      "Epoch: 125, Loss: 0.8117, Val: 0.6941, Test: 0.6796\n",
      "Epoch: 126, Loss: 0.8117, Val: 0.6965, Test: 0.6819\n",
      "Epoch: 127, Loss: 0.8065, Val: 0.6973, Test: 0.6839\n",
      "Epoch: 128, Loss: 0.8123, Val: 0.6963, Test: 0.6817\n",
      "Epoch: 129, Loss: 0.8123, Val: 0.6970, Test: 0.6828\n",
      "Epoch: 130, Loss: 0.8104, Val: 0.6970, Test: 0.6846\n",
      "Epoch: 131, Loss: 0.8120, Val: 0.6973, Test: 0.6854\n",
      "Epoch: 132, Loss: 0.8096, Val: 0.6965, Test: 0.6821\n",
      "Epoch: 133, Loss: 0.8075, Val: 0.6982, Test: 0.6854\n",
      "Epoch: 134, Loss: 0.8033, Val: 0.6979, Test: 0.6850\n",
      "Epoch: 135, Loss: 0.8080, Val: 0.6976, Test: 0.6844\n",
      "Epoch: 136, Loss: 0.8109, Val: 0.6964, Test: 0.6836\n",
      "Epoch: 137, Loss: 0.8116, Val: 0.6989, Test: 0.6890\n",
      "Epoch: 138, Loss: 0.8111, Val: 0.6973, Test: 0.6849\n",
      "Epoch: 139, Loss: 0.8078, Val: 0.6966, Test: 0.6817\n",
      "Epoch: 140, Loss: 0.8114, Val: 0.6996, Test: 0.6870\n",
      "Epoch: 141, Loss: 0.8064, Val: 0.6977, Test: 0.6852\n",
      "Epoch: 142, Loss: 0.8089, Val: 0.6980, Test: 0.6861\n",
      "Epoch: 143, Loss: 0.8102, Val: 0.6936, Test: 0.6794\n",
      "Epoch: 144, Loss: 0.8113, Val: 0.6996, Test: 0.6878\n",
      "Epoch: 145, Loss: 0.8104, Val: 0.6961, Test: 0.6820\n",
      "Epoch: 146, Loss: 0.8049, Val: 0.6951, Test: 0.6810\n",
      "Epoch: 147, Loss: 0.8113, Val: 0.6978, Test: 0.6845\n",
      "Epoch: 148, Loss: 0.8132, Val: 0.6952, Test: 0.6824\n",
      "Epoch: 149, Loss: 0.8124, Val: 0.6963, Test: 0.6834\n",
      "Epoch: 150, Loss: 0.8135, Val: 0.6967, Test: 0.6839\n",
      "Epoch: 151, Loss: 0.8156, Val: 0.6971, Test: 0.6850\n",
      "Epoch: 152, Loss: 0.8118, Val: 0.6942, Test: 0.6793\n",
      "Epoch: 153, Loss: 0.8097, Val: 0.6999, Test: 0.6880\n",
      "Epoch: 154, Loss: 0.8130, Val: 0.6948, Test: 0.6807\n",
      "Epoch: 155, Loss: 0.8134, Val: 0.6917, Test: 0.6802\n",
      "Epoch: 156, Loss: 0.8097, Val: 0.6989, Test: 0.6844\n",
      "Epoch: 157, Loss: 0.8096, Val: 0.6913, Test: 0.6759\n",
      "Epoch: 158, Loss: 0.8107, Val: 0.6975, Test: 0.6839\n",
      "Epoch: 159, Loss: 0.8092, Val: 0.6946, Test: 0.6809\n",
      "Epoch: 160, Loss: 0.8077, Val: 0.6939, Test: 0.6808\n",
      "Epoch: 161, Loss: 0.8103, Val: 0.6988, Test: 0.6841\n",
      "Epoch: 162, Loss: 0.8109, Val: 0.6935, Test: 0.6797\n",
      "Epoch: 163, Loss: 0.8087, Val: 0.6960, Test: 0.6814\n",
      "Epoch: 164, Loss: 0.8072, Val: 0.6977, Test: 0.6839\n",
      "Epoch: 165, Loss: 0.8086, Val: 0.6931, Test: 0.6791\n",
      "Epoch: 166, Loss: 0.8082, Val: 0.6956, Test: 0.6816\n",
      "Epoch: 167, Loss: 0.8072, Val: 0.6965, Test: 0.6836\n",
      "Epoch: 168, Loss: 0.8076, Val: 0.6950, Test: 0.6809\n",
      "Epoch: 169, Loss: 0.8106, Val: 0.6955, Test: 0.6824\n",
      "Epoch: 170, Loss: 0.8048, Val: 0.6963, Test: 0.6827\n",
      "Epoch: 171, Loss: 0.8077, Val: 0.6960, Test: 0.6818\n",
      "Epoch: 172, Loss: 0.8087, Val: 0.6953, Test: 0.6832\n",
      "Epoch: 173, Loss: 0.8081, Val: 0.6973, Test: 0.6819\n",
      "Epoch: 174, Loss: 0.8105, Val: 0.6976, Test: 0.6859\n",
      "Epoch: 175, Loss: 0.8129, Val: 0.6951, Test: 0.6809\n",
      "Epoch: 176, Loss: 0.8107, Val: 0.7012, Test: 0.6910\n",
      "Epoch: 177, Loss: 0.8107, Val: 0.6958, Test: 0.6828\n",
      "Epoch: 178, Loss: 0.8105, Val: 0.6980, Test: 0.6866\n",
      "Epoch: 179, Loss: 0.8072, Val: 0.7005, Test: 0.6905\n",
      "Epoch: 180, Loss: 0.8053, Val: 0.6960, Test: 0.6820\n",
      "Epoch: 181, Loss: 0.8069, Val: 0.6980, Test: 0.6862\n",
      "Epoch: 182, Loss: 0.8053, Val: 0.6997, Test: 0.6880\n",
      "Epoch: 183, Loss: 0.8057, Val: 0.6995, Test: 0.6881\n",
      "Epoch: 184, Loss: 0.8081, Val: 0.6980, Test: 0.6853\n",
      "Epoch: 185, Loss: 0.8038, Val: 0.6989, Test: 0.6875\n",
      "Epoch: 186, Loss: 0.8103, Val: 0.6988, Test: 0.6901\n",
      "Epoch: 187, Loss: 0.8127, Val: 0.6960, Test: 0.6817\n",
      "Epoch: 188, Loss: 0.8158, Val: 0.6991, Test: 0.6911\n",
      "Epoch: 189, Loss: 0.8105, Val: 0.6988, Test: 0.6857\n",
      "Epoch: 190, Loss: 0.8110, Val: 0.6973, Test: 0.6875\n",
      "Epoch: 191, Loss: 0.8078, Val: 0.7005, Test: 0.6896\n",
      "Epoch: 192, Loss: 0.8062, Val: 0.6943, Test: 0.6831\n",
      "Epoch: 193, Loss: 0.8082, Val: 0.7003, Test: 0.6910\n",
      "Epoch: 194, Loss: 0.8101, Val: 0.7009, Test: 0.6892\n",
      "Epoch: 195, Loss: 0.8085, Val: 0.6974, Test: 0.6874\n",
      "Epoch: 196, Loss: 0.8090, Val: 0.6998, Test: 0.6910\n",
      "Epoch: 197, Loss: 0.8064, Val: 0.6962, Test: 0.6856\n",
      "Epoch: 198, Loss: 0.8112, Val: 0.6987, Test: 0.6888\n",
      "Epoch: 199, Loss: 0.8130, Val: 0.6987, Test: 0.6889\n",
      "Epoch: 200, Loss: 0.8070, Val: 0.6955, Test: 0.6845\n",
      "Epoch: 201, Loss: 0.8062, Val: 0.6993, Test: 0.6886\n",
      "Epoch: 202, Loss: 0.8092, Val: 0.6964, Test: 0.6858\n",
      "Epoch: 203, Loss: 0.8074, Val: 0.6974, Test: 0.6871\n",
      "Epoch: 204, Loss: 0.8049, Val: 0.6994, Test: 0.6883\n",
      "Epoch: 205, Loss: 0.8065, Val: 0.6952, Test: 0.6842\n",
      "Epoch: 206, Loss: 0.8078, Val: 0.6988, Test: 0.6886\n",
      "Epoch: 207, Loss: 0.8056, Val: 0.6982, Test: 0.6857\n",
      "Epoch: 208, Loss: 0.8063, Val: 0.6966, Test: 0.6850\n",
      "Epoch: 209, Loss: 0.8016, Val: 0.6986, Test: 0.6885\n",
      "Epoch: 210, Loss: 0.8052, Val: 0.6985, Test: 0.6866\n",
      "Epoch: 211, Loss: 0.8052, Val: 0.6957, Test: 0.6830\n",
      "Epoch: 212, Loss: 0.8043, Val: 0.6993, Test: 0.6884\n",
      "Epoch: 213, Loss: 0.8048, Val: 0.6964, Test: 0.6828\n",
      "Epoch: 214, Loss: 0.8094, Val: 0.6977, Test: 0.6859\n",
      "Epoch: 215, Loss: 0.8050, Val: 0.6991, Test: 0.6852\n",
      "Epoch: 216, Loss: 0.8071, Val: 0.6976, Test: 0.6868\n",
      "Epoch: 217, Loss: 0.8083, Val: 0.6973, Test: 0.6844\n",
      "Epoch: 218, Loss: 0.8099, Val: 0.7001, Test: 0.6901\n",
      "Epoch: 219, Loss: 0.8045, Val: 0.6978, Test: 0.6845\n",
      "Epoch: 220, Loss: 0.8068, Val: 0.6999, Test: 0.6895\n",
      "Epoch: 221, Loss: 0.8057, Val: 0.6979, Test: 0.6870\n",
      "Epoch: 222, Loss: 0.8049, Val: 0.6985, Test: 0.6861\n",
      "Epoch: 223, Loss: 0.8049, Val: 0.7004, Test: 0.6909\n",
      "Epoch: 224, Loss: 0.8080, Val: 0.6955, Test: 0.6809\n",
      "Epoch: 225, Loss: 0.8070, Val: 0.6982, Test: 0.6885\n",
      "Epoch: 226, Loss: 0.8106, Val: 0.6983, Test: 0.6860\n",
      "Epoch: 227, Loss: 0.8032, Val: 0.6969, Test: 0.6839\n",
      "Epoch: 228, Loss: 0.8059, Val: 0.7005, Test: 0.6906\n",
      "Epoch: 229, Loss: 0.8047, Val: 0.6977, Test: 0.6863\n",
      "Epoch: 230, Loss: 0.8039, Val: 0.6975, Test: 0.6851\n",
      "Epoch: 231, Loss: 0.8053, Val: 0.6999, Test: 0.6887\n",
      "Epoch: 232, Loss: 0.8044, Val: 0.6989, Test: 0.6879\n",
      "Epoch: 233, Loss: 0.8032, Val: 0.7003, Test: 0.6865\n",
      "Epoch: 234, Loss: 0.8054, Val: 0.6991, Test: 0.6898\n",
      "Epoch: 235, Loss: 0.8082, Val: 0.6986, Test: 0.6850\n",
      "Epoch: 236, Loss: 0.8021, Val: 0.6995, Test: 0.6892\n",
      "Epoch: 237, Loss: 0.8037, Val: 0.7000, Test: 0.6897\n",
      "Epoch: 238, Loss: 0.8010, Val: 0.6998, Test: 0.6888\n",
      "Epoch: 239, Loss: 0.8013, Val: 0.6986, Test: 0.6885\n",
      "Epoch: 240, Loss: 0.8065, Val: 0.6975, Test: 0.6856\n",
      "Epoch: 241, Loss: 0.8018, Val: 0.7021, Test: 0.6915\n",
      "Epoch: 242, Loss: 0.7998, Val: 0.6983, Test: 0.6871\n",
      "Epoch: 243, Loss: 0.8025, Val: 0.6984, Test: 0.6851\n",
      "Epoch: 244, Loss: 0.8040, Val: 0.7006, Test: 0.6892\n",
      "Epoch: 245, Loss: 0.8027, Val: 0.7001, Test: 0.6892\n",
      "Epoch: 246, Loss: 0.8020, Val: 0.6998, Test: 0.6876\n",
      "Epoch: 247, Loss: 0.8044, Val: 0.6999, Test: 0.6887\n",
      "Epoch: 248, Loss: 0.8025, Val: 0.7021, Test: 0.6911\n",
      "Epoch: 249, Loss: 0.8025, Val: 0.6999, Test: 0.6882\n",
      "Epoch: 250, Loss: 0.8024, Val: 0.6999, Test: 0.6884\n",
      "Epoch: 251, Loss: 0.8040, Val: 0.7005, Test: 0.6898\n",
      "Epoch: 252, Loss: 0.8054, Val: 0.6999, Test: 0.6893\n",
      "Epoch: 253, Loss: 0.8047, Val: 0.6971, Test: 0.6843\n",
      "Epoch: 254, Loss: 0.8098, Val: 0.7024, Test: 0.6908\n",
      "Epoch: 255, Loss: 0.8014, Val: 0.6969, Test: 0.6850\n",
      "Epoch: 256, Loss: 0.8045, Val: 0.7014, Test: 0.6899\n",
      "Epoch: 257, Loss: 0.8041, Val: 0.7009, Test: 0.6895\n",
      "Epoch: 258, Loss: 0.8042, Val: 0.6989, Test: 0.6859\n",
      "Epoch: 259, Loss: 0.8079, Val: 0.7030, Test: 0.6913\n",
      "Epoch: 260, Loss: 0.8067, Val: 0.6971, Test: 0.6832\n",
      "Epoch: 261, Loss: 0.8065, Val: 0.7033, Test: 0.6935\n",
      "Epoch: 262, Loss: 0.8038, Val: 0.6992, Test: 0.6880\n",
      "Epoch: 263, Loss: 0.8037, Val: 0.6995, Test: 0.6863\n",
      "Epoch: 264, Loss: 0.8023, Val: 0.7029, Test: 0.6931\n",
      "Epoch: 265, Loss: 0.8043, Val: 0.6976, Test: 0.6860\n",
      "Epoch: 266, Loss: 0.8043, Val: 0.7024, Test: 0.6923\n",
      "Epoch: 267, Loss: 0.8025, Val: 0.6991, Test: 0.6890\n",
      "Epoch: 268, Loss: 0.8043, Val: 0.6977, Test: 0.6874\n",
      "Epoch: 269, Loss: 0.7975, Val: 0.7029, Test: 0.6931\n",
      "Epoch: 270, Loss: 0.8024, Val: 0.6987, Test: 0.6881\n",
      "Epoch: 271, Loss: 0.8030, Val: 0.6979, Test: 0.6870\n",
      "Epoch: 272, Loss: 0.8003, Val: 0.7017, Test: 0.6917\n",
      "Epoch: 273, Loss: 0.8009, Val: 0.6976, Test: 0.6863\n",
      "Epoch: 274, Loss: 0.8028, Val: 0.7019, Test: 0.6916\n",
      "Epoch: 275, Loss: 0.7996, Val: 0.7022, Test: 0.6901\n",
      "Epoch: 276, Loss: 0.8012, Val: 0.6960, Test: 0.6835\n",
      "Epoch: 277, Loss: 0.8049, Val: 0.7033, Test: 0.6916\n",
      "Epoch: 278, Loss: 0.8059, Val: 0.6986, Test: 0.6873\n",
      "Epoch: 279, Loss: 0.7999, Val: 0.6979, Test: 0.6855\n",
      "Epoch: 280, Loss: 0.8015, Val: 0.7047, Test: 0.6924\n",
      "Epoch: 281, Loss: 0.8029, Val: 0.6980, Test: 0.6864\n",
      "Epoch: 282, Loss: 0.8036, Val: 0.6981, Test: 0.6847\n",
      "Epoch: 283, Loss: 0.8019, Val: 0.7024, Test: 0.6931\n",
      "Epoch: 284, Loss: 0.8046, Val: 0.6963, Test: 0.6829\n",
      "Epoch: 285, Loss: 0.8040, Val: 0.6994, Test: 0.6886\n",
      "Epoch: 286, Loss: 0.8032, Val: 0.6999, Test: 0.6846\n",
      "Epoch: 287, Loss: 0.8060, Val: 0.6948, Test: 0.6811\n",
      "Epoch: 288, Loss: 0.8010, Val: 0.7006, Test: 0.6885\n",
      "Epoch: 289, Loss: 0.7984, Val: 0.6972, Test: 0.6834\n",
      "Epoch: 290, Loss: 0.8029, Val: 0.6986, Test: 0.6843\n",
      "Epoch: 291, Loss: 0.7978, Val: 0.6978, Test: 0.6849\n",
      "Epoch: 292, Loss: 0.8004, Val: 0.6970, Test: 0.6835\n",
      "Epoch: 293, Loss: 0.8018, Val: 0.7011, Test: 0.6878\n",
      "Epoch: 294, Loss: 0.8059, Val: 0.6977, Test: 0.6856\n",
      "Epoch: 295, Loss: 0.7989, Val: 0.6967, Test: 0.6822\n",
      "Epoch: 296, Loss: 0.8027, Val: 0.6992, Test: 0.6880\n",
      "Epoch: 297, Loss: 0.8015, Val: 0.7010, Test: 0.6880\n",
      "Epoch: 298, Loss: 0.8005, Val: 0.6978, Test: 0.6847\n",
      "Epoch: 299, Loss: 0.7994, Val: 0.6989, Test: 0.6871\n",
      "Epoch: 300, Loss: 0.8015, Val: 0.6981, Test: 0.6863\n",
      "0.1\n",
      "Label_rate: 0.1\n",
      "Epoch: 001, Loss: 1.0915, Val: 0.3559, Test: 0.2873\n",
      "Epoch: 002, Loss: 1.0479, Val: 0.3774, Test: 0.3071\n",
      "Epoch: 003, Loss: 0.9955, Val: 0.4972, Test: 0.4107\n",
      "Epoch: 004, Loss: 0.9513, Val: 0.5783, Test: 0.5265\n",
      "Epoch: 005, Loss: 0.9236, Val: 0.5980, Test: 0.5461\n",
      "Epoch: 006, Loss: 0.9169, Val: 0.6159, Test: 0.5703\n",
      "Epoch: 007, Loss: 0.8950, Val: 0.6209, Test: 0.5720\n",
      "Epoch: 008, Loss: 0.8921, Val: 0.6352, Test: 0.5903\n",
      "Epoch: 009, Loss: 0.8866, Val: 0.6437, Test: 0.6007\n",
      "Epoch: 010, Loss: 0.8722, Val: 0.6433, Test: 0.6016\n",
      "Epoch: 011, Loss: 0.8756, Val: 0.6519, Test: 0.6130\n",
      "Epoch: 012, Loss: 0.8621, Val: 0.6570, Test: 0.6193\n",
      "Epoch: 013, Loss: 0.8652, Val: 0.6574, Test: 0.6205\n",
      "Epoch: 014, Loss: 0.8601, Val: 0.6612, Test: 0.6259\n",
      "Epoch: 015, Loss: 0.8619, Val: 0.6682, Test: 0.6346\n",
      "Epoch: 016, Loss: 0.8574, Val: 0.6689, Test: 0.6358\n",
      "Epoch: 017, Loss: 0.8570, Val: 0.6674, Test: 0.6334\n",
      "Epoch: 018, Loss: 0.8540, Val: 0.6725, Test: 0.6385\n",
      "Epoch: 019, Loss: 0.8500, Val: 0.6753, Test: 0.6421\n",
      "Epoch: 020, Loss: 0.8474, Val: 0.6738, Test: 0.6419\n",
      "Epoch: 021, Loss: 0.8397, Val: 0.6766, Test: 0.6466\n",
      "Epoch: 022, Loss: 0.8401, Val: 0.6767, Test: 0.6441\n",
      "Epoch: 023, Loss: 0.8374, Val: 0.6763, Test: 0.6436\n",
      "Epoch: 024, Loss: 0.8413, Val: 0.6781, Test: 0.6471\n",
      "Epoch: 025, Loss: 0.8336, Val: 0.6796, Test: 0.6506\n",
      "Epoch: 026, Loss: 0.8342, Val: 0.6811, Test: 0.6533\n",
      "Epoch: 027, Loss: 0.8310, Val: 0.6812, Test: 0.6535\n",
      "Epoch: 028, Loss: 0.8344, Val: 0.6828, Test: 0.6553\n",
      "Epoch: 029, Loss: 0.8217, Val: 0.6830, Test: 0.6603\n",
      "Epoch: 030, Loss: 0.8293, Val: 0.6844, Test: 0.6596\n",
      "Epoch: 031, Loss: 0.8315, Val: 0.6877, Test: 0.6639\n",
      "Epoch: 032, Loss: 0.8291, Val: 0.6871, Test: 0.6658\n",
      "Epoch: 033, Loss: 0.8253, Val: 0.6858, Test: 0.6643\n",
      "Epoch: 034, Loss: 0.8287, Val: 0.6872, Test: 0.6674\n",
      "Epoch: 035, Loss: 0.8259, Val: 0.6892, Test: 0.6690\n",
      "Epoch: 036, Loss: 0.8246, Val: 0.6871, Test: 0.6657\n",
      "Epoch: 037, Loss: 0.8228, Val: 0.6870, Test: 0.6652\n",
      "Epoch: 038, Loss: 0.8233, Val: 0.6878, Test: 0.6677\n",
      "Epoch: 039, Loss: 0.8257, Val: 0.6893, Test: 0.6655\n",
      "Epoch: 040, Loss: 0.8199, Val: 0.6889, Test: 0.6669\n",
      "Epoch: 041, Loss: 0.8227, Val: 0.6894, Test: 0.6662\n",
      "Epoch: 042, Loss: 0.8185, Val: 0.6906, Test: 0.6660\n",
      "Epoch: 043, Loss: 0.8278, Val: 0.6914, Test: 0.6703\n",
      "Epoch: 044, Loss: 0.8186, Val: 0.6899, Test: 0.6671\n",
      "Epoch: 045, Loss: 0.8222, Val: 0.6890, Test: 0.6665\n",
      "Epoch: 046, Loss: 0.8192, Val: 0.6914, Test: 0.6703\n",
      "Epoch: 047, Loss: 0.8176, Val: 0.6919, Test: 0.6691\n",
      "Epoch: 048, Loss: 0.8184, Val: 0.6903, Test: 0.6703\n",
      "Epoch: 049, Loss: 0.8176, Val: 0.6894, Test: 0.6686\n",
      "Epoch: 050, Loss: 0.8155, Val: 0.6919, Test: 0.6688\n",
      "Epoch: 051, Loss: 0.8168, Val: 0.6919, Test: 0.6703\n",
      "Epoch: 052, Loss: 0.8195, Val: 0.6914, Test: 0.6686\n",
      "Epoch: 053, Loss: 0.8170, Val: 0.6916, Test: 0.6703\n",
      "Epoch: 054, Loss: 0.8158, Val: 0.6919, Test: 0.6709\n",
      "Epoch: 055, Loss: 0.8128, Val: 0.6908, Test: 0.6697\n",
      "Epoch: 056, Loss: 0.8147, Val: 0.6914, Test: 0.6709\n",
      "Epoch: 057, Loss: 0.8109, Val: 0.6923, Test: 0.6713\n",
      "Epoch: 058, Loss: 0.8109, Val: 0.6927, Test: 0.6701\n",
      "Epoch: 059, Loss: 0.8130, Val: 0.6913, Test: 0.6714\n",
      "Epoch: 060, Loss: 0.8148, Val: 0.6910, Test: 0.6686\n",
      "Epoch: 061, Loss: 0.8156, Val: 0.6936, Test: 0.6719\n",
      "Epoch: 062, Loss: 0.8133, Val: 0.6933, Test: 0.6724\n",
      "Epoch: 063, Loss: 0.8099, Val: 0.6928, Test: 0.6697\n",
      "Epoch: 064, Loss: 0.8122, Val: 0.6934, Test: 0.6740\n",
      "Epoch: 065, Loss: 0.8123, Val: 0.6929, Test: 0.6720\n",
      "Epoch: 066, Loss: 0.8107, Val: 0.6929, Test: 0.6705\n",
      "Epoch: 067, Loss: 0.8118, Val: 0.6926, Test: 0.6719\n",
      "Epoch: 068, Loss: 0.8118, Val: 0.6931, Test: 0.6744\n",
      "Epoch: 069, Loss: 0.8147, Val: 0.6936, Test: 0.6733\n",
      "Epoch: 070, Loss: 0.8104, Val: 0.6934, Test: 0.6724\n",
      "Epoch: 071, Loss: 0.8108, Val: 0.6942, Test: 0.6740\n",
      "Epoch: 072, Loss: 0.8164, Val: 0.6948, Test: 0.6753\n",
      "Epoch: 073, Loss: 0.8122, Val: 0.6946, Test: 0.6737\n",
      "Epoch: 074, Loss: 0.8110, Val: 0.6943, Test: 0.6747\n",
      "Epoch: 075, Loss: 0.8123, Val: 0.6949, Test: 0.6755\n",
      "Epoch: 076, Loss: 0.8097, Val: 0.6947, Test: 0.6737\n",
      "Epoch: 077, Loss: 0.8129, Val: 0.6957, Test: 0.6773\n",
      "Epoch: 078, Loss: 0.8129, Val: 0.6955, Test: 0.6754\n",
      "Epoch: 079, Loss: 0.8089, Val: 0.6938, Test: 0.6740\n",
      "Epoch: 080, Loss: 0.8109, Val: 0.6959, Test: 0.6770\n",
      "Epoch: 081, Loss: 0.8072, Val: 0.6954, Test: 0.6757\n",
      "Epoch: 082, Loss: 0.8115, Val: 0.6955, Test: 0.6764\n",
      "Epoch: 083, Loss: 0.8081, Val: 0.6958, Test: 0.6766\n",
      "Epoch: 084, Loss: 0.8117, Val: 0.6957, Test: 0.6762\n",
      "Epoch: 085, Loss: 0.8091, Val: 0.6962, Test: 0.6762\n",
      "Epoch: 086, Loss: 0.8119, Val: 0.6969, Test: 0.6774\n",
      "Epoch: 087, Loss: 0.8110, Val: 0.6968, Test: 0.6786\n",
      "Epoch: 088, Loss: 0.8088, Val: 0.6966, Test: 0.6783\n",
      "Epoch: 089, Loss: 0.8074, Val: 0.6956, Test: 0.6762\n",
      "Epoch: 090, Loss: 0.8101, Val: 0.6959, Test: 0.6786\n",
      "Epoch: 091, Loss: 0.8116, Val: 0.6964, Test: 0.6765\n",
      "Epoch: 092, Loss: 0.8069, Val: 0.6985, Test: 0.6799\n",
      "Epoch: 093, Loss: 0.8072, Val: 0.6961, Test: 0.6769\n",
      "Epoch: 094, Loss: 0.8084, Val: 0.6967, Test: 0.6792\n",
      "Epoch: 095, Loss: 0.8061, Val: 0.6985, Test: 0.6799\n",
      "Epoch: 096, Loss: 0.8059, Val: 0.6980, Test: 0.6794\n",
      "Epoch: 097, Loss: 0.8077, Val: 0.6966, Test: 0.6797\n",
      "Epoch: 098, Loss: 0.8099, Val: 0.6969, Test: 0.6775\n",
      "Epoch: 099, Loss: 0.8052, Val: 0.6989, Test: 0.6820\n",
      "Epoch: 100, Loss: 0.8061, Val: 0.6979, Test: 0.6792\n",
      "Epoch: 101, Loss: 0.8091, Val: 0.6988, Test: 0.6809\n",
      "Epoch: 102, Loss: 0.8079, Val: 0.6994, Test: 0.6816\n",
      "Epoch: 103, Loss: 0.8080, Val: 0.6958, Test: 0.6763\n",
      "Epoch: 104, Loss: 0.8082, Val: 0.6990, Test: 0.6815\n",
      "Epoch: 105, Loss: 0.8079, Val: 0.6988, Test: 0.6800\n",
      "Epoch: 106, Loss: 0.8052, Val: 0.6973, Test: 0.6785\n",
      "Epoch: 107, Loss: 0.8100, Val: 0.6983, Test: 0.6823\n",
      "Epoch: 108, Loss: 0.8061, Val: 0.6976, Test: 0.6796\n",
      "Epoch: 109, Loss: 0.8073, Val: 0.6996, Test: 0.6823\n",
      "Epoch: 110, Loss: 0.8053, Val: 0.6997, Test: 0.6818\n",
      "Epoch: 111, Loss: 0.8073, Val: 0.6974, Test: 0.6795\n",
      "Epoch: 112, Loss: 0.8061, Val: 0.6991, Test: 0.6795\n",
      "Epoch: 113, Loss: 0.8072, Val: 0.6991, Test: 0.6829\n",
      "Epoch: 114, Loss: 0.8048, Val: 0.6996, Test: 0.6819\n",
      "Epoch: 115, Loss: 0.8045, Val: 0.6986, Test: 0.6808\n",
      "Epoch: 116, Loss: 0.8047, Val: 0.6977, Test: 0.6803\n",
      "Epoch: 117, Loss: 0.8073, Val: 0.6993, Test: 0.6807\n",
      "Epoch: 118, Loss: 0.8062, Val: 0.6996, Test: 0.6820\n",
      "Epoch: 119, Loss: 0.8082, Val: 0.6967, Test: 0.6781\n",
      "Epoch: 120, Loss: 0.8078, Val: 0.6999, Test: 0.6823\n",
      "Epoch: 121, Loss: 0.8012, Val: 0.6991, Test: 0.6820\n",
      "Epoch: 122, Loss: 0.8035, Val: 0.6956, Test: 0.6753\n",
      "Epoch: 123, Loss: 0.8059, Val: 0.6998, Test: 0.6838\n",
      "Epoch: 124, Loss: 0.8057, Val: 0.6988, Test: 0.6812\n",
      "Epoch: 125, Loss: 0.8054, Val: 0.6976, Test: 0.6795\n",
      "Epoch: 126, Loss: 0.8031, Val: 0.6995, Test: 0.6839\n",
      "Epoch: 127, Loss: 0.8080, Val: 0.6971, Test: 0.6767\n",
      "Epoch: 128, Loss: 0.8077, Val: 0.6994, Test: 0.6844\n",
      "Epoch: 129, Loss: 0.8098, Val: 0.6987, Test: 0.6815\n",
      "Epoch: 130, Loss: 0.8059, Val: 0.6965, Test: 0.6767\n",
      "Epoch: 131, Loss: 0.8055, Val: 0.7000, Test: 0.6853\n",
      "Epoch: 132, Loss: 0.8032, Val: 0.6978, Test: 0.6782\n",
      "Epoch: 133, Loss: 0.8103, Val: 0.6991, Test: 0.6820\n",
      "Epoch: 134, Loss: 0.8060, Val: 0.6992, Test: 0.6814\n",
      "Epoch: 135, Loss: 0.8027, Val: 0.6989, Test: 0.6801\n",
      "Epoch: 136, Loss: 0.8047, Val: 0.6991, Test: 0.6822\n",
      "Epoch: 137, Loss: 0.8066, Val: 0.6987, Test: 0.6784\n",
      "Epoch: 138, Loss: 0.8121, Val: 0.6975, Test: 0.6813\n",
      "Epoch: 139, Loss: 0.8044, Val: 0.6999, Test: 0.6830\n",
      "Epoch: 140, Loss: 0.8055, Val: 0.7004, Test: 0.6824\n",
      "Epoch: 141, Loss: 0.8071, Val: 0.6991, Test: 0.6832\n",
      "Epoch: 142, Loss: 0.8069, Val: 0.7012, Test: 0.6856\n",
      "Epoch: 143, Loss: 0.8062, Val: 0.7006, Test: 0.6852\n",
      "Epoch: 144, Loss: 0.8040, Val: 0.6992, Test: 0.6825\n",
      "Epoch: 145, Loss: 0.7999, Val: 0.7021, Test: 0.6875\n",
      "Epoch: 146, Loss: 0.8038, Val: 0.7012, Test: 0.6850\n",
      "Epoch: 147, Loss: 0.8052, Val: 0.6992, Test: 0.6806\n",
      "Epoch: 148, Loss: 0.8042, Val: 0.7015, Test: 0.6875\n",
      "Epoch: 149, Loss: 0.8027, Val: 0.7004, Test: 0.6825\n",
      "Epoch: 150, Loss: 0.8066, Val: 0.7017, Test: 0.6843\n",
      "Epoch: 151, Loss: 0.8046, Val: 0.7020, Test: 0.6857\n",
      "Epoch: 152, Loss: 0.8024, Val: 0.7017, Test: 0.6837\n",
      "Epoch: 153, Loss: 0.8076, Val: 0.7011, Test: 0.6856\n",
      "Epoch: 154, Loss: 0.8069, Val: 0.7016, Test: 0.6845\n",
      "Epoch: 155, Loss: 0.8067, Val: 0.7025, Test: 0.6850\n",
      "Epoch: 156, Loss: 0.8071, Val: 0.7024, Test: 0.6853\n",
      "Epoch: 157, Loss: 0.8032, Val: 0.7021, Test: 0.6848\n",
      "Epoch: 158, Loss: 0.8037, Val: 0.7020, Test: 0.6861\n",
      "Epoch: 159, Loss: 0.8051, Val: 0.7006, Test: 0.6809\n",
      "Epoch: 160, Loss: 0.8047, Val: 0.7032, Test: 0.6875\n",
      "Epoch: 161, Loss: 0.8049, Val: 0.7018, Test: 0.6842\n",
      "Epoch: 162, Loss: 0.8034, Val: 0.7006, Test: 0.6808\n",
      "Epoch: 163, Loss: 0.8012, Val: 0.7033, Test: 0.6884\n",
      "Epoch: 164, Loss: 0.8057, Val: 0.7019, Test: 0.6839\n",
      "Epoch: 165, Loss: 0.8025, Val: 0.7014, Test: 0.6837\n",
      "Epoch: 166, Loss: 0.8014, Val: 0.7021, Test: 0.6852\n",
      "Epoch: 167, Loss: 0.8004, Val: 0.7000, Test: 0.6798\n",
      "Epoch: 168, Loss: 0.8077, Val: 0.7025, Test: 0.6874\n",
      "Epoch: 169, Loss: 0.8077, Val: 0.7000, Test: 0.6792\n",
      "Epoch: 170, Loss: 0.7990, Val: 0.7025, Test: 0.6850\n",
      "Epoch: 171, Loss: 0.8000, Val: 0.7028, Test: 0.6846\n",
      "Epoch: 172, Loss: 0.7986, Val: 0.7027, Test: 0.6833\n",
      "Epoch: 173, Loss: 0.8007, Val: 0.7032, Test: 0.6866\n",
      "Epoch: 174, Loss: 0.8029, Val: 0.7018, Test: 0.6838\n",
      "Epoch: 175, Loss: 0.7984, Val: 0.7030, Test: 0.6859\n",
      "Epoch: 176, Loss: 0.8008, Val: 0.7020, Test: 0.6856\n",
      "Epoch: 177, Loss: 0.8006, Val: 0.7008, Test: 0.6835\n",
      "Epoch: 178, Loss: 0.7992, Val: 0.7033, Test: 0.6874\n",
      "Epoch: 179, Loss: 0.7997, Val: 0.7029, Test: 0.6854\n",
      "Epoch: 180, Loss: 0.8046, Val: 0.7011, Test: 0.6836\n",
      "Epoch: 181, Loss: 0.7998, Val: 0.7017, Test: 0.6841\n",
      "Epoch: 182, Loss: 0.8029, Val: 0.7019, Test: 0.6839\n",
      "Epoch: 183, Loss: 0.8002, Val: 0.7011, Test: 0.6831\n",
      "Epoch: 184, Loss: 0.7998, Val: 0.7040, Test: 0.6850\n",
      "Epoch: 185, Loss: 0.8008, Val: 0.7025, Test: 0.6839\n",
      "Epoch: 186, Loss: 0.7975, Val: 0.7008, Test: 0.6825\n",
      "Epoch: 187, Loss: 0.8003, Val: 0.7043, Test: 0.6866\n",
      "Epoch: 188, Loss: 0.8002, Val: 0.7004, Test: 0.6798\n",
      "Epoch: 189, Loss: 0.8016, Val: 0.7035, Test: 0.6862\n",
      "Epoch: 190, Loss: 0.8011, Val: 0.7041, Test: 0.6874\n",
      "Epoch: 191, Loss: 0.8005, Val: 0.7017, Test: 0.6826\n",
      "Epoch: 192, Loss: 0.8027, Val: 0.7024, Test: 0.6853\n",
      "Epoch: 193, Loss: 0.7997, Val: 0.7052, Test: 0.6887\n",
      "Epoch: 194, Loss: 0.8025, Val: 0.7018, Test: 0.6841\n",
      "Epoch: 195, Loss: 0.7994, Val: 0.7027, Test: 0.6865\n",
      "Epoch: 196, Loss: 0.7990, Val: 0.7027, Test: 0.6862\n",
      "Epoch: 197, Loss: 0.8001, Val: 0.7044, Test: 0.6878\n",
      "Epoch: 198, Loss: 0.8009, Val: 0.7045, Test: 0.6872\n",
      "Epoch: 199, Loss: 0.7968, Val: 0.7027, Test: 0.6851\n",
      "Epoch: 200, Loss: 0.7993, Val: 0.7043, Test: 0.6891\n",
      "Epoch: 201, Loss: 0.7990, Val: 0.7039, Test: 0.6876\n",
      "Epoch: 202, Loss: 0.7998, Val: 0.7029, Test: 0.6880\n",
      "Epoch: 203, Loss: 0.7981, Val: 0.7020, Test: 0.6841\n",
      "Epoch: 204, Loss: 0.7992, Val: 0.7045, Test: 0.6884\n",
      "Epoch: 205, Loss: 0.7974, Val: 0.7033, Test: 0.6885\n",
      "Epoch: 206, Loss: 0.7979, Val: 0.7029, Test: 0.6851\n",
      "Epoch: 207, Loss: 0.8050, Val: 0.7054, Test: 0.6941\n",
      "Epoch: 208, Loss: 0.8046, Val: 0.7007, Test: 0.6818\n",
      "Epoch: 209, Loss: 0.8004, Val: 0.7049, Test: 0.6916\n",
      "Epoch: 210, Loss: 0.7964, Val: 0.7055, Test: 0.6922\n",
      "Epoch: 211, Loss: 0.8008, Val: 0.7022, Test: 0.6835\n",
      "Epoch: 212, Loss: 0.8031, Val: 0.7040, Test: 0.6906\n",
      "Epoch: 213, Loss: 0.8011, Val: 0.7029, Test: 0.6883\n",
      "Epoch: 214, Loss: 0.8000, Val: 0.7031, Test: 0.6865\n",
      "Epoch: 215, Loss: 0.7940, Val: 0.7032, Test: 0.6879\n",
      "Epoch: 216, Loss: 0.7985, Val: 0.7024, Test: 0.6837\n",
      "Epoch: 217, Loss: 0.8002, Val: 0.7048, Test: 0.6904\n",
      "Epoch: 218, Loss: 0.7977, Val: 0.7036, Test: 0.6876\n",
      "Epoch: 219, Loss: 0.7979, Val: 0.7020, Test: 0.6841\n",
      "Epoch: 220, Loss: 0.7988, Val: 0.7034, Test: 0.6895\n",
      "Epoch: 221, Loss: 0.7978, Val: 0.7046, Test: 0.6866\n",
      "Epoch: 222, Loss: 0.7992, Val: 0.7038, Test: 0.6871\n",
      "Epoch: 223, Loss: 0.7926, Val: 0.7041, Test: 0.6862\n",
      "Epoch: 224, Loss: 0.8004, Val: 0.7046, Test: 0.6872\n",
      "Epoch: 225, Loss: 0.7999, Val: 0.7043, Test: 0.6891\n",
      "Epoch: 226, Loss: 0.7993, Val: 0.7028, Test: 0.6848\n",
      "Epoch: 227, Loss: 0.7940, Val: 0.7047, Test: 0.6876\n",
      "Epoch: 228, Loss: 0.7969, Val: 0.7048, Test: 0.6879\n",
      "Epoch: 229, Loss: 0.7946, Val: 0.7023, Test: 0.6869\n",
      "Epoch: 230, Loss: 0.7970, Val: 0.7056, Test: 0.6899\n",
      "Epoch: 231, Loss: 0.7953, Val: 0.7029, Test: 0.6851\n",
      "Epoch: 232, Loss: 0.7963, Val: 0.7036, Test: 0.6900\n",
      "Epoch: 233, Loss: 0.7966, Val: 0.7054, Test: 0.6886\n",
      "Epoch: 234, Loss: 0.7973, Val: 0.7031, Test: 0.6868\n",
      "Epoch: 235, Loss: 0.7970, Val: 0.7048, Test: 0.6893\n",
      "Epoch: 236, Loss: 0.7976, Val: 0.7046, Test: 0.6878\n",
      "Epoch: 237, Loss: 0.8009, Val: 0.7018, Test: 0.6829\n",
      "Epoch: 238, Loss: 0.7971, Val: 0.7056, Test: 0.6892\n",
      "Epoch: 239, Loss: 0.8022, Val: 0.7035, Test: 0.6876\n",
      "Epoch: 240, Loss: 0.7993, Val: 0.7012, Test: 0.6795\n",
      "Epoch: 241, Loss: 0.8028, Val: 0.7049, Test: 0.6919\n",
      "Epoch: 242, Loss: 0.8020, Val: 0.7045, Test: 0.6857\n",
      "Epoch: 243, Loss: 0.7958, Val: 0.7033, Test: 0.6833\n",
      "Epoch: 244, Loss: 0.8006, Val: 0.7042, Test: 0.6872\n",
      "Epoch: 245, Loss: 0.7988, Val: 0.7048, Test: 0.6884\n",
      "Epoch: 246, Loss: 0.7953, Val: 0.7036, Test: 0.6855\n",
      "Epoch: 247, Loss: 0.7984, Val: 0.7041, Test: 0.6846\n",
      "Epoch: 248, Loss: 0.7943, Val: 0.7037, Test: 0.6863\n",
      "Epoch: 249, Loss: 0.7966, Val: 0.7054, Test: 0.6878\n",
      "Epoch: 250, Loss: 0.8001, Val: 0.7022, Test: 0.6828\n",
      "Epoch: 251, Loss: 0.7973, Val: 0.7049, Test: 0.6908\n",
      "Epoch: 252, Loss: 0.7987, Val: 0.7062, Test: 0.6900\n",
      "Epoch: 253, Loss: 0.7984, Val: 0.7033, Test: 0.6861\n",
      "Epoch: 254, Loss: 0.7984, Val: 0.7045, Test: 0.6888\n",
      "Epoch: 255, Loss: 0.7983, Val: 0.7055, Test: 0.6908\n",
      "Epoch: 256, Loss: 0.7958, Val: 0.7041, Test: 0.6878\n",
      "Epoch: 257, Loss: 0.7982, Val: 0.7056, Test: 0.6908\n",
      "Epoch: 258, Loss: 0.7957, Val: 0.7052, Test: 0.6895\n",
      "Epoch: 259, Loss: 0.7953, Val: 0.7053, Test: 0.6908\n",
      "Epoch: 260, Loss: 0.7982, Val: 0.7052, Test: 0.6915\n",
      "Epoch: 261, Loss: 0.7968, Val: 0.7045, Test: 0.6893\n",
      "Epoch: 262, Loss: 0.7972, Val: 0.7054, Test: 0.6897\n",
      "Epoch: 263, Loss: 0.7934, Val: 0.7058, Test: 0.6896\n",
      "Epoch: 264, Loss: 0.7958, Val: 0.7027, Test: 0.6879\n",
      "Epoch: 265, Loss: 0.7941, Val: 0.7048, Test: 0.6887\n",
      "Epoch: 266, Loss: 0.7935, Val: 0.7057, Test: 0.6889\n",
      "Epoch: 267, Loss: 0.7967, Val: 0.7023, Test: 0.6864\n",
      "Epoch: 268, Loss: 0.7940, Val: 0.7044, Test: 0.6872\n",
      "Epoch: 269, Loss: 0.7951, Val: 0.7055, Test: 0.6882\n",
      "Epoch: 270, Loss: 0.7950, Val: 0.7046, Test: 0.6881\n",
      "Epoch: 271, Loss: 0.7950, Val: 0.7055, Test: 0.6887\n",
      "Epoch: 272, Loss: 0.7945, Val: 0.7044, Test: 0.6880\n",
      "Epoch: 273, Loss: 0.7941, Val: 0.7053, Test: 0.6891\n",
      "Epoch: 274, Loss: 0.7932, Val: 0.7064, Test: 0.6917\n",
      "Epoch: 275, Loss: 0.7943, Val: 0.7037, Test: 0.6875\n",
      "Epoch: 276, Loss: 0.7947, Val: 0.7060, Test: 0.6894\n",
      "Epoch: 277, Loss: 0.7949, Val: 0.7053, Test: 0.6883\n",
      "Epoch: 278, Loss: 0.7952, Val: 0.7036, Test: 0.6862\n",
      "Epoch: 279, Loss: 0.7935, Val: 0.7052, Test: 0.6882\n",
      "Epoch: 280, Loss: 0.7948, Val: 0.7037, Test: 0.6881\n",
      "Epoch: 281, Loss: 0.7935, Val: 0.7054, Test: 0.6878\n",
      "Epoch: 282, Loss: 0.7969, Val: 0.7071, Test: 0.6930\n",
      "Epoch: 283, Loss: 0.7935, Val: 0.7028, Test: 0.6860\n",
      "Epoch: 284, Loss: 0.7940, Val: 0.7067, Test: 0.6902\n",
      "Epoch: 285, Loss: 0.7963, Val: 0.7048, Test: 0.6869\n",
      "Epoch: 286, Loss: 0.7920, Val: 0.7039, Test: 0.6855\n",
      "Epoch: 287, Loss: 0.7948, Val: 0.7073, Test: 0.6907\n",
      "Epoch: 288, Loss: 0.7968, Val: 0.7036, Test: 0.6842\n",
      "Epoch: 289, Loss: 0.7980, Val: 0.7068, Test: 0.6919\n",
      "Epoch: 290, Loss: 0.7929, Val: 0.7028, Test: 0.6834\n",
      "Epoch: 291, Loss: 0.7964, Val: 0.7050, Test: 0.6892\n",
      "Epoch: 292, Loss: 0.7999, Val: 0.7059, Test: 0.6887\n",
      "Epoch: 293, Loss: 0.7958, Val: 0.7055, Test: 0.6903\n",
      "Epoch: 294, Loss: 0.7944, Val: 0.7051, Test: 0.6871\n",
      "Epoch: 295, Loss: 0.7950, Val: 0.7038, Test: 0.6867\n",
      "Epoch: 296, Loss: 0.7938, Val: 0.7046, Test: 0.6893\n",
      "Epoch: 297, Loss: 0.7943, Val: 0.7031, Test: 0.6846\n",
      "Epoch: 298, Loss: 0.7955, Val: 0.7042, Test: 0.6887\n",
      "Epoch: 299, Loss: 0.7950, Val: 0.7049, Test: 0.6863\n",
      "Epoch: 300, Loss: 0.7901, Val: 0.7039, Test: 0.6845\n",
      "0.15000000000000002\n",
      "Label_rate: 0.15000000000000002\n",
      "Epoch: 001, Loss: 1.2922, Val: 0.4481, Test: 0.3889\n",
      "Epoch: 002, Loss: 1.3011, Val: 0.4705, Test: 0.4133\n",
      "Epoch: 003, Loss: 1.2735, Val: 0.5033, Test: 0.4389\n",
      "Epoch: 004, Loss: 1.2129, Val: 0.5541, Test: 0.4993\n",
      "Epoch: 005, Loss: 1.1511, Val: 0.5921, Test: 0.5463\n",
      "Epoch: 006, Loss: 1.0559, Val: 0.5967, Test: 0.5455\n",
      "Epoch: 007, Loss: 1.0605, Val: 0.6260, Test: 0.5837\n",
      "Epoch: 008, Loss: 1.0142, Val: 0.6499, Test: 0.6194\n",
      "Epoch: 009, Loss: 1.0146, Val: 0.6606, Test: 0.6360\n",
      "Epoch: 010, Loss: 0.9833, Val: 0.6623, Test: 0.6368\n",
      "Epoch: 011, Loss: 0.9829, Val: 0.6706, Test: 0.6424\n",
      "Epoch: 012, Loss: 0.9462, Val: 0.6738, Test: 0.6445\n",
      "Epoch: 013, Loss: 0.9405, Val: 0.6714, Test: 0.6408\n",
      "Epoch: 014, Loss: 0.9514, Val: 0.6710, Test: 0.6365\n",
      "Epoch: 015, Loss: 0.9239, Val: 0.6697, Test: 0.6366\n",
      "Epoch: 016, Loss: 0.9201, Val: 0.6758, Test: 0.6465\n",
      "Epoch: 017, Loss: 0.9204, Val: 0.6815, Test: 0.6546\n",
      "Epoch: 018, Loss: 0.9038, Val: 0.6801, Test: 0.6528\n",
      "Epoch: 019, Loss: 0.9046, Val: 0.6815, Test: 0.6569\n",
      "Epoch: 020, Loss: 0.8936, Val: 0.6854, Test: 0.6593\n",
      "Epoch: 021, Loss: 0.8927, Val: 0.6820, Test: 0.6549\n",
      "Epoch: 022, Loss: 0.8876, Val: 0.6826, Test: 0.6574\n",
      "Epoch: 023, Loss: 0.8792, Val: 0.6872, Test: 0.6657\n",
      "Epoch: 024, Loss: 0.8783, Val: 0.6879, Test: 0.6644\n",
      "Epoch: 025, Loss: 0.8786, Val: 0.6859, Test: 0.6595\n",
      "Epoch: 026, Loss: 0.8729, Val: 0.6877, Test: 0.6634\n",
      "Epoch: 027, Loss: 0.8645, Val: 0.6901, Test: 0.6665\n",
      "Epoch: 028, Loss: 0.8672, Val: 0.6869, Test: 0.6614\n",
      "Epoch: 029, Loss: 0.8635, Val: 0.6847, Test: 0.6565\n",
      "Epoch: 030, Loss: 0.8591, Val: 0.6882, Test: 0.6606\n",
      "Epoch: 031, Loss: 0.8551, Val: 0.6930, Test: 0.6691\n",
      "Epoch: 032, Loss: 0.8535, Val: 0.6922, Test: 0.6690\n",
      "Epoch: 033, Loss: 0.8524, Val: 0.6879, Test: 0.6597\n",
      "Epoch: 034, Loss: 0.8448, Val: 0.6867, Test: 0.6574\n",
      "Epoch: 035, Loss: 0.8467, Val: 0.6936, Test: 0.6671\n",
      "Epoch: 036, Loss: 0.8472, Val: 0.6919, Test: 0.6641\n",
      "Epoch: 037, Loss: 0.8413, Val: 0.6864, Test: 0.6569\n",
      "Epoch: 038, Loss: 0.8429, Val: 0.6869, Test: 0.6571\n",
      "Epoch: 039, Loss: 0.8365, Val: 0.6939, Test: 0.6647\n",
      "Epoch: 040, Loss: 0.8410, Val: 0.6946, Test: 0.6652\n",
      "Epoch: 041, Loss: 0.8348, Val: 0.6908, Test: 0.6609\n",
      "Epoch: 042, Loss: 0.8333, Val: 0.6903, Test: 0.6592\n",
      "Epoch: 043, Loss: 0.8344, Val: 0.6923, Test: 0.6637\n",
      "Epoch: 044, Loss: 0.8305, Val: 0.6931, Test: 0.6642\n",
      "Epoch: 045, Loss: 0.8281, Val: 0.6927, Test: 0.6638\n",
      "Epoch: 046, Loss: 0.8273, Val: 0.6922, Test: 0.6648\n",
      "Epoch: 047, Loss: 0.8258, Val: 0.6925, Test: 0.6663\n",
      "Epoch: 048, Loss: 0.8320, Val: 0.6927, Test: 0.6670\n",
      "Epoch: 049, Loss: 0.8252, Val: 0.6948, Test: 0.6690\n",
      "Epoch: 050, Loss: 0.8276, Val: 0.6945, Test: 0.6695\n",
      "Epoch: 051, Loss: 0.8222, Val: 0.6943, Test: 0.6699\n",
      "Epoch: 052, Loss: 0.8196, Val: 0.6941, Test: 0.6681\n",
      "Epoch: 053, Loss: 0.8265, Val: 0.6950, Test: 0.6686\n",
      "Epoch: 054, Loss: 0.8220, Val: 0.6972, Test: 0.6733\n",
      "Epoch: 055, Loss: 0.8160, Val: 0.6966, Test: 0.6744\n",
      "Epoch: 056, Loss: 0.8232, Val: 0.6952, Test: 0.6715\n",
      "Epoch: 057, Loss: 0.8207, Val: 0.6937, Test: 0.6689\n",
      "Epoch: 058, Loss: 0.8204, Val: 0.6964, Test: 0.6749\n",
      "Epoch: 059, Loss: 0.8197, Val: 0.6982, Test: 0.6773\n",
      "Epoch: 060, Loss: 0.8181, Val: 0.6966, Test: 0.6736\n",
      "Epoch: 061, Loss: 0.8148, Val: 0.6959, Test: 0.6737\n",
      "Epoch: 062, Loss: 0.8227, Val: 0.6976, Test: 0.6765\n",
      "Epoch: 063, Loss: 0.8177, Val: 0.6984, Test: 0.6774\n",
      "Epoch: 064, Loss: 0.8212, Val: 0.6976, Test: 0.6770\n",
      "Epoch: 065, Loss: 0.8149, Val: 0.6964, Test: 0.6763\n",
      "Epoch: 066, Loss: 0.8190, Val: 0.6963, Test: 0.6755\n",
      "Epoch: 067, Loss: 0.8161, Val: 0.6978, Test: 0.6765\n",
      "Epoch: 068, Loss: 0.8161, Val: 0.6991, Test: 0.6787\n",
      "Epoch: 069, Loss: 0.8173, Val: 0.6981, Test: 0.6775\n",
      "Epoch: 070, Loss: 0.8152, Val: 0.6962, Test: 0.6770\n",
      "Epoch: 071, Loss: 0.8181, Val: 0.6983, Test: 0.6792\n",
      "Epoch: 072, Loss: 0.8154, Val: 0.6997, Test: 0.6809\n",
      "Epoch: 073, Loss: 0.8146, Val: 0.6983, Test: 0.6791\n",
      "Epoch: 074, Loss: 0.8085, Val: 0.6988, Test: 0.6793\n",
      "Epoch: 075, Loss: 0.8164, Val: 0.6995, Test: 0.6808\n",
      "Epoch: 076, Loss: 0.8099, Val: 0.7010, Test: 0.6822\n",
      "Epoch: 077, Loss: 0.8133, Val: 0.6997, Test: 0.6819\n",
      "Epoch: 078, Loss: 0.8111, Val: 0.7002, Test: 0.6834\n",
      "Epoch: 079, Loss: 0.8110, Val: 0.7015, Test: 0.6850\n",
      "Epoch: 080, Loss: 0.8120, Val: 0.7003, Test: 0.6823\n",
      "Epoch: 081, Loss: 0.8130, Val: 0.7023, Test: 0.6855\n",
      "Epoch: 082, Loss: 0.8105, Val: 0.7042, Test: 0.6877\n",
      "Epoch: 083, Loss: 0.8069, Val: 0.7021, Test: 0.6838\n",
      "Epoch: 084, Loss: 0.8137, Val: 0.7029, Test: 0.6851\n",
      "Epoch: 085, Loss: 0.8103, Val: 0.7039, Test: 0.6892\n",
      "Epoch: 086, Loss: 0.8086, Val: 0.7031, Test: 0.6864\n",
      "Epoch: 087, Loss: 0.8115, Val: 0.7027, Test: 0.6854\n",
      "Epoch: 088, Loss: 0.8071, Val: 0.7039, Test: 0.6884\n",
      "Epoch: 089, Loss: 0.8096, Val: 0.7043, Test: 0.6879\n",
      "Epoch: 090, Loss: 0.8072, Val: 0.7029, Test: 0.6864\n",
      "Epoch: 091, Loss: 0.8099, Val: 0.7038, Test: 0.6884\n",
      "Epoch: 092, Loss: 0.8049, Val: 0.7024, Test: 0.6853\n",
      "Epoch: 093, Loss: 0.8101, Val: 0.7042, Test: 0.6881\n",
      "Epoch: 094, Loss: 0.8083, Val: 0.7053, Test: 0.6898\n",
      "Epoch: 095, Loss: 0.8069, Val: 0.7024, Test: 0.6865\n",
      "Epoch: 096, Loss: 0.8093, Val: 0.7035, Test: 0.6870\n",
      "Epoch: 097, Loss: 0.8023, Val: 0.7045, Test: 0.6896\n",
      "Epoch: 098, Loss: 0.8074, Val: 0.7030, Test: 0.6871\n",
      "Epoch: 099, Loss: 0.8045, Val: 0.7014, Test: 0.6850\n",
      "Epoch: 100, Loss: 0.8061, Val: 0.7028, Test: 0.6879\n",
      "Epoch: 101, Loss: 0.8078, Val: 0.7038, Test: 0.6895\n",
      "Epoch: 102, Loss: 0.7990, Val: 0.7034, Test: 0.6885\n",
      "Epoch: 103, Loss: 0.8086, Val: 0.7021, Test: 0.6867\n",
      "Epoch: 104, Loss: 0.8065, Val: 0.7044, Test: 0.6897\n",
      "Epoch: 105, Loss: 0.8074, Val: 0.7042, Test: 0.6896\n",
      "Epoch: 106, Loss: 0.8026, Val: 0.7029, Test: 0.6883\n",
      "Epoch: 107, Loss: 0.8053, Val: 0.7033, Test: 0.6884\n",
      "Epoch: 108, Loss: 0.8036, Val: 0.7030, Test: 0.6885\n",
      "Epoch: 109, Loss: 0.8082, Val: 0.7023, Test: 0.6868\n",
      "Epoch: 110, Loss: 0.8044, Val: 0.7040, Test: 0.6887\n",
      "Epoch: 111, Loss: 0.8060, Val: 0.7041, Test: 0.6894\n",
      "Epoch: 112, Loss: 0.8035, Val: 0.7013, Test: 0.6856\n",
      "Epoch: 113, Loss: 0.8045, Val: 0.7048, Test: 0.6896\n",
      "Epoch: 114, Loss: 0.8054, Val: 0.7055, Test: 0.6909\n",
      "Epoch: 115, Loss: 0.8048, Val: 0.7031, Test: 0.6867\n",
      "Epoch: 116, Loss: 0.8014, Val: 0.7026, Test: 0.6857\n",
      "Epoch: 117, Loss: 0.8037, Val: 0.7033, Test: 0.6893\n",
      "Epoch: 118, Loss: 0.8038, Val: 0.7030, Test: 0.6878\n",
      "Epoch: 119, Loss: 0.8030, Val: 0.7037, Test: 0.6885\n",
      "Epoch: 120, Loss: 0.8012, Val: 0.7029, Test: 0.6870\n",
      "Epoch: 121, Loss: 0.8026, Val: 0.7041, Test: 0.6895\n",
      "Epoch: 122, Loss: 0.8065, Val: 0.7054, Test: 0.6905\n",
      "Epoch: 123, Loss: 0.8007, Val: 0.7031, Test: 0.6880\n",
      "Epoch: 124, Loss: 0.8074, Val: 0.7048, Test: 0.6900\n",
      "Epoch: 125, Loss: 0.8038, Val: 0.7050, Test: 0.6893\n",
      "Epoch: 126, Loss: 0.8010, Val: 0.7029, Test: 0.6875\n",
      "Epoch: 127, Loss: 0.8010, Val: 0.7033, Test: 0.6881\n",
      "Epoch: 128, Loss: 0.8030, Val: 0.7031, Test: 0.6884\n",
      "Epoch: 129, Loss: 0.8022, Val: 0.7038, Test: 0.6884\n",
      "Epoch: 130, Loss: 0.8040, Val: 0.7044, Test: 0.6903\n",
      "Epoch: 131, Loss: 0.8057, Val: 0.7034, Test: 0.6885\n",
      "Epoch: 132, Loss: 0.8029, Val: 0.7038, Test: 0.6877\n",
      "Epoch: 133, Loss: 0.8030, Val: 0.7051, Test: 0.6894\n",
      "Epoch: 134, Loss: 0.7986, Val: 0.7039, Test: 0.6890\n",
      "Epoch: 135, Loss: 0.7999, Val: 0.7027, Test: 0.6877\n",
      "Epoch: 136, Loss: 0.8042, Val: 0.7056, Test: 0.6919\n",
      "Epoch: 137, Loss: 0.8024, Val: 0.7043, Test: 0.6895\n",
      "Epoch: 138, Loss: 0.8012, Val: 0.7036, Test: 0.6890\n",
      "Epoch: 139, Loss: 0.8013, Val: 0.7053, Test: 0.6914\n",
      "Epoch: 140, Loss: 0.8018, Val: 0.7053, Test: 0.6908\n",
      "Epoch: 141, Loss: 0.8000, Val: 0.7041, Test: 0.6900\n",
      "Epoch: 142, Loss: 0.8009, Val: 0.7041, Test: 0.6906\n",
      "Epoch: 143, Loss: 0.7983, Val: 0.7057, Test: 0.6918\n",
      "Epoch: 144, Loss: 0.7998, Val: 0.7070, Test: 0.6924\n",
      "Epoch: 145, Loss: 0.8010, Val: 0.7036, Test: 0.6887\n",
      "Epoch: 146, Loss: 0.8021, Val: 0.7039, Test: 0.6901\n",
      "Epoch: 147, Loss: 0.7985, Val: 0.7060, Test: 0.6931\n",
      "Epoch: 148, Loss: 0.8027, Val: 0.7051, Test: 0.6911\n",
      "Epoch: 149, Loss: 0.7988, Val: 0.7031, Test: 0.6893\n",
      "Epoch: 150, Loss: 0.7998, Val: 0.7057, Test: 0.6923\n",
      "Epoch: 151, Loss: 0.8008, Val: 0.7048, Test: 0.6906\n",
      "Epoch: 152, Loss: 0.7983, Val: 0.7026, Test: 0.6892\n",
      "Epoch: 153, Loss: 0.8023, Val: 0.7059, Test: 0.6922\n",
      "Epoch: 154, Loss: 0.7981, Val: 0.7045, Test: 0.6896\n",
      "Epoch: 155, Loss: 0.7992, Val: 0.7034, Test: 0.6905\n",
      "Epoch: 156, Loss: 0.8001, Val: 0.7046, Test: 0.6911\n",
      "Epoch: 157, Loss: 0.7999, Val: 0.7044, Test: 0.6898\n",
      "Epoch: 158, Loss: 0.8005, Val: 0.7045, Test: 0.6910\n",
      "Epoch: 159, Loss: 0.7989, Val: 0.7050, Test: 0.6920\n",
      "Epoch: 160, Loss: 0.7960, Val: 0.7052, Test: 0.6912\n",
      "Epoch: 161, Loss: 0.7976, Val: 0.7059, Test: 0.6916\n",
      "Epoch: 162, Loss: 0.8003, Val: 0.7042, Test: 0.6907\n",
      "Epoch: 163, Loss: 0.7979, Val: 0.7056, Test: 0.6932\n",
      "Epoch: 164, Loss: 0.7981, Val: 0.7058, Test: 0.6929\n",
      "Epoch: 165, Loss: 0.8025, Val: 0.7060, Test: 0.6926\n",
      "Epoch: 166, Loss: 0.7965, Val: 0.7065, Test: 0.6944\n",
      "Epoch: 167, Loss: 0.7973, Val: 0.7064, Test: 0.6931\n",
      "Epoch: 168, Loss: 0.7936, Val: 0.7058, Test: 0.6922\n",
      "Epoch: 169, Loss: 0.7987, Val: 0.7064, Test: 0.6944\n",
      "Epoch: 170, Loss: 0.7987, Val: 0.7051, Test: 0.6917\n",
      "Epoch: 171, Loss: 0.7978, Val: 0.7071, Test: 0.6938\n",
      "Epoch: 172, Loss: 0.7965, Val: 0.7071, Test: 0.6955\n",
      "Epoch: 173, Loss: 0.7958, Val: 0.7065, Test: 0.6936\n",
      "Epoch: 174, Loss: 0.7969, Val: 0.7058, Test: 0.6930\n",
      "Epoch: 175, Loss: 0.7954, Val: 0.7085, Test: 0.6959\n",
      "Epoch: 176, Loss: 0.7959, Val: 0.7082, Test: 0.6965\n",
      "Epoch: 177, Loss: 0.7979, Val: 0.7052, Test: 0.6921\n",
      "Epoch: 178, Loss: 0.7983, Val: 0.7061, Test: 0.6934\n",
      "Epoch: 179, Loss: 0.7981, Val: 0.7088, Test: 0.6970\n",
      "Epoch: 180, Loss: 0.7960, Val: 0.7042, Test: 0.6913\n",
      "Epoch: 181, Loss: 0.8003, Val: 0.7041, Test: 0.6922\n",
      "Epoch: 182, Loss: 0.7952, Val: 0.7080, Test: 0.6964\n",
      "Epoch: 183, Loss: 0.7950, Val: 0.7083, Test: 0.6958\n",
      "Epoch: 184, Loss: 0.7945, Val: 0.7043, Test: 0.6923\n",
      "Epoch: 185, Loss: 0.7968, Val: 0.7062, Test: 0.6930\n",
      "Epoch: 186, Loss: 0.7929, Val: 0.7080, Test: 0.6961\n",
      "Epoch: 187, Loss: 0.7947, Val: 0.7068, Test: 0.6930\n",
      "Epoch: 188, Loss: 0.7949, Val: 0.7067, Test: 0.6938\n",
      "Epoch: 189, Loss: 0.7914, Val: 0.7073, Test: 0.6945\n",
      "Epoch: 190, Loss: 0.7944, Val: 0.7079, Test: 0.6947\n",
      "Epoch: 191, Loss: 0.7957, Val: 0.7078, Test: 0.6956\n",
      "Epoch: 192, Loss: 0.7955, Val: 0.7077, Test: 0.6952\n",
      "Epoch: 193, Loss: 0.7970, Val: 0.7077, Test: 0.6945\n",
      "Epoch: 194, Loss: 0.7946, Val: 0.7086, Test: 0.6952\n",
      "Epoch: 195, Loss: 0.7915, Val: 0.7096, Test: 0.6973\n",
      "Epoch: 196, Loss: 0.7969, Val: 0.7084, Test: 0.6953\n",
      "Epoch: 197, Loss: 0.7936, Val: 0.7072, Test: 0.6945\n",
      "Epoch: 198, Loss: 0.7927, Val: 0.7082, Test: 0.6969\n",
      "Epoch: 199, Loss: 0.7964, Val: 0.7083, Test: 0.6968\n",
      "Epoch: 200, Loss: 0.7968, Val: 0.7075, Test: 0.6940\n",
      "Epoch: 201, Loss: 0.7969, Val: 0.7074, Test: 0.6956\n",
      "Epoch: 202, Loss: 0.7940, Val: 0.7065, Test: 0.6928\n",
      "Epoch: 203, Loss: 0.7944, Val: 0.7087, Test: 0.6963\n",
      "Epoch: 204, Loss: 0.7917, Val: 0.7073, Test: 0.6951\n",
      "Epoch: 205, Loss: 0.7894, Val: 0.7036, Test: 0.6901\n",
      "Epoch: 206, Loss: 0.7912, Val: 0.7065, Test: 0.6925\n",
      "Epoch: 207, Loss: 0.7955, Val: 0.7075, Test: 0.6961\n",
      "Epoch: 208, Loss: 0.7957, Val: 0.7054, Test: 0.6923\n",
      "Epoch: 209, Loss: 0.7935, Val: 0.7044, Test: 0.6902\n",
      "Epoch: 210, Loss: 0.7945, Val: 0.7066, Test: 0.6949\n",
      "Epoch: 211, Loss: 0.7900, Val: 0.7066, Test: 0.6939\n",
      "Epoch: 212, Loss: 0.7922, Val: 0.7052, Test: 0.6914\n",
      "Epoch: 213, Loss: 0.7971, Val: 0.7066, Test: 0.6928\n",
      "Epoch: 214, Loss: 0.7897, Val: 0.7086, Test: 0.6956\n",
      "Epoch: 215, Loss: 0.7976, Val: 0.7070, Test: 0.6937\n",
      "Epoch: 216, Loss: 0.7982, Val: 0.7060, Test: 0.6912\n",
      "Epoch: 217, Loss: 0.7929, Val: 0.7093, Test: 0.6961\n",
      "Epoch: 218, Loss: 0.7911, Val: 0.7090, Test: 0.6978\n",
      "Epoch: 219, Loss: 0.7889, Val: 0.7068, Test: 0.6916\n",
      "Epoch: 220, Loss: 0.7919, Val: 0.7082, Test: 0.6960\n",
      "Epoch: 221, Loss: 0.7965, Val: 0.7084, Test: 0.6965\n",
      "Epoch: 222, Loss: 0.7953, Val: 0.7078, Test: 0.6950\n",
      "Epoch: 223, Loss: 0.7946, Val: 0.7089, Test: 0.6960\n",
      "Epoch: 224, Loss: 0.7950, Val: 0.7075, Test: 0.6964\n",
      "Epoch: 225, Loss: 0.7925, Val: 0.7073, Test: 0.6950\n",
      "Epoch: 226, Loss: 0.7924, Val: 0.7104, Test: 0.6984\n",
      "Epoch: 227, Loss: 0.7879, Val: 0.7070, Test: 0.6940\n",
      "Epoch: 228, Loss: 0.7936, Val: 0.7068, Test: 0.6942\n",
      "Epoch: 229, Loss: 0.7912, Val: 0.7088, Test: 0.6972\n",
      "Epoch: 230, Loss: 0.7895, Val: 0.7074, Test: 0.6936\n",
      "Epoch: 231, Loss: 0.7923, Val: 0.7083, Test: 0.6953\n",
      "Epoch: 232, Loss: 0.7879, Val: 0.7087, Test: 0.6973\n",
      "Epoch: 233, Loss: 0.7921, Val: 0.7083, Test: 0.6964\n",
      "Epoch: 234, Loss: 0.7918, Val: 0.7067, Test: 0.6920\n",
      "Epoch: 235, Loss: 0.7924, Val: 0.7089, Test: 0.6985\n",
      "Epoch: 236, Loss: 0.7934, Val: 0.7075, Test: 0.6953\n",
      "Epoch: 237, Loss: 0.7918, Val: 0.7066, Test: 0.6938\n",
      "Epoch: 238, Loss: 0.7900, Val: 0.7092, Test: 0.6982\n",
      "Epoch: 239, Loss: 0.7976, Val: 0.7066, Test: 0.6939\n",
      "Epoch: 240, Loss: 0.7921, Val: 0.7082, Test: 0.6962\n",
      "Epoch: 241, Loss: 0.7902, Val: 0.7101, Test: 0.6981\n",
      "Epoch: 242, Loss: 0.7924, Val: 0.7083, Test: 0.6948\n",
      "Epoch: 243, Loss: 0.7918, Val: 0.7082, Test: 0.6947\n",
      "Epoch: 244, Loss: 0.7887, Val: 0.7084, Test: 0.6970\n",
      "Epoch: 245, Loss: 0.7919, Val: 0.7075, Test: 0.6946\n",
      "Epoch: 246, Loss: 0.7907, Val: 0.7090, Test: 0.6970\n",
      "Epoch: 247, Loss: 0.7912, Val: 0.7056, Test: 0.6916\n",
      "Epoch: 248, Loss: 0.7903, Val: 0.7074, Test: 0.6949\n",
      "Epoch: 249, Loss: 0.7888, Val: 0.7088, Test: 0.6969\n",
      "Epoch: 250, Loss: 0.7916, Val: 0.7057, Test: 0.6921\n",
      "Epoch: 251, Loss: 0.7929, Val: 0.7096, Test: 0.6989\n",
      "Epoch: 252, Loss: 0.7898, Val: 0.7087, Test: 0.6970\n",
      "Epoch: 253, Loss: 0.7888, Val: 0.7066, Test: 0.6925\n",
      "Epoch: 254, Loss: 0.7927, Val: 0.7093, Test: 0.6976\n",
      "Epoch: 255, Loss: 0.7953, Val: 0.7084, Test: 0.6951\n",
      "Epoch: 256, Loss: 0.7894, Val: 0.7071, Test: 0.6944\n",
      "Epoch: 257, Loss: 0.7893, Val: 0.7095, Test: 0.6989\n",
      "Epoch: 258, Loss: 0.7917, Val: 0.7067, Test: 0.6927\n",
      "Epoch: 259, Loss: 0.7889, Val: 0.7078, Test: 0.6957\n",
      "Epoch: 260, Loss: 0.7934, Val: 0.7090, Test: 0.6991\n",
      "Epoch: 261, Loss: 0.7914, Val: 0.7080, Test: 0.6950\n",
      "Epoch: 262, Loss: 0.7904, Val: 0.7080, Test: 0.6949\n",
      "Epoch: 263, Loss: 0.7893, Val: 0.7095, Test: 0.6983\n",
      "Epoch: 264, Loss: 0.7923, Val: 0.7075, Test: 0.6955\n",
      "Epoch: 265, Loss: 0.7895, Val: 0.7084, Test: 0.6952\n",
      "Epoch: 266, Loss: 0.7870, Val: 0.7105, Test: 0.6992\n",
      "Epoch: 267, Loss: 0.7873, Val: 0.7073, Test: 0.6947\n",
      "Epoch: 268, Loss: 0.7880, Val: 0.7071, Test: 0.6933\n",
      "Epoch: 269, Loss: 0.7925, Val: 0.7092, Test: 0.6981\n",
      "Epoch: 270, Loss: 0.7905, Val: 0.7080, Test: 0.6945\n",
      "Epoch: 271, Loss: 0.7904, Val: 0.7082, Test: 0.6951\n",
      "Epoch: 272, Loss: 0.7849, Val: 0.7088, Test: 0.6965\n",
      "Epoch: 273, Loss: 0.7909, Val: 0.7081, Test: 0.6942\n",
      "Epoch: 274, Loss: 0.7903, Val: 0.7080, Test: 0.6956\n",
      "Epoch: 275, Loss: 0.7910, Val: 0.7092, Test: 0.6965\n",
      "Epoch: 276, Loss: 0.7897, Val: 0.7069, Test: 0.6937\n",
      "Epoch: 277, Loss: 0.7907, Val: 0.7095, Test: 0.6982\n",
      "Epoch: 278, Loss: 0.7907, Val: 0.7080, Test: 0.6951\n",
      "Epoch: 279, Loss: 0.7918, Val: 0.7073, Test: 0.6956\n",
      "Epoch: 280, Loss: 0.7876, Val: 0.7105, Test: 0.6994\n",
      "Epoch: 281, Loss: 0.7855, Val: 0.7089, Test: 0.6981\n",
      "Epoch: 282, Loss: 0.7910, Val: 0.7079, Test: 0.6964\n",
      "Epoch: 283, Loss: 0.7912, Val: 0.7091, Test: 0.6966\n",
      "Epoch: 284, Loss: 0.7899, Val: 0.7088, Test: 0.6968\n",
      "Epoch: 285, Loss: 0.7866, Val: 0.7095, Test: 0.6982\n",
      "Epoch: 286, Loss: 0.7884, Val: 0.7086, Test: 0.6966\n",
      "Epoch: 287, Loss: 0.7891, Val: 0.7091, Test: 0.6964\n",
      "Epoch: 288, Loss: 0.7879, Val: 0.7093, Test: 0.6971\n",
      "Epoch: 289, Loss: 0.7848, Val: 0.7091, Test: 0.6978\n",
      "Epoch: 290, Loss: 0.7890, Val: 0.7089, Test: 0.6959\n",
      "Epoch: 291, Loss: 0.7870, Val: 0.7083, Test: 0.6957\n",
      "Epoch: 292, Loss: 0.7899, Val: 0.7105, Test: 0.6977\n",
      "Epoch: 293, Loss: 0.7874, Val: 0.7095, Test: 0.6972\n",
      "Epoch: 294, Loss: 0.7899, Val: 0.7068, Test: 0.6951\n",
      "Epoch: 295, Loss: 0.7873, Val: 0.7101, Test: 0.6996\n",
      "Epoch: 296, Loss: 0.7913, Val: 0.7075, Test: 0.6939\n",
      "Epoch: 297, Loss: 0.7909, Val: 0.7083, Test: 0.6959\n",
      "Epoch: 298, Loss: 0.7870, Val: 0.7097, Test: 0.7000\n",
      "Epoch: 299, Loss: 0.7895, Val: 0.7081, Test: 0.6934\n",
      "Epoch: 300, Loss: 0.7901, Val: 0.7081, Test: 0.6953\n",
      "0.2\n",
      "Label_rate: 0.2\n",
      "Epoch: 001, Loss: 1.8981, Val: 0.4691, Test: 0.4329\n",
      "Epoch: 002, Loss: 1.8076, Val: 0.5472, Test: 0.5226\n",
      "Epoch: 003, Loss: 1.4433, Val: 0.5271, Test: 0.5139\n",
      "Epoch: 004, Loss: 1.6210, Val: 0.5935, Test: 0.5797\n",
      "Epoch: 005, Loss: 1.2939, Val: 0.6204, Test: 0.5982\n",
      "Epoch: 006, Loss: 1.2389, Val: 0.6257, Test: 0.5998\n",
      "Epoch: 007, Loss: 1.1983, Val: 0.6309, Test: 0.6063\n",
      "Epoch: 008, Loss: 1.2250, Val: 0.6430, Test: 0.6203\n",
      "Epoch: 009, Loss: 1.1895, Val: 0.6533, Test: 0.6335\n",
      "Epoch: 010, Loss: 1.1718, Val: 0.6543, Test: 0.6295\n",
      "Epoch: 011, Loss: 1.1379, Val: 0.6512, Test: 0.6211\n",
      "Epoch: 012, Loss: 1.1273, Val: 0.6690, Test: 0.6427\n",
      "Epoch: 013, Loss: 1.0902, Val: 0.6836, Test: 0.6685\n",
      "Epoch: 014, Loss: 1.0683, Val: 0.6877, Test: 0.6751\n",
      "Epoch: 015, Loss: 1.0530, Val: 0.6878, Test: 0.6702\n",
      "Epoch: 016, Loss: 1.0486, Val: 0.6847, Test: 0.6658\n",
      "Epoch: 017, Loss: 1.0387, Val: 0.6848, Test: 0.6679\n",
      "Epoch: 018, Loss: 1.0240, Val: 0.6889, Test: 0.6721\n",
      "Epoch: 019, Loss: 1.0106, Val: 0.6906, Test: 0.6726\n",
      "Epoch: 020, Loss: 0.9896, Val: 0.6932, Test: 0.6759\n",
      "Epoch: 021, Loss: 0.9913, Val: 0.6960, Test: 0.6816\n",
      "Epoch: 022, Loss: 0.9750, Val: 0.6947, Test: 0.6807\n",
      "Epoch: 023, Loss: 0.9676, Val: 0.6923, Test: 0.6771\n",
      "Epoch: 024, Loss: 0.9659, Val: 0.6918, Test: 0.6740\n",
      "Epoch: 025, Loss: 0.9536, Val: 0.6940, Test: 0.6780\n",
      "Epoch: 026, Loss: 0.9525, Val: 0.6980, Test: 0.6851\n",
      "Epoch: 027, Loss: 0.9499, Val: 0.7025, Test: 0.6921\n",
      "Epoch: 028, Loss: 0.9448, Val: 0.7034, Test: 0.6930\n",
      "Epoch: 029, Loss: 0.9377, Val: 0.7005, Test: 0.6908\n",
      "Epoch: 030, Loss: 0.9280, Val: 0.6989, Test: 0.6885\n",
      "Epoch: 031, Loss: 0.9252, Val: 0.7011, Test: 0.6909\n",
      "Epoch: 032, Loss: 0.9136, Val: 0.7033, Test: 0.6934\n",
      "Epoch: 033, Loss: 0.9136, Val: 0.7028, Test: 0.6929\n",
      "Epoch: 034, Loss: 0.9040, Val: 0.7017, Test: 0.6899\n",
      "Epoch: 035, Loss: 0.9030, Val: 0.7015, Test: 0.6889\n",
      "Epoch: 036, Loss: 0.8987, Val: 0.7042, Test: 0.6917\n",
      "Epoch: 037, Loss: 0.8942, Val: 0.7047, Test: 0.6944\n",
      "Epoch: 038, Loss: 0.8945, Val: 0.7046, Test: 0.6931\n",
      "Epoch: 039, Loss: 0.8930, Val: 0.7015, Test: 0.6898\n",
      "Epoch: 040, Loss: 0.8893, Val: 0.7006, Test: 0.6879\n",
      "Epoch: 041, Loss: 0.8896, Val: 0.7027, Test: 0.6897\n",
      "Epoch: 042, Loss: 0.8800, Val: 0.7050, Test: 0.6922\n",
      "Epoch: 043, Loss: 0.8811, Val: 0.7051, Test: 0.6917\n",
      "Epoch: 044, Loss: 0.8791, Val: 0.7030, Test: 0.6881\n",
      "Epoch: 045, Loss: 0.8753, Val: 0.7015, Test: 0.6863\n",
      "Epoch: 046, Loss: 0.8739, Val: 0.7022, Test: 0.6876\n",
      "Epoch: 047, Loss: 0.8711, Val: 0.7050, Test: 0.6912\n",
      "Epoch: 048, Loss: 0.8728, Val: 0.7055, Test: 0.6914\n",
      "Epoch: 049, Loss: 0.8659, Val: 0.7044, Test: 0.6906\n",
      "Epoch: 050, Loss: 0.8628, Val: 0.7044, Test: 0.6908\n",
      "Epoch: 051, Loss: 0.8671, Val: 0.7065, Test: 0.6934\n",
      "Epoch: 052, Loss: 0.8591, Val: 0.7074, Test: 0.6928\n",
      "Epoch: 053, Loss: 0.8547, Val: 0.7063, Test: 0.6917\n",
      "Epoch: 054, Loss: 0.8546, Val: 0.7063, Test: 0.6910\n",
      "Epoch: 055, Loss: 0.8551, Val: 0.7061, Test: 0.6912\n",
      "Epoch: 056, Loss: 0.8538, Val: 0.7069, Test: 0.6928\n",
      "Epoch: 057, Loss: 0.8515, Val: 0.7081, Test: 0.6936\n",
      "Epoch: 058, Loss: 0.8531, Val: 0.7058, Test: 0.6913\n",
      "Epoch: 059, Loss: 0.8487, Val: 0.7035, Test: 0.6877\n",
      "Epoch: 060, Loss: 0.8526, Val: 0.7048, Test: 0.6893\n",
      "Epoch: 061, Loss: 0.8532, Val: 0.7072, Test: 0.6921\n",
      "Epoch: 062, Loss: 0.8471, Val: 0.7092, Test: 0.6942\n",
      "Epoch: 063, Loss: 0.8462, Val: 0.7087, Test: 0.6938\n",
      "Epoch: 064, Loss: 0.8422, Val: 0.7072, Test: 0.6925\n",
      "Epoch: 065, Loss: 0.8428, Val: 0.7069, Test: 0.6922\n",
      "Epoch: 066, Loss: 0.8422, Val: 0.7087, Test: 0.6929\n",
      "Epoch: 067, Loss: 0.8395, Val: 0.7098, Test: 0.6936\n",
      "Epoch: 068, Loss: 0.8407, Val: 0.7089, Test: 0.6946\n",
      "Epoch: 069, Loss: 0.8419, Val: 0.7088, Test: 0.6953\n",
      "Epoch: 070, Loss: 0.8404, Val: 0.7090, Test: 0.6951\n",
      "Epoch: 071, Loss: 0.8405, Val: 0.7093, Test: 0.6956\n",
      "Epoch: 072, Loss: 0.8351, Val: 0.7099, Test: 0.6951\n",
      "Epoch: 073, Loss: 0.8315, Val: 0.7096, Test: 0.6952\n",
      "Epoch: 074, Loss: 0.8378, Val: 0.7098, Test: 0.6951\n",
      "Epoch: 075, Loss: 0.8312, Val: 0.7092, Test: 0.6940\n",
      "Epoch: 076, Loss: 0.8358, Val: 0.7090, Test: 0.6945\n",
      "Epoch: 077, Loss: 0.8284, Val: 0.7092, Test: 0.6957\n",
      "Epoch: 078, Loss: 0.8358, Val: 0.7101, Test: 0.6962\n",
      "Epoch: 079, Loss: 0.8320, Val: 0.7105, Test: 0.6965\n",
      "Epoch: 080, Loss: 0.8321, Val: 0.7099, Test: 0.6949\n",
      "Epoch: 081, Loss: 0.8325, Val: 0.7100, Test: 0.6949\n",
      "Epoch: 082, Loss: 0.8320, Val: 0.7107, Test: 0.6970\n",
      "Epoch: 083, Loss: 0.8317, Val: 0.7115, Test: 0.6990\n",
      "Epoch: 084, Loss: 0.8274, Val: 0.7116, Test: 0.6977\n",
      "Epoch: 085, Loss: 0.8315, Val: 0.7108, Test: 0.6955\n",
      "Epoch: 086, Loss: 0.8251, Val: 0.7104, Test: 0.6944\n",
      "Epoch: 087, Loss: 0.8266, Val: 0.7106, Test: 0.6949\n",
      "Epoch: 088, Loss: 0.8242, Val: 0.7112, Test: 0.6978\n",
      "Epoch: 089, Loss: 0.8272, Val: 0.7120, Test: 0.6993\n",
      "Epoch: 090, Loss: 0.8260, Val: 0.7115, Test: 0.6982\n",
      "Epoch: 091, Loss: 0.8235, Val: 0.7108, Test: 0.6965\n",
      "Epoch: 092, Loss: 0.8222, Val: 0.7114, Test: 0.6970\n",
      "Epoch: 093, Loss: 0.8203, Val: 0.7114, Test: 0.6968\n",
      "Epoch: 094, Loss: 0.8232, Val: 0.7114, Test: 0.6970\n",
      "Epoch: 095, Loss: 0.8213, Val: 0.7121, Test: 0.6983\n",
      "Epoch: 096, Loss: 0.8256, Val: 0.7118, Test: 0.6985\n",
      "Epoch: 097, Loss: 0.8265, Val: 0.7113, Test: 0.6983\n",
      "Epoch: 098, Loss: 0.8178, Val: 0.7125, Test: 0.7005\n",
      "Epoch: 099, Loss: 0.8185, Val: 0.7124, Test: 0.6987\n",
      "Epoch: 100, Loss: 0.8189, Val: 0.7115, Test: 0.6964\n",
      "Epoch: 101, Loss: 0.8232, Val: 0.7109, Test: 0.6966\n",
      "Epoch: 102, Loss: 0.8163, Val: 0.7125, Test: 0.6994\n",
      "Epoch: 103, Loss: 0.8196, Val: 0.7127, Test: 0.6993\n",
      "Epoch: 104, Loss: 0.8150, Val: 0.7127, Test: 0.6986\n",
      "Epoch: 105, Loss: 0.8198, Val: 0.7115, Test: 0.6969\n",
      "Epoch: 106, Loss: 0.8205, Val: 0.7118, Test: 0.6998\n",
      "Epoch: 107, Loss: 0.8177, Val: 0.7121, Test: 0.6985\n",
      "Epoch: 108, Loss: 0.8154, Val: 0.7121, Test: 0.6974\n",
      "Epoch: 109, Loss: 0.8149, Val: 0.7122, Test: 0.6991\n",
      "Epoch: 110, Loss: 0.8182, Val: 0.7133, Test: 0.7010\n",
      "Epoch: 111, Loss: 0.8157, Val: 0.7124, Test: 0.6996\n",
      "Epoch: 112, Loss: 0.8191, Val: 0.7124, Test: 0.6979\n",
      "Epoch: 113, Loss: 0.8192, Val: 0.7126, Test: 0.6994\n",
      "Epoch: 114, Loss: 0.8179, Val: 0.7133, Test: 0.7009\n",
      "Epoch: 115, Loss: 0.8127, Val: 0.7124, Test: 0.7002\n",
      "Epoch: 116, Loss: 0.8147, Val: 0.7113, Test: 0.6974\n",
      "Epoch: 117, Loss: 0.8172, Val: 0.7116, Test: 0.6967\n",
      "Epoch: 118, Loss: 0.8126, Val: 0.7123, Test: 0.6985\n",
      "Epoch: 119, Loss: 0.8148, Val: 0.7125, Test: 0.6993\n",
      "Epoch: 120, Loss: 0.8169, Val: 0.7135, Test: 0.6993\n",
      "Epoch: 121, Loss: 0.8139, Val: 0.7129, Test: 0.6979\n",
      "Epoch: 122, Loss: 0.8146, Val: 0.7122, Test: 0.6976\n",
      "Epoch: 123, Loss: 0.8160, Val: 0.7127, Test: 0.6988\n",
      "Epoch: 124, Loss: 0.8109, Val: 0.7131, Test: 0.6999\n",
      "Epoch: 125, Loss: 0.8121, Val: 0.7119, Test: 0.6964\n",
      "Epoch: 126, Loss: 0.8152, Val: 0.7119, Test: 0.6965\n",
      "Epoch: 127, Loss: 0.8095, Val: 0.7128, Test: 0.6985\n",
      "Epoch: 128, Loss: 0.8095, Val: 0.7133, Test: 0.6993\n",
      "Epoch: 129, Loss: 0.8138, Val: 0.7130, Test: 0.6984\n",
      "Epoch: 130, Loss: 0.8095, Val: 0.7123, Test: 0.6978\n",
      "Epoch: 131, Loss: 0.8107, Val: 0.7126, Test: 0.6992\n",
      "Epoch: 132, Loss: 0.8122, Val: 0.7133, Test: 0.6997\n",
      "Epoch: 133, Loss: 0.8097, Val: 0.7131, Test: 0.6984\n",
      "Epoch: 134, Loss: 0.8086, Val: 0.7134, Test: 0.6990\n",
      "Epoch: 135, Loss: 0.8068, Val: 0.7141, Test: 0.7012\n",
      "Epoch: 136, Loss: 0.8126, Val: 0.7140, Test: 0.7011\n",
      "Epoch: 137, Loss: 0.8099, Val: 0.7131, Test: 0.7000\n",
      "Epoch: 138, Loss: 0.8069, Val: 0.7141, Test: 0.7002\n",
      "Epoch: 139, Loss: 0.8063, Val: 0.7147, Test: 0.7020\n",
      "Epoch: 140, Loss: 0.8105, Val: 0.7135, Test: 0.7006\n",
      "Epoch: 141, Loss: 0.8111, Val: 0.7125, Test: 0.6991\n",
      "Epoch: 142, Loss: 0.8100, Val: 0.7137, Test: 0.7007\n",
      "Epoch: 143, Loss: 0.8067, Val: 0.7146, Test: 0.7032\n",
      "Epoch: 144, Loss: 0.8072, Val: 0.7146, Test: 0.7020\n",
      "Epoch: 145, Loss: 0.8097, Val: 0.7139, Test: 0.7003\n",
      "Epoch: 146, Loss: 0.8036, Val: 0.7137, Test: 0.7010\n",
      "Epoch: 147, Loss: 0.8078, Val: 0.7151, Test: 0.7037\n",
      "Epoch: 148, Loss: 0.8086, Val: 0.7142, Test: 0.7018\n",
      "Epoch: 149, Loss: 0.8055, Val: 0.7119, Test: 0.6984\n",
      "Epoch: 150, Loss: 0.8015, Val: 0.7141, Test: 0.7023\n",
      "Epoch: 151, Loss: 0.8103, Val: 0.7155, Test: 0.7048\n",
      "Epoch: 152, Loss: 0.8103, Val: 0.7138, Test: 0.7028\n",
      "Epoch: 153, Loss: 0.8061, Val: 0.7121, Test: 0.6982\n",
      "Epoch: 154, Loss: 0.8094, Val: 0.7143, Test: 0.7015\n",
      "Epoch: 155, Loss: 0.8066, Val: 0.7153, Test: 0.7038\n",
      "Epoch: 156, Loss: 0.8082, Val: 0.7148, Test: 0.7031\n",
      "Epoch: 157, Loss: 0.8017, Val: 0.7142, Test: 0.7034\n",
      "Epoch: 158, Loss: 0.8061, Val: 0.7146, Test: 0.7036\n",
      "Epoch: 159, Loss: 0.8035, Val: 0.7146, Test: 0.7026\n",
      "Epoch: 160, Loss: 0.8006, Val: 0.7140, Test: 0.7017\n",
      "Epoch: 161, Loss: 0.8060, Val: 0.7140, Test: 0.7028\n",
      "Epoch: 162, Loss: 0.8027, Val: 0.7142, Test: 0.7030\n",
      "Epoch: 163, Loss: 0.8056, Val: 0.7142, Test: 0.7021\n",
      "Epoch: 164, Loss: 0.8092, Val: 0.7150, Test: 0.7020\n",
      "Epoch: 165, Loss: 0.8057, Val: 0.7141, Test: 0.7020\n",
      "Epoch: 166, Loss: 0.8025, Val: 0.7139, Test: 0.7009\n",
      "Epoch: 167, Loss: 0.8019, Val: 0.7143, Test: 0.7018\n",
      "Epoch: 168, Loss: 0.8033, Val: 0.7147, Test: 0.7035\n",
      "Epoch: 169, Loss: 0.8025, Val: 0.7138, Test: 0.7025\n",
      "Epoch: 170, Loss: 0.8042, Val: 0.7134, Test: 0.6998\n",
      "Epoch: 171, Loss: 0.7979, Val: 0.7145, Test: 0.7008\n",
      "Epoch: 172, Loss: 0.7982, Val: 0.7151, Test: 0.7035\n",
      "Epoch: 173, Loss: 0.8072, Val: 0.7147, Test: 0.7038\n",
      "Epoch: 174, Loss: 0.8023, Val: 0.7140, Test: 0.7025\n",
      "Epoch: 175, Loss: 0.8001, Val: 0.7131, Test: 0.7011\n",
      "Epoch: 176, Loss: 0.7998, Val: 0.7148, Test: 0.7022\n",
      "Epoch: 177, Loss: 0.7997, Val: 0.7153, Test: 0.7028\n",
      "Epoch: 178, Loss: 0.8013, Val: 0.7148, Test: 0.7040\n",
      "Epoch: 179, Loss: 0.8064, Val: 0.7153, Test: 0.7050\n",
      "Epoch: 180, Loss: 0.8011, Val: 0.7144, Test: 0.7021\n",
      "Epoch: 181, Loss: 0.7965, Val: 0.7141, Test: 0.7014\n",
      "Epoch: 182, Loss: 0.8058, Val: 0.7156, Test: 0.7039\n",
      "Epoch: 183, Loss: 0.7975, Val: 0.7160, Test: 0.7050\n",
      "Epoch: 184, Loss: 0.7990, Val: 0.7150, Test: 0.7041\n",
      "Epoch: 185, Loss: 0.8023, Val: 0.7146, Test: 0.7021\n",
      "Epoch: 186, Loss: 0.8056, Val: 0.7145, Test: 0.7011\n",
      "Epoch: 187, Loss: 0.8090, Val: 0.7156, Test: 0.7034\n",
      "Epoch: 188, Loss: 0.8017, Val: 0.7153, Test: 0.7056\n",
      "Epoch: 189, Loss: 0.8016, Val: 0.7151, Test: 0.7047\n",
      "Epoch: 190, Loss: 0.7997, Val: 0.7147, Test: 0.7026\n",
      "Epoch: 191, Loss: 0.8036, Val: 0.7137, Test: 0.7013\n",
      "Epoch: 192, Loss: 0.8035, Val: 0.7152, Test: 0.7047\n",
      "Epoch: 193, Loss: 0.8003, Val: 0.7156, Test: 0.7062\n",
      "Epoch: 194, Loss: 0.8040, Val: 0.7155, Test: 0.7039\n",
      "Epoch: 195, Loss: 0.8026, Val: 0.7158, Test: 0.7038\n",
      "Epoch: 196, Loss: 0.8014, Val: 0.7154, Test: 0.7033\n",
      "Epoch: 197, Loss: 0.8002, Val: 0.7159, Test: 0.7042\n",
      "Epoch: 198, Loss: 0.8001, Val: 0.7171, Test: 0.7054\n",
      "Epoch: 199, Loss: 0.7967, Val: 0.7155, Test: 0.7047\n",
      "Epoch: 200, Loss: 0.8049, Val: 0.7145, Test: 0.7045\n",
      "Epoch: 201, Loss: 0.8000, Val: 0.7145, Test: 0.7043\n",
      "Epoch: 202, Loss: 0.8016, Val: 0.7158, Test: 0.7046\n",
      "Epoch: 203, Loss: 0.8009, Val: 0.7159, Test: 0.7039\n",
      "Epoch: 204, Loss: 0.7978, Val: 0.7151, Test: 0.7031\n",
      "Epoch: 205, Loss: 0.8039, Val: 0.7155, Test: 0.7026\n",
      "Epoch: 206, Loss: 0.7988, Val: 0.7154, Test: 0.7030\n",
      "Epoch: 207, Loss: 0.8029, Val: 0.7161, Test: 0.7039\n",
      "Epoch: 208, Loss: 0.7998, Val: 0.7159, Test: 0.7045\n",
      "Epoch: 209, Loss: 0.8001, Val: 0.7146, Test: 0.7022\n",
      "Epoch: 210, Loss: 0.7970, Val: 0.7144, Test: 0.7004\n",
      "Epoch: 211, Loss: 0.8016, Val: 0.7167, Test: 0.7048\n",
      "Epoch: 212, Loss: 0.7986, Val: 0.7156, Test: 0.7044\n",
      "Epoch: 213, Loss: 0.8016, Val: 0.7136, Test: 0.6997\n",
      "Epoch: 214, Loss: 0.8018, Val: 0.7147, Test: 0.7017\n",
      "Epoch: 215, Loss: 0.7971, Val: 0.7171, Test: 0.7060\n",
      "Epoch: 216, Loss: 0.7989, Val: 0.7166, Test: 0.7053\n",
      "Epoch: 217, Loss: 0.7967, Val: 0.7144, Test: 0.7003\n",
      "Epoch: 218, Loss: 0.7969, Val: 0.7144, Test: 0.7012\n",
      "Epoch: 219, Loss: 0.7968, Val: 0.7157, Test: 0.7038\n",
      "Epoch: 220, Loss: 0.8025, Val: 0.7149, Test: 0.7004\n",
      "Epoch: 221, Loss: 0.8009, Val: 0.7144, Test: 0.6998\n",
      "Epoch: 222, Loss: 0.7960, Val: 0.7146, Test: 0.7031\n",
      "Epoch: 223, Loss: 0.8004, Val: 0.7153, Test: 0.7034\n",
      "Epoch: 224, Loss: 0.7964, Val: 0.7144, Test: 0.6987\n",
      "Epoch: 225, Loss: 0.8003, Val: 0.7148, Test: 0.7016\n",
      "Epoch: 226, Loss: 0.7930, Val: 0.7158, Test: 0.7048\n",
      "Epoch: 227, Loss: 0.7965, Val: 0.7154, Test: 0.7021\n",
      "Epoch: 228, Loss: 0.7920, Val: 0.7149, Test: 0.7001\n",
      "Epoch: 229, Loss: 0.7958, Val: 0.7163, Test: 0.7053\n",
      "Epoch: 230, Loss: 0.7972, Val: 0.7155, Test: 0.7023\n",
      "Epoch: 231, Loss: 0.7989, Val: 0.7145, Test: 0.6993\n",
      "Epoch: 232, Loss: 0.7926, Val: 0.7154, Test: 0.7033\n",
      "Epoch: 233, Loss: 0.7983, Val: 0.7167, Test: 0.7060\n",
      "Epoch: 234, Loss: 0.7989, Val: 0.7144, Test: 0.7009\n",
      "Epoch: 235, Loss: 0.7955, Val: 0.7147, Test: 0.7005\n",
      "Epoch: 236, Loss: 0.8013, Val: 0.7175, Test: 0.7063\n",
      "Epoch: 237, Loss: 0.7960, Val: 0.7164, Test: 0.7066\n",
      "Epoch: 238, Loss: 0.7974, Val: 0.7135, Test: 0.6995\n",
      "Epoch: 239, Loss: 0.7968, Val: 0.7136, Test: 0.7000\n",
      "Epoch: 240, Loss: 0.7973, Val: 0.7164, Test: 0.7046\n",
      "Epoch: 241, Loss: 0.7958, Val: 0.7156, Test: 0.7065\n",
      "Epoch: 242, Loss: 0.7975, Val: 0.7144, Test: 0.7017\n",
      "Epoch: 243, Loss: 0.7935, Val: 0.7144, Test: 0.7006\n",
      "Epoch: 244, Loss: 0.7917, Val: 0.7167, Test: 0.7069\n",
      "Epoch: 245, Loss: 0.7975, Val: 0.7172, Test: 0.7063\n",
      "Epoch: 246, Loss: 0.7949, Val: 0.7155, Test: 0.7033\n",
      "Epoch: 247, Loss: 0.7973, Val: 0.7154, Test: 0.7039\n",
      "Epoch: 248, Loss: 0.7945, Val: 0.7161, Test: 0.7043\n",
      "Epoch: 249, Loss: 0.7926, Val: 0.7169, Test: 0.7046\n",
      "Epoch: 250, Loss: 0.7951, Val: 0.7151, Test: 0.7032\n",
      "Epoch: 251, Loss: 0.7982, Val: 0.7147, Test: 0.7029\n",
      "Epoch: 252, Loss: 0.7919, Val: 0.7163, Test: 0.7040\n",
      "Epoch: 253, Loss: 0.7914, Val: 0.7167, Test: 0.7059\n",
      "Epoch: 254, Loss: 0.7929, Val: 0.7162, Test: 0.7048\n",
      "Epoch: 255, Loss: 0.7947, Val: 0.7165, Test: 0.7046\n",
      "Epoch: 256, Loss: 0.7952, Val: 0.7166, Test: 0.7038\n",
      "Epoch: 257, Loss: 0.7911, Val: 0.7178, Test: 0.7050\n",
      "Epoch: 258, Loss: 0.7915, Val: 0.7175, Test: 0.7065\n",
      "Epoch: 259, Loss: 0.7965, Val: 0.7160, Test: 0.7047\n",
      "Epoch: 260, Loss: 0.7951, Val: 0.7147, Test: 0.6996\n",
      "Epoch: 261, Loss: 0.7921, Val: 0.7170, Test: 0.7040\n",
      "Epoch: 262, Loss: 0.7931, Val: 0.7182, Test: 0.7076\n",
      "Epoch: 263, Loss: 0.7983, Val: 0.7166, Test: 0.7024\n",
      "Epoch: 264, Loss: 0.7940, Val: 0.7145, Test: 0.6991\n",
      "Epoch: 265, Loss: 0.7960, Val: 0.7161, Test: 0.7040\n",
      "Epoch: 266, Loss: 0.7990, Val: 0.7169, Test: 0.7053\n",
      "Epoch: 267, Loss: 0.7974, Val: 0.7154, Test: 0.7011\n",
      "Epoch: 268, Loss: 0.7978, Val: 0.7173, Test: 0.7058\n",
      "Epoch: 269, Loss: 0.7954, Val: 0.7168, Test: 0.7061\n",
      "Epoch: 270, Loss: 0.7952, Val: 0.7164, Test: 0.7016\n",
      "Epoch: 271, Loss: 0.7940, Val: 0.7168, Test: 0.7025\n",
      "Epoch: 272, Loss: 0.7898, Val: 0.7176, Test: 0.7071\n",
      "Epoch: 273, Loss: 0.7939, Val: 0.7155, Test: 0.7020\n",
      "Epoch: 274, Loss: 0.7879, Val: 0.7143, Test: 0.6993\n",
      "Epoch: 275, Loss: 0.7934, Val: 0.7165, Test: 0.7051\n",
      "Epoch: 276, Loss: 0.7935, Val: 0.7187, Test: 0.7063\n",
      "Epoch: 277, Loss: 0.7957, Val: 0.7158, Test: 0.7015\n",
      "Epoch: 278, Loss: 0.7889, Val: 0.7161, Test: 0.7029\n",
      "Epoch: 279, Loss: 0.7926, Val: 0.7163, Test: 0.7052\n",
      "Epoch: 280, Loss: 0.7930, Val: 0.7175, Test: 0.7040\n",
      "Epoch: 281, Loss: 0.7956, Val: 0.7176, Test: 0.7029\n",
      "Epoch: 282, Loss: 0.7897, Val: 0.7170, Test: 0.7047\n",
      "Epoch: 283, Loss: 0.7926, Val: 0.7158, Test: 0.7039\n",
      "Epoch: 284, Loss: 0.7850, Val: 0.7171, Test: 0.7045\n",
      "Epoch: 285, Loss: 0.7896, Val: 0.7179, Test: 0.7055\n",
      "Epoch: 286, Loss: 0.7883, Val: 0.7176, Test: 0.7046\n",
      "Epoch: 287, Loss: 0.7869, Val: 0.7166, Test: 0.7031\n",
      "Epoch: 288, Loss: 0.7937, Val: 0.7166, Test: 0.7039\n",
      "Epoch: 289, Loss: 0.7912, Val: 0.7169, Test: 0.7037\n",
      "Epoch: 290, Loss: 0.7922, Val: 0.7169, Test: 0.7032\n",
      "Epoch: 291, Loss: 0.7926, Val: 0.7176, Test: 0.7046\n",
      "Epoch: 292, Loss: 0.7892, Val: 0.7171, Test: 0.7039\n",
      "Epoch: 293, Loss: 0.7936, Val: 0.7165, Test: 0.7038\n",
      "Epoch: 294, Loss: 0.7899, Val: 0.7169, Test: 0.7036\n",
      "Epoch: 295, Loss: 0.7944, Val: 0.7163, Test: 0.7036\n",
      "Epoch: 296, Loss: 0.7896, Val: 0.7167, Test: 0.7050\n",
      "Epoch: 297, Loss: 0.7901, Val: 0.7166, Test: 0.7031\n",
      "Epoch: 298, Loss: 0.7882, Val: 0.7162, Test: 0.7032\n",
      "Epoch: 299, Loss: 0.7938, Val: 0.7178, Test: 0.7049\n",
      "Epoch: 300, Loss: 0.7925, Val: 0.7168, Test: 0.7041\n",
      "0.25\n",
      "Label_rate: 0.25\n",
      "Epoch: 001, Loss: 1.5469, Val: 0.4645, Test: 0.3704\n",
      "Epoch: 002, Loss: 1.2961, Val: 0.5555, Test: 0.4812\n",
      "Epoch: 003, Loss: 1.1745, Val: 0.6042, Test: 0.5600\n",
      "Epoch: 004, Loss: 1.1064, Val: 0.6316, Test: 0.5985\n",
      "Epoch: 005, Loss: 1.0693, Val: 0.6422, Test: 0.6119\n",
      "Epoch: 006, Loss: 1.0528, Val: 0.6496, Test: 0.6144\n",
      "Epoch: 007, Loss: 1.0076, Val: 0.6606, Test: 0.6286\n",
      "Epoch: 008, Loss: 0.9920, Val: 0.6681, Test: 0.6451\n",
      "Epoch: 009, Loss: 0.9751, Val: 0.6737, Test: 0.6482\n",
      "Epoch: 010, Loss: 0.9508, Val: 0.6738, Test: 0.6452\n",
      "Epoch: 011, Loss: 0.9525, Val: 0.6799, Test: 0.6541\n",
      "Epoch: 012, Loss: 0.9333, Val: 0.6862, Test: 0.6673\n",
      "Epoch: 013, Loss: 0.9270, Val: 0.6896, Test: 0.6734\n",
      "Epoch: 014, Loss: 0.9226, Val: 0.6913, Test: 0.6735\n",
      "Epoch: 015, Loss: 0.9229, Val: 0.6946, Test: 0.6752\n",
      "Epoch: 016, Loss: 0.9096, Val: 0.6983, Test: 0.6815\n",
      "Epoch: 017, Loss: 0.9002, Val: 0.7002, Test: 0.6840\n",
      "Epoch: 018, Loss: 0.8954, Val: 0.7010, Test: 0.6850\n",
      "Epoch: 019, Loss: 0.8910, Val: 0.7008, Test: 0.6859\n",
      "Epoch: 020, Loss: 0.8864, Val: 0.7022, Test: 0.6875\n",
      "Epoch: 021, Loss: 0.8813, Val: 0.7035, Test: 0.6889\n",
      "Epoch: 022, Loss: 0.8760, Val: 0.7025, Test: 0.6887\n",
      "Epoch: 023, Loss: 0.8716, Val: 0.7027, Test: 0.6895\n",
      "Epoch: 024, Loss: 0.8741, Val: 0.7050, Test: 0.6933\n",
      "Epoch: 025, Loss: 0.8711, Val: 0.7068, Test: 0.6935\n",
      "Epoch: 026, Loss: 0.8679, Val: 0.7064, Test: 0.6908\n",
      "Epoch: 027, Loss: 0.8715, Val: 0.7056, Test: 0.6881\n",
      "Epoch: 028, Loss: 0.8606, Val: 0.7065, Test: 0.6895\n",
      "Epoch: 029, Loss: 0.8584, Val: 0.7076, Test: 0.6911\n",
      "Epoch: 030, Loss: 0.8571, Val: 0.7092, Test: 0.6934\n",
      "Epoch: 031, Loss: 0.8569, Val: 0.7096, Test: 0.6936\n",
      "Epoch: 032, Loss: 0.8532, Val: 0.7078, Test: 0.6925\n",
      "Epoch: 033, Loss: 0.8497, Val: 0.7073, Test: 0.6905\n",
      "Epoch: 034, Loss: 0.8464, Val: 0.7077, Test: 0.6902\n",
      "Epoch: 035, Loss: 0.8470, Val: 0.7081, Test: 0.6915\n",
      "Epoch: 036, Loss: 0.8405, Val: 0.7085, Test: 0.6910\n",
      "Epoch: 037, Loss: 0.8417, Val: 0.7088, Test: 0.6910\n",
      "Epoch: 038, Loss: 0.8426, Val: 0.7084, Test: 0.6928\n",
      "Epoch: 039, Loss: 0.8431, Val: 0.7092, Test: 0.6946\n",
      "Epoch: 040, Loss: 0.8483, Val: 0.7078, Test: 0.6914\n",
      "Epoch: 041, Loss: 0.8383, Val: 0.7086, Test: 0.6913\n",
      "Epoch: 042, Loss: 0.8370, Val: 0.7095, Test: 0.6923\n",
      "Epoch: 043, Loss: 0.8352, Val: 0.7101, Test: 0.6937\n",
      "Epoch: 044, Loss: 0.8358, Val: 0.7098, Test: 0.6924\n",
      "Epoch: 045, Loss: 0.8368, Val: 0.7102, Test: 0.6921\n",
      "Epoch: 046, Loss: 0.8358, Val: 0.7106, Test: 0.6928\n",
      "Epoch: 047, Loss: 0.8281, Val: 0.7108, Test: 0.6946\n",
      "Epoch: 048, Loss: 0.8350, Val: 0.7112, Test: 0.6966\n",
      "Epoch: 049, Loss: 0.8310, Val: 0.7109, Test: 0.6952\n",
      "Epoch: 050, Loss: 0.8302, Val: 0.7103, Test: 0.6937\n",
      "Epoch: 051, Loss: 0.8274, Val: 0.7110, Test: 0.6951\n",
      "Epoch: 052, Loss: 0.8280, Val: 0.7121, Test: 0.6947\n",
      "Epoch: 053, Loss: 0.8268, Val: 0.7109, Test: 0.6938\n",
      "Epoch: 054, Loss: 0.8289, Val: 0.7108, Test: 0.6939\n",
      "Epoch: 055, Loss: 0.8217, Val: 0.7113, Test: 0.6952\n",
      "Epoch: 056, Loss: 0.8285, Val: 0.7120, Test: 0.6953\n",
      "Epoch: 057, Loss: 0.8246, Val: 0.7117, Test: 0.6957\n",
      "Epoch: 058, Loss: 0.8243, Val: 0.7127, Test: 0.6972\n",
      "Epoch: 059, Loss: 0.8236, Val: 0.7117, Test: 0.6942\n",
      "Epoch: 060, Loss: 0.8233, Val: 0.7100, Test: 0.6909\n",
      "Epoch: 061, Loss: 0.8214, Val: 0.7107, Test: 0.6924\n",
      "Epoch: 062, Loss: 0.8216, Val: 0.7123, Test: 0.6965\n",
      "Epoch: 063, Loss: 0.8214, Val: 0.7125, Test: 0.6964\n",
      "Epoch: 064, Loss: 0.8178, Val: 0.7105, Test: 0.6948\n",
      "Epoch: 065, Loss: 0.8195, Val: 0.7123, Test: 0.6964\n",
      "Epoch: 066, Loss: 0.8236, Val: 0.7125, Test: 0.6982\n",
      "Epoch: 067, Loss: 0.8224, Val: 0.7109, Test: 0.6953\n",
      "Epoch: 068, Loss: 0.8191, Val: 0.7113, Test: 0.6946\n",
      "Epoch: 069, Loss: 0.8189, Val: 0.7116, Test: 0.6970\n",
      "Epoch: 070, Loss: 0.8180, Val: 0.7119, Test: 0.6978\n",
      "Epoch: 071, Loss: 0.8184, Val: 0.7111, Test: 0.6962\n",
      "Epoch: 072, Loss: 0.8164, Val: 0.7117, Test: 0.6952\n",
      "Epoch: 073, Loss: 0.8167, Val: 0.7117, Test: 0.6967\n",
      "Epoch: 074, Loss: 0.8160, Val: 0.7123, Test: 0.6978\n",
      "Epoch: 075, Loss: 0.8139, Val: 0.7118, Test: 0.6955\n",
      "Epoch: 076, Loss: 0.8187, Val: 0.7108, Test: 0.6944\n",
      "Epoch: 077, Loss: 0.8203, Val: 0.7105, Test: 0.6936\n",
      "Epoch: 078, Loss: 0.8122, Val: 0.7117, Test: 0.6959\n",
      "Epoch: 079, Loss: 0.8127, Val: 0.7129, Test: 0.6981\n",
      "Epoch: 080, Loss: 0.8098, Val: 0.7123, Test: 0.6977\n",
      "Epoch: 081, Loss: 0.8124, Val: 0.7109, Test: 0.6961\n",
      "Epoch: 082, Loss: 0.8177, Val: 0.7111, Test: 0.6971\n",
      "Epoch: 083, Loss: 0.8136, Val: 0.7125, Test: 0.6983\n",
      "Epoch: 084, Loss: 0.8169, Val: 0.7136, Test: 0.6988\n",
      "Epoch: 085, Loss: 0.8132, Val: 0.7127, Test: 0.6964\n",
      "Epoch: 086, Loss: 0.8128, Val: 0.7121, Test: 0.6960\n",
      "Epoch: 087, Loss: 0.8128, Val: 0.7116, Test: 0.6970\n",
      "Epoch: 088, Loss: 0.8099, Val: 0.7126, Test: 0.6964\n",
      "Epoch: 089, Loss: 0.8069, Val: 0.7126, Test: 0.6959\n",
      "Epoch: 090, Loss: 0.8149, Val: 0.7121, Test: 0.6961\n",
      "Epoch: 091, Loss: 0.8080, Val: 0.7134, Test: 0.6995\n",
      "Epoch: 092, Loss: 0.8176, Val: 0.7109, Test: 0.6954\n",
      "Epoch: 093, Loss: 0.8101, Val: 0.7115, Test: 0.6944\n",
      "Epoch: 094, Loss: 0.8130, Val: 0.7142, Test: 0.6999\n",
      "Epoch: 095, Loss: 0.8084, Val: 0.7153, Test: 0.7013\n",
      "Epoch: 096, Loss: 0.8088, Val: 0.7125, Test: 0.6965\n",
      "Epoch: 097, Loss: 0.8084, Val: 0.7119, Test: 0.6932\n",
      "Epoch: 098, Loss: 0.8135, Val: 0.7125, Test: 0.6978\n",
      "Epoch: 099, Loss: 0.8119, Val: 0.7130, Test: 0.6991\n",
      "Epoch: 100, Loss: 0.8076, Val: 0.7115, Test: 0.6960\n",
      "Epoch: 101, Loss: 0.8119, Val: 0.7116, Test: 0.6950\n",
      "Epoch: 102, Loss: 0.8050, Val: 0.7136, Test: 0.6983\n",
      "Epoch: 103, Loss: 0.8147, Val: 0.7134, Test: 0.6972\n",
      "Epoch: 104, Loss: 0.8078, Val: 0.7116, Test: 0.6955\n",
      "Epoch: 105, Loss: 0.8056, Val: 0.7132, Test: 0.6979\n",
      "Epoch: 106, Loss: 0.8093, Val: 0.7129, Test: 0.6987\n",
      "Epoch: 107, Loss: 0.8036, Val: 0.7131, Test: 0.6970\n",
      "Epoch: 108, Loss: 0.8069, Val: 0.7136, Test: 0.6970\n",
      "Epoch: 109, Loss: 0.8090, Val: 0.7132, Test: 0.6979\n",
      "Epoch: 110, Loss: 0.7997, Val: 0.7119, Test: 0.6959\n",
      "Epoch: 111, Loss: 0.8050, Val: 0.7124, Test: 0.6961\n",
      "Epoch: 112, Loss: 0.8111, Val: 0.7135, Test: 0.6989\n",
      "Epoch: 113, Loss: 0.8061, Val: 0.7136, Test: 0.6991\n",
      "Epoch: 114, Loss: 0.8015, Val: 0.7118, Test: 0.6956\n",
      "Epoch: 115, Loss: 0.8027, Val: 0.7111, Test: 0.6937\n",
      "Epoch: 116, Loss: 0.8097, Val: 0.7153, Test: 0.6988\n",
      "Epoch: 117, Loss: 0.8049, Val: 0.7149, Test: 0.6992\n",
      "Epoch: 118, Loss: 0.8061, Val: 0.7111, Test: 0.6951\n",
      "Epoch: 119, Loss: 0.8095, Val: 0.7114, Test: 0.6951\n",
      "Epoch: 120, Loss: 0.8032, Val: 0.7143, Test: 0.6984\n",
      "Epoch: 121, Loss: 0.8034, Val: 0.7140, Test: 0.6988\n",
      "Epoch: 122, Loss: 0.8059, Val: 0.7145, Test: 0.6988\n",
      "Epoch: 123, Loss: 0.8040, Val: 0.7130, Test: 0.6963\n",
      "Epoch: 124, Loss: 0.8090, Val: 0.7138, Test: 0.6964\n",
      "Epoch: 125, Loss: 0.8053, Val: 0.7142, Test: 0.6973\n",
      "Epoch: 126, Loss: 0.8045, Val: 0.7142, Test: 0.6975\n",
      "Epoch: 127, Loss: 0.8042, Val: 0.7132, Test: 0.6969\n",
      "Epoch: 128, Loss: 0.8001, Val: 0.7138, Test: 0.6971\n",
      "Epoch: 129, Loss: 0.8036, Val: 0.7132, Test: 0.6965\n",
      "Epoch: 130, Loss: 0.8002, Val: 0.7140, Test: 0.6979\n",
      "Epoch: 131, Loss: 0.8060, Val: 0.7138, Test: 0.6973\n",
      "Epoch: 132, Loss: 0.8037, Val: 0.7139, Test: 0.6974\n",
      "Epoch: 133, Loss: 0.8066, Val: 0.7144, Test: 0.6971\n",
      "Epoch: 134, Loss: 0.8044, Val: 0.7147, Test: 0.6972\n",
      "Epoch: 135, Loss: 0.8005, Val: 0.7118, Test: 0.6946\n",
      "Epoch: 136, Loss: 0.8008, Val: 0.7130, Test: 0.6956\n",
      "Epoch: 137, Loss: 0.8001, Val: 0.7160, Test: 0.6998\n",
      "Epoch: 138, Loss: 0.8000, Val: 0.7155, Test: 0.6995\n",
      "Epoch: 139, Loss: 0.8030, Val: 0.7132, Test: 0.6966\n",
      "Epoch: 140, Loss: 0.8010, Val: 0.7138, Test: 0.6966\n",
      "Epoch: 141, Loss: 0.8029, Val: 0.7158, Test: 0.6976\n",
      "Epoch: 142, Loss: 0.7999, Val: 0.7152, Test: 0.6976\n",
      "Epoch: 143, Loss: 0.7999, Val: 0.7147, Test: 0.6985\n",
      "Epoch: 144, Loss: 0.7991, Val: 0.7158, Test: 0.7012\n",
      "Epoch: 145, Loss: 0.8023, Val: 0.7141, Test: 0.6987\n",
      "Epoch: 146, Loss: 0.7998, Val: 0.7145, Test: 0.6987\n",
      "Epoch: 147, Loss: 0.8000, Val: 0.7147, Test: 0.6993\n",
      "Epoch: 148, Loss: 0.8025, Val: 0.7155, Test: 0.6996\n",
      "Epoch: 149, Loss: 0.7989, Val: 0.7145, Test: 0.6986\n",
      "Epoch: 150, Loss: 0.8048, Val: 0.7145, Test: 0.6993\n",
      "Epoch: 151, Loss: 0.8016, Val: 0.7159, Test: 0.7007\n",
      "Epoch: 152, Loss: 0.7986, Val: 0.7164, Test: 0.7007\n",
      "Epoch: 153, Loss: 0.8060, Val: 0.7142, Test: 0.6970\n",
      "Epoch: 154, Loss: 0.7970, Val: 0.7149, Test: 0.6981\n",
      "Epoch: 155, Loss: 0.7999, Val: 0.7143, Test: 0.6986\n",
      "Epoch: 156, Loss: 0.7938, Val: 0.7149, Test: 0.6997\n",
      "Epoch: 157, Loss: 0.7982, Val: 0.7167, Test: 0.7002\n",
      "Epoch: 158, Loss: 0.8010, Val: 0.7160, Test: 0.7002\n",
      "Epoch: 159, Loss: 0.7992, Val: 0.7144, Test: 0.6976\n",
      "Epoch: 160, Loss: 0.8023, Val: 0.7135, Test: 0.6978\n",
      "Epoch: 161, Loss: 0.8019, Val: 0.7158, Test: 0.7011\n",
      "Epoch: 162, Loss: 0.7974, Val: 0.7152, Test: 0.7002\n",
      "Epoch: 163, Loss: 0.7979, Val: 0.7141, Test: 0.6977\n",
      "Epoch: 164, Loss: 0.8002, Val: 0.7126, Test: 0.6962\n",
      "Epoch: 165, Loss: 0.7984, Val: 0.7156, Test: 0.6993\n",
      "Epoch: 166, Loss: 0.7976, Val: 0.7150, Test: 0.6987\n",
      "Epoch: 167, Loss: 0.7975, Val: 0.7126, Test: 0.6958\n",
      "Epoch: 168, Loss: 0.7992, Val: 0.7141, Test: 0.6991\n",
      "Epoch: 169, Loss: 0.7993, Val: 0.7153, Test: 0.7005\n",
      "Epoch: 170, Loss: 0.8029, Val: 0.7122, Test: 0.6953\n",
      "Epoch: 171, Loss: 0.8024, Val: 0.7130, Test: 0.6966\n",
      "Epoch: 172, Loss: 0.7993, Val: 0.7150, Test: 0.6997\n",
      "Epoch: 173, Loss: 0.7965, Val: 0.7163, Test: 0.7007\n",
      "Epoch: 174, Loss: 0.7972, Val: 0.7153, Test: 0.7007\n",
      "Epoch: 175, Loss: 0.7989, Val: 0.7141, Test: 0.6989\n",
      "Epoch: 176, Loss: 0.8006, Val: 0.7140, Test: 0.6971\n",
      "Epoch: 177, Loss: 0.7966, Val: 0.7147, Test: 0.6988\n",
      "Epoch: 178, Loss: 0.7929, Val: 0.7159, Test: 0.7001\n",
      "Epoch: 179, Loss: 0.7980, Val: 0.7171, Test: 0.7017\n",
      "Epoch: 180, Loss: 0.7994, Val: 0.7144, Test: 0.6996\n",
      "Epoch: 181, Loss: 0.7980, Val: 0.7133, Test: 0.6960\n",
      "Epoch: 182, Loss: 0.7982, Val: 0.7159, Test: 0.6985\n",
      "Epoch: 183, Loss: 0.7924, Val: 0.7161, Test: 0.6999\n",
      "Epoch: 184, Loss: 0.7984, Val: 0.7132, Test: 0.6968\n",
      "Epoch: 185, Loss: 0.7962, Val: 0.7124, Test: 0.6970\n",
      "Epoch: 186, Loss: 0.7947, Val: 0.7149, Test: 0.7009\n",
      "Epoch: 187, Loss: 0.7996, Val: 0.7155, Test: 0.6996\n",
      "Epoch: 188, Loss: 0.7977, Val: 0.7141, Test: 0.6965\n",
      "Epoch: 189, Loss: 0.7979, Val: 0.7157, Test: 0.7009\n",
      "Epoch: 190, Loss: 0.7963, Val: 0.7164, Test: 0.7025\n",
      "Epoch: 191, Loss: 0.7986, Val: 0.7144, Test: 0.6979\n",
      "Epoch: 192, Loss: 0.7918, Val: 0.7135, Test: 0.6968\n",
      "Epoch: 193, Loss: 0.7966, Val: 0.7166, Test: 0.7010\n",
      "Epoch: 194, Loss: 0.7993, Val: 0.7137, Test: 0.6991\n",
      "Epoch: 195, Loss: 0.7906, Val: 0.7129, Test: 0.6968\n",
      "Epoch: 196, Loss: 0.7939, Val: 0.7157, Test: 0.7011\n",
      "Epoch: 197, Loss: 0.7957, Val: 0.7172, Test: 0.7035\n",
      "Epoch: 198, Loss: 0.7952, Val: 0.7136, Test: 0.6982\n",
      "Epoch: 199, Loss: 0.7967, Val: 0.7122, Test: 0.6965\n",
      "Epoch: 200, Loss: 0.7984, Val: 0.7160, Test: 0.7035\n",
      "Epoch: 201, Loss: 0.7970, Val: 0.7160, Test: 0.7031\n",
      "Epoch: 202, Loss: 0.7945, Val: 0.7128, Test: 0.6963\n",
      "Epoch: 203, Loss: 0.7997, Val: 0.7137, Test: 0.6975\n",
      "Epoch: 204, Loss: 0.7952, Val: 0.7153, Test: 0.7005\n",
      "Epoch: 205, Loss: 0.7958, Val: 0.7156, Test: 0.6995\n",
      "Epoch: 206, Loss: 0.7932, Val: 0.7152, Test: 0.6991\n",
      "Epoch: 207, Loss: 0.7959, Val: 0.7161, Test: 0.7010\n",
      "Epoch: 208, Loss: 0.7979, Val: 0.7161, Test: 0.7001\n",
      "Epoch: 209, Loss: 0.7926, Val: 0.7130, Test: 0.6968\n",
      "Epoch: 210, Loss: 0.7927, Val: 0.7152, Test: 0.7000\n",
      "Epoch: 211, Loss: 0.7944, Val: 0.7176, Test: 0.7049\n",
      "Epoch: 212, Loss: 0.7957, Val: 0.7164, Test: 0.7015\n",
      "Epoch: 213, Loss: 0.7925, Val: 0.7126, Test: 0.6966\n",
      "Epoch: 214, Loss: 0.7967, Val: 0.7151, Test: 0.7006\n",
      "Epoch: 215, Loss: 0.7957, Val: 0.7173, Test: 0.7035\n",
      "Epoch: 216, Loss: 0.7908, Val: 0.7156, Test: 0.7007\n",
      "Epoch: 217, Loss: 0.7862, Val: 0.7155, Test: 0.7011\n",
      "Epoch: 218, Loss: 0.7928, Val: 0.7162, Test: 0.7010\n",
      "Epoch: 219, Loss: 0.7943, Val: 0.7155, Test: 0.7012\n",
      "Epoch: 220, Loss: 0.7912, Val: 0.7163, Test: 0.7014\n",
      "Epoch: 221, Loss: 0.7945, Val: 0.7169, Test: 0.7021\n",
      "Epoch: 222, Loss: 0.7905, Val: 0.7163, Test: 0.7019\n",
      "Epoch: 223, Loss: 0.7931, Val: 0.7142, Test: 0.6983\n",
      "Epoch: 224, Loss: 0.7870, Val: 0.7131, Test: 0.6962\n",
      "Epoch: 225, Loss: 0.7933, Val: 0.7157, Test: 0.7005\n",
      "Epoch: 226, Loss: 0.7915, Val: 0.7154, Test: 0.7012\n",
      "Epoch: 227, Loss: 0.7908, Val: 0.7117, Test: 0.6943\n",
      "Epoch: 228, Loss: 0.7957, Val: 0.7145, Test: 0.6971\n",
      "Epoch: 229, Loss: 0.7922, Val: 0.7190, Test: 0.7036\n",
      "Epoch: 230, Loss: 0.7924, Val: 0.7164, Test: 0.7017\n",
      "Epoch: 231, Loss: 0.7904, Val: 0.7128, Test: 0.6967\n",
      "Epoch: 232, Loss: 0.7864, Val: 0.7143, Test: 0.6983\n",
      "Epoch: 233, Loss: 0.7933, Val: 0.7181, Test: 0.7025\n",
      "Epoch: 234, Loss: 0.7926, Val: 0.7178, Test: 0.7017\n",
      "Epoch: 235, Loss: 0.7979, Val: 0.7129, Test: 0.6967\n",
      "Epoch: 236, Loss: 0.7921, Val: 0.7139, Test: 0.6986\n",
      "Epoch: 237, Loss: 0.7895, Val: 0.7157, Test: 0.6994\n",
      "Epoch: 238, Loss: 0.7911, Val: 0.7147, Test: 0.6980\n",
      "Epoch: 239, Loss: 0.7871, Val: 0.7158, Test: 0.7007\n",
      "Epoch: 240, Loss: 0.7872, Val: 0.7162, Test: 0.7028\n",
      "Epoch: 241, Loss: 0.7888, Val: 0.7134, Test: 0.6979\n",
      "Epoch: 242, Loss: 0.7935, Val: 0.7137, Test: 0.6982\n",
      "Epoch: 243, Loss: 0.7866, Val: 0.7170, Test: 0.7024\n",
      "Epoch: 244, Loss: 0.7875, Val: 0.7158, Test: 0.7005\n",
      "Epoch: 245, Loss: 0.7916, Val: 0.7140, Test: 0.6984\n",
      "Epoch: 246, Loss: 0.7861, Val: 0.7164, Test: 0.7026\n",
      "Epoch: 247, Loss: 0.7849, Val: 0.7165, Test: 0.7027\n",
      "Epoch: 248, Loss: 0.7910, Val: 0.7146, Test: 0.6996\n",
      "Epoch: 249, Loss: 0.7919, Val: 0.7160, Test: 0.7013\n",
      "Epoch: 250, Loss: 0.7826, Val: 0.7175, Test: 0.7037\n",
      "Epoch: 251, Loss: 0.7930, Val: 0.7159, Test: 0.7012\n",
      "Epoch: 252, Loss: 0.7908, Val: 0.7131, Test: 0.6974\n",
      "Epoch: 253, Loss: 0.7945, Val: 0.7156, Test: 0.7017\n",
      "Epoch: 254, Loss: 0.7864, Val: 0.7158, Test: 0.7034\n",
      "Epoch: 255, Loss: 0.7950, Val: 0.7150, Test: 0.7010\n",
      "Epoch: 256, Loss: 0.7917, Val: 0.7146, Test: 0.6995\n",
      "Epoch: 257, Loss: 0.7867, Val: 0.7144, Test: 0.7006\n",
      "Epoch: 258, Loss: 0.7850, Val: 0.7156, Test: 0.7016\n",
      "Epoch: 259, Loss: 0.7884, Val: 0.7161, Test: 0.7006\n",
      "Epoch: 260, Loss: 0.7911, Val: 0.7164, Test: 0.7014\n",
      "Epoch: 261, Loss: 0.7900, Val: 0.7174, Test: 0.7035\n",
      "Epoch: 262, Loss: 0.7912, Val: 0.7152, Test: 0.6982\n",
      "Epoch: 263, Loss: 0.7933, Val: 0.7166, Test: 0.6997\n",
      "Epoch: 264, Loss: 0.7893, Val: 0.7185, Test: 0.7043\n",
      "Epoch: 265, Loss: 0.7901, Val: 0.7162, Test: 0.7023\n",
      "Epoch: 266, Loss: 0.7868, Val: 0.7133, Test: 0.6963\n",
      "Epoch: 267, Loss: 0.7899, Val: 0.7169, Test: 0.7019\n",
      "Epoch: 268, Loss: 0.7840, Val: 0.7184, Test: 0.7063\n",
      "Epoch: 269, Loss: 0.7895, Val: 0.7159, Test: 0.7005\n",
      "Epoch: 270, Loss: 0.7880, Val: 0.7145, Test: 0.6984\n",
      "Epoch: 271, Loss: 0.7877, Val: 0.7178, Test: 0.7045\n",
      "Epoch: 272, Loss: 0.7843, Val: 0.7174, Test: 0.7048\n",
      "Epoch: 273, Loss: 0.7896, Val: 0.7158, Test: 0.7009\n",
      "Epoch: 274, Loss: 0.7868, Val: 0.7178, Test: 0.7038\n",
      "Epoch: 275, Loss: 0.7908, Val: 0.7175, Test: 0.7038\n",
      "Epoch: 276, Loss: 0.7907, Val: 0.7154, Test: 0.7010\n",
      "Epoch: 277, Loss: 0.7865, Val: 0.7156, Test: 0.7016\n",
      "Epoch: 278, Loss: 0.7862, Val: 0.7178, Test: 0.7061\n",
      "Epoch: 279, Loss: 0.7850, Val: 0.7175, Test: 0.7028\n",
      "Epoch: 280, Loss: 0.7842, Val: 0.7148, Test: 0.6981\n",
      "Epoch: 281, Loss: 0.7858, Val: 0.7160, Test: 0.7020\n",
      "Epoch: 282, Loss: 0.7898, Val: 0.7185, Test: 0.7054\n",
      "Epoch: 283, Loss: 0.7836, Val: 0.7176, Test: 0.7016\n",
      "Epoch: 284, Loss: 0.7862, Val: 0.7161, Test: 0.6997\n",
      "Epoch: 285, Loss: 0.7838, Val: 0.7174, Test: 0.7023\n",
      "Epoch: 286, Loss: 0.7907, Val: 0.7173, Test: 0.7024\n",
      "Epoch: 287, Loss: 0.7838, Val: 0.7157, Test: 0.7007\n",
      "Epoch: 288, Loss: 0.7870, Val: 0.7156, Test: 0.7009\n",
      "Epoch: 289, Loss: 0.7866, Val: 0.7163, Test: 0.7018\n",
      "Epoch: 290, Loss: 0.7868, Val: 0.7176, Test: 0.7038\n",
      "Epoch: 291, Loss: 0.7851, Val: 0.7177, Test: 0.7020\n",
      "Epoch: 292, Loss: 0.7820, Val: 0.7146, Test: 0.6990\n",
      "Epoch: 293, Loss: 0.7873, Val: 0.7147, Test: 0.7004\n",
      "Epoch: 294, Loss: 0.7856, Val: 0.7170, Test: 0.7041\n",
      "Epoch: 295, Loss: 0.7835, Val: 0.7165, Test: 0.7028\n",
      "Epoch: 296, Loss: 0.7838, Val: 0.7156, Test: 0.7003\n",
      "Epoch: 297, Loss: 0.7865, Val: 0.7169, Test: 0.7026\n",
      "Epoch: 298, Loss: 0.7846, Val: 0.7172, Test: 0.7031\n",
      "Epoch: 299, Loss: 0.7851, Val: 0.7171, Test: 0.7025\n",
      "Epoch: 300, Loss: 0.7878, Val: 0.7174, Test: 0.7032\n",
      "0.30000000000000004\n",
      "Label_rate: 0.30000000000000004\n",
      "Epoch: 001, Loss: 2.0728, Val: 0.4173, Test: 0.3596\n",
      "Epoch: 002, Loss: 1.5253, Val: 0.5282, Test: 0.4779\n",
      "Epoch: 003, Loss: 1.3105, Val: 0.5532, Test: 0.5011\n",
      "Epoch: 004, Loss: 1.2743, Val: 0.5918, Test: 0.5553\n",
      "Epoch: 005, Loss: 1.1937, Val: 0.6154, Test: 0.5829\n",
      "Epoch: 006, Loss: 1.1480, Val: 0.6335, Test: 0.6022\n",
      "Epoch: 007, Loss: 1.0975, Val: 0.6429, Test: 0.6232\n",
      "Epoch: 008, Loss: 1.0988, Val: 0.6638, Test: 0.6528\n",
      "Epoch: 009, Loss: 1.0652, Val: 0.6676, Test: 0.6526\n",
      "Epoch: 010, Loss: 1.0491, Val: 0.6703, Test: 0.6512\n",
      "Epoch: 011, Loss: 1.0228, Val: 0.6736, Test: 0.6559\n",
      "Epoch: 012, Loss: 1.0126, Val: 0.6767, Test: 0.6612\n",
      "Epoch: 013, Loss: 1.0025, Val: 0.6836, Test: 0.6681\n",
      "Epoch: 014, Loss: 0.9833, Val: 0.6859, Test: 0.6733\n",
      "Epoch: 015, Loss: 0.9853, Val: 0.6914, Test: 0.6820\n",
      "Epoch: 016, Loss: 0.9642, Val: 0.6937, Test: 0.6857\n",
      "Epoch: 017, Loss: 0.9617, Val: 0.6902, Test: 0.6812\n",
      "Epoch: 018, Loss: 0.9568, Val: 0.6871, Test: 0.6753\n",
      "Epoch: 019, Loss: 0.9488, Val: 0.6887, Test: 0.6762\n",
      "Epoch: 020, Loss: 0.9350, Val: 0.6960, Test: 0.6856\n",
      "Epoch: 021, Loss: 0.9354, Val: 0.6998, Test: 0.6913\n",
      "Epoch: 022, Loss: 0.9319, Val: 0.7003, Test: 0.6927\n",
      "Epoch: 023, Loss: 0.9162, Val: 0.7010, Test: 0.6925\n",
      "Epoch: 024, Loss: 0.9203, Val: 0.6987, Test: 0.6915\n",
      "Epoch: 025, Loss: 0.9173, Val: 0.6991, Test: 0.6908\n",
      "Epoch: 026, Loss: 0.9057, Val: 0.7010, Test: 0.6906\n",
      "Epoch: 027, Loss: 0.9048, Val: 0.7023, Test: 0.6905\n",
      "Epoch: 028, Loss: 0.8970, Val: 0.7028, Test: 0.6906\n",
      "Epoch: 029, Loss: 0.9004, Val: 0.7016, Test: 0.6893\n",
      "Epoch: 030, Loss: 0.8910, Val: 0.7030, Test: 0.6913\n",
      "Epoch: 031, Loss: 0.8881, Val: 0.7054, Test: 0.6950\n",
      "Epoch: 032, Loss: 0.8913, Val: 0.7061, Test: 0.6948\n",
      "Epoch: 033, Loss: 0.8825, Val: 0.7060, Test: 0.6930\n",
      "Epoch: 034, Loss: 0.8821, Val: 0.7050, Test: 0.6911\n",
      "Epoch: 035, Loss: 0.8817, Val: 0.7043, Test: 0.6910\n",
      "Epoch: 036, Loss: 0.8743, Val: 0.7047, Test: 0.6915\n",
      "Epoch: 037, Loss: 0.8708, Val: 0.7059, Test: 0.6921\n",
      "Epoch: 038, Loss: 0.8757, Val: 0.7050, Test: 0.6917\n",
      "Epoch: 039, Loss: 0.8678, Val: 0.7042, Test: 0.6908\n",
      "Epoch: 040, Loss: 0.8640, Val: 0.7048, Test: 0.6910\n",
      "Epoch: 041, Loss: 0.8658, Val: 0.7057, Test: 0.6917\n",
      "Epoch: 042, Loss: 0.8546, Val: 0.7061, Test: 0.6919\n",
      "Epoch: 043, Loss: 0.8642, Val: 0.7068, Test: 0.6914\n",
      "Epoch: 044, Loss: 0.8553, Val: 0.7071, Test: 0.6904\n",
      "Epoch: 045, Loss: 0.8560, Val: 0.7067, Test: 0.6892\n",
      "Epoch: 046, Loss: 0.8537, Val: 0.7072, Test: 0.6904\n",
      "Epoch: 047, Loss: 0.8510, Val: 0.7069, Test: 0.6901\n",
      "Epoch: 048, Loss: 0.8492, Val: 0.7060, Test: 0.6892\n",
      "Epoch: 049, Loss: 0.8546, Val: 0.7074, Test: 0.6883\n",
      "Epoch: 050, Loss: 0.8475, Val: 0.7082, Test: 0.6874\n",
      "Epoch: 051, Loss: 0.8469, Val: 0.7075, Test: 0.6873\n",
      "Epoch: 052, Loss: 0.8495, Val: 0.7079, Test: 0.6870\n",
      "Epoch: 053, Loss: 0.8465, Val: 0.7071, Test: 0.6877\n",
      "Epoch: 054, Loss: 0.8451, Val: 0.7078, Test: 0.6888\n",
      "Epoch: 055, Loss: 0.8477, Val: 0.7077, Test: 0.6893\n",
      "Epoch: 056, Loss: 0.8437, Val: 0.7080, Test: 0.6905\n",
      "Epoch: 057, Loss: 0.8438, Val: 0.7086, Test: 0.6923\n",
      "Epoch: 058, Loss: 0.8379, Val: 0.7080, Test: 0.6907\n",
      "Epoch: 059, Loss: 0.8387, Val: 0.7077, Test: 0.6901\n",
      "Epoch: 060, Loss: 0.8408, Val: 0.7081, Test: 0.6886\n",
      "Epoch: 061, Loss: 0.8403, Val: 0.7078, Test: 0.6884\n",
      "Epoch: 062, Loss: 0.8350, Val: 0.7078, Test: 0.6900\n",
      "Epoch: 063, Loss: 0.8380, Val: 0.7077, Test: 0.6898\n",
      "Epoch: 064, Loss: 0.8404, Val: 0.7076, Test: 0.6894\n",
      "Epoch: 065, Loss: 0.8315, Val: 0.7079, Test: 0.6892\n",
      "Epoch: 066, Loss: 0.8254, Val: 0.7091, Test: 0.6896\n",
      "Epoch: 067, Loss: 0.8331, Val: 0.7095, Test: 0.6902\n",
      "Epoch: 068, Loss: 0.8379, Val: 0.7087, Test: 0.6886\n",
      "Epoch: 069, Loss: 0.8325, Val: 0.7085, Test: 0.6886\n",
      "Epoch: 070, Loss: 0.8333, Val: 0.7089, Test: 0.6898\n",
      "Epoch: 071, Loss: 0.8367, Val: 0.7099, Test: 0.6915\n",
      "Epoch: 072, Loss: 0.8285, Val: 0.7096, Test: 0.6912\n",
      "Epoch: 073, Loss: 0.8353, Val: 0.7096, Test: 0.6906\n",
      "Epoch: 074, Loss: 0.8254, Val: 0.7088, Test: 0.6900\n",
      "Epoch: 075, Loss: 0.8297, Val: 0.7093, Test: 0.6904\n",
      "Epoch: 076, Loss: 0.8314, Val: 0.7107, Test: 0.6935\n",
      "Epoch: 077, Loss: 0.8300, Val: 0.7105, Test: 0.6924\n",
      "Epoch: 078, Loss: 0.8268, Val: 0.7103, Test: 0.6908\n",
      "Epoch: 079, Loss: 0.8291, Val: 0.7105, Test: 0.6913\n",
      "Epoch: 080, Loss: 0.8296, Val: 0.7108, Test: 0.6917\n",
      "Epoch: 081, Loss: 0.8289, Val: 0.7096, Test: 0.6904\n",
      "Epoch: 082, Loss: 0.8213, Val: 0.7107, Test: 0.6928\n",
      "Epoch: 083, Loss: 0.8250, Val: 0.7110, Test: 0.6935\n",
      "Epoch: 084, Loss: 0.8237, Val: 0.7102, Test: 0.6930\n",
      "Epoch: 085, Loss: 0.8217, Val: 0.7092, Test: 0.6910\n",
      "Epoch: 086, Loss: 0.8253, Val: 0.7100, Test: 0.6918\n",
      "Epoch: 087, Loss: 0.8189, Val: 0.7127, Test: 0.6958\n",
      "Epoch: 088, Loss: 0.8256, Val: 0.7129, Test: 0.6972\n",
      "Epoch: 089, Loss: 0.8211, Val: 0.7100, Test: 0.6934\n",
      "Epoch: 090, Loss: 0.8254, Val: 0.7096, Test: 0.6921\n",
      "Epoch: 091, Loss: 0.8213, Val: 0.7102, Test: 0.6935\n",
      "Epoch: 092, Loss: 0.8180, Val: 0.7115, Test: 0.6957\n",
      "Epoch: 093, Loss: 0.8222, Val: 0.7117, Test: 0.6943\n",
      "Epoch: 094, Loss: 0.8189, Val: 0.7106, Test: 0.6935\n",
      "Epoch: 095, Loss: 0.8300, Val: 0.7104, Test: 0.6929\n",
      "Epoch: 096, Loss: 0.8211, Val: 0.7102, Test: 0.6920\n",
      "Epoch: 097, Loss: 0.8185, Val: 0.7108, Test: 0.6932\n",
      "Epoch: 098, Loss: 0.8163, Val: 0.7114, Test: 0.6944\n",
      "Epoch: 099, Loss: 0.8163, Val: 0.7115, Test: 0.6954\n",
      "Epoch: 100, Loss: 0.8193, Val: 0.7104, Test: 0.6936\n",
      "Epoch: 101, Loss: 0.8177, Val: 0.7102, Test: 0.6933\n",
      "Epoch: 102, Loss: 0.8117, Val: 0.7106, Test: 0.6920\n",
      "Epoch: 103, Loss: 0.8176, Val: 0.7116, Test: 0.6937\n",
      "Epoch: 104, Loss: 0.8164, Val: 0.7130, Test: 0.6943\n",
      "Epoch: 105, Loss: 0.8198, Val: 0.7116, Test: 0.6913\n",
      "Epoch: 106, Loss: 0.8164, Val: 0.7107, Test: 0.6906\n",
      "Epoch: 107, Loss: 0.8183, Val: 0.7111, Test: 0.6916\n",
      "Epoch: 108, Loss: 0.8136, Val: 0.7115, Test: 0.6936\n",
      "Epoch: 109, Loss: 0.8186, Val: 0.7102, Test: 0.6919\n",
      "Epoch: 110, Loss: 0.8089, Val: 0.7096, Test: 0.6896\n",
      "Epoch: 111, Loss: 0.8182, Val: 0.7114, Test: 0.6938\n",
      "Epoch: 112, Loss: 0.8140, Val: 0.7134, Test: 0.6980\n",
      "Epoch: 113, Loss: 0.8214, Val: 0.7134, Test: 0.6945\n",
      "Epoch: 114, Loss: 0.8105, Val: 0.7119, Test: 0.6920\n",
      "Epoch: 115, Loss: 0.8147, Val: 0.7114, Test: 0.6927\n",
      "Epoch: 116, Loss: 0.8099, Val: 0.7124, Test: 0.6958\n",
      "Epoch: 117, Loss: 0.8126, Val: 0.7129, Test: 0.6944\n",
      "Epoch: 118, Loss: 0.8048, Val: 0.7129, Test: 0.6933\n",
      "Epoch: 119, Loss: 0.8143, Val: 0.7134, Test: 0.6924\n",
      "Epoch: 120, Loss: 0.8116, Val: 0.7120, Test: 0.6937\n",
      "Epoch: 121, Loss: 0.8151, Val: 0.7119, Test: 0.6946\n",
      "Epoch: 122, Loss: 0.8096, Val: 0.7125, Test: 0.6951\n",
      "Epoch: 123, Loss: 0.8107, Val: 0.7123, Test: 0.6945\n",
      "Epoch: 124, Loss: 0.8130, Val: 0.7128, Test: 0.6961\n",
      "Epoch: 125, Loss: 0.8049, Val: 0.7127, Test: 0.6951\n",
      "Epoch: 126, Loss: 0.8055, Val: 0.7125, Test: 0.6942\n",
      "Epoch: 127, Loss: 0.8028, Val: 0.7117, Test: 0.6935\n",
      "Epoch: 128, Loss: 0.8072, Val: 0.7123, Test: 0.6951\n",
      "Epoch: 129, Loss: 0.8094, Val: 0.7131, Test: 0.6975\n",
      "Epoch: 130, Loss: 0.8073, Val: 0.7140, Test: 0.6965\n",
      "Epoch: 131, Loss: 0.8093, Val: 0.7139, Test: 0.6961\n",
      "Epoch: 132, Loss: 0.8093, Val: 0.7130, Test: 0.6952\n",
      "Epoch: 133, Loss: 0.8088, Val: 0.7116, Test: 0.6941\n",
      "Epoch: 134, Loss: 0.8053, Val: 0.7121, Test: 0.6945\n",
      "Epoch: 135, Loss: 0.8105, Val: 0.7133, Test: 0.6956\n",
      "Epoch: 136, Loss: 0.8089, Val: 0.7135, Test: 0.6963\n",
      "Epoch: 137, Loss: 0.8052, Val: 0.7134, Test: 0.6948\n",
      "Epoch: 138, Loss: 0.8078, Val: 0.7114, Test: 0.6935\n",
      "Epoch: 139, Loss: 0.8049, Val: 0.7129, Test: 0.6957\n",
      "Epoch: 140, Loss: 0.8070, Val: 0.7142, Test: 0.6982\n",
      "Epoch: 141, Loss: 0.8010, Val: 0.7140, Test: 0.6982\n",
      "Epoch: 142, Loss: 0.8071, Val: 0.7139, Test: 0.6974\n",
      "Epoch: 143, Loss: 0.8090, Val: 0.7117, Test: 0.6949\n",
      "Epoch: 144, Loss: 0.8043, Val: 0.7111, Test: 0.6935\n",
      "Epoch: 145, Loss: 0.8033, Val: 0.7124, Test: 0.6934\n",
      "Epoch: 146, Loss: 0.8126, Val: 0.7126, Test: 0.6920\n",
      "Epoch: 147, Loss: 0.8054, Val: 0.7130, Test: 0.6931\n",
      "Epoch: 148, Loss: 0.8079, Val: 0.7139, Test: 0.6984\n",
      "Epoch: 149, Loss: 0.8038, Val: 0.7148, Test: 0.6990\n",
      "Epoch: 150, Loss: 0.8061, Val: 0.7129, Test: 0.6958\n",
      "Epoch: 151, Loss: 0.8047, Val: 0.7127, Test: 0.6944\n",
      "Epoch: 152, Loss: 0.8010, Val: 0.7141, Test: 0.6967\n",
      "Epoch: 153, Loss: 0.8019, Val: 0.7146, Test: 0.6981\n",
      "Epoch: 154, Loss: 0.8072, Val: 0.7138, Test: 0.6967\n",
      "Epoch: 155, Loss: 0.8005, Val: 0.7124, Test: 0.6946\n",
      "Epoch: 156, Loss: 0.8023, Val: 0.7121, Test: 0.6936\n",
      "Epoch: 157, Loss: 0.8011, Val: 0.7139, Test: 0.6966\n",
      "Epoch: 158, Loss: 0.7998, Val: 0.7149, Test: 0.6995\n",
      "Epoch: 159, Loss: 0.7978, Val: 0.7128, Test: 0.6961\n",
      "Epoch: 160, Loss: 0.7917, Val: 0.7108, Test: 0.6915\n",
      "Epoch: 161, Loss: 0.8028, Val: 0.7124, Test: 0.6940\n",
      "Epoch: 162, Loss: 0.7963, Val: 0.7159, Test: 0.7015\n",
      "Epoch: 163, Loss: 0.8023, Val: 0.7163, Test: 0.7024\n",
      "Epoch: 164, Loss: 0.8007, Val: 0.7149, Test: 0.6986\n",
      "Epoch: 165, Loss: 0.7972, Val: 0.7136, Test: 0.6954\n",
      "Epoch: 166, Loss: 0.7995, Val: 0.7134, Test: 0.6947\n",
      "Epoch: 167, Loss: 0.8035, Val: 0.7127, Test: 0.6958\n",
      "Epoch: 168, Loss: 0.8000, Val: 0.7136, Test: 0.6986\n",
      "Epoch: 169, Loss: 0.8048, Val: 0.7142, Test: 0.6993\n",
      "Epoch: 170, Loss: 0.7992, Val: 0.7143, Test: 0.6984\n",
      "Epoch: 171, Loss: 0.7984, Val: 0.7133, Test: 0.6961\n",
      "Epoch: 172, Loss: 0.8021, Val: 0.7130, Test: 0.6965\n",
      "Epoch: 173, Loss: 0.7979, Val: 0.7140, Test: 0.6981\n",
      "Epoch: 174, Loss: 0.7984, Val: 0.7157, Test: 0.7001\n",
      "Epoch: 175, Loss: 0.7988, Val: 0.7146, Test: 0.7002\n",
      "Epoch: 176, Loss: 0.7998, Val: 0.7150, Test: 0.6998\n",
      "Epoch: 177, Loss: 0.7959, Val: 0.7148, Test: 0.7002\n",
      "Epoch: 178, Loss: 0.8012, Val: 0.7148, Test: 0.6998\n",
      "Epoch: 179, Loss: 0.7984, Val: 0.7130, Test: 0.6962\n",
      "Epoch: 180, Loss: 0.7983, Val: 0.7142, Test: 0.6969\n",
      "Epoch: 181, Loss: 0.7979, Val: 0.7150, Test: 0.6986\n",
      "Epoch: 182, Loss: 0.7979, Val: 0.7154, Test: 0.6995\n",
      "Epoch: 183, Loss: 0.8008, Val: 0.7136, Test: 0.6974\n",
      "Epoch: 184, Loss: 0.7948, Val: 0.7117, Test: 0.6938\n",
      "Epoch: 185, Loss: 0.7976, Val: 0.7135, Test: 0.6962\n",
      "Epoch: 186, Loss: 0.7928, Val: 0.7158, Test: 0.7012\n",
      "Epoch: 187, Loss: 0.8000, Val: 0.7152, Test: 0.6995\n",
      "Epoch: 188, Loss: 0.7957, Val: 0.7143, Test: 0.6975\n",
      "Epoch: 189, Loss: 0.7977, Val: 0.7148, Test: 0.6979\n",
      "Epoch: 190, Loss: 0.7942, Val: 0.7163, Test: 0.7011\n",
      "Epoch: 191, Loss: 0.7989, Val: 0.7151, Test: 0.6993\n",
      "Epoch: 192, Loss: 0.7968, Val: 0.7145, Test: 0.6973\n",
      "Epoch: 193, Loss: 0.7959, Val: 0.7160, Test: 0.7003\n",
      "Epoch: 194, Loss: 0.7977, Val: 0.7160, Test: 0.7009\n",
      "Epoch: 195, Loss: 0.7924, Val: 0.7152, Test: 0.7001\n",
      "Epoch: 196, Loss: 0.7994, Val: 0.7143, Test: 0.6989\n",
      "Epoch: 197, Loss: 0.8004, Val: 0.7149, Test: 0.6991\n",
      "Epoch: 198, Loss: 0.7942, Val: 0.7156, Test: 0.6992\n",
      "Epoch: 199, Loss: 0.7951, Val: 0.7156, Test: 0.6996\n",
      "Epoch: 200, Loss: 0.7987, Val: 0.7153, Test: 0.6986\n",
      "Epoch: 201, Loss: 0.7946, Val: 0.7147, Test: 0.6992\n",
      "Epoch: 202, Loss: 0.7933, Val: 0.7142, Test: 0.6984\n",
      "Epoch: 203, Loss: 0.7963, Val: 0.7141, Test: 0.6969\n",
      "Epoch: 204, Loss: 0.7954, Val: 0.7148, Test: 0.6980\n",
      "Epoch: 205, Loss: 0.7964, Val: 0.7156, Test: 0.7000\n",
      "Epoch: 206, Loss: 0.7930, Val: 0.7145, Test: 0.7007\n",
      "Epoch: 207, Loss: 0.7942, Val: 0.7140, Test: 0.6999\n",
      "Epoch: 208, Loss: 0.7904, Val: 0.7153, Test: 0.7002\n",
      "Epoch: 209, Loss: 0.7919, Val: 0.7148, Test: 0.6992\n",
      "Epoch: 210, Loss: 0.7983, Val: 0.7151, Test: 0.6991\n",
      "Epoch: 211, Loss: 0.7935, Val: 0.7156, Test: 0.7006\n",
      "Epoch: 212, Loss: 0.7872, Val: 0.7144, Test: 0.6990\n",
      "Epoch: 213, Loss: 0.7968, Val: 0.7144, Test: 0.6974\n",
      "Epoch: 214, Loss: 0.7983, Val: 0.7145, Test: 0.6987\n",
      "Epoch: 215, Loss: 0.7922, Val: 0.7157, Test: 0.7026\n",
      "Epoch: 216, Loss: 0.7913, Val: 0.7151, Test: 0.6995\n",
      "Epoch: 217, Loss: 0.7975, Val: 0.7146, Test: 0.6974\n",
      "Epoch: 218, Loss: 0.7894, Val: 0.7133, Test: 0.6984\n",
      "Epoch: 219, Loss: 0.7966, Val: 0.7137, Test: 0.6991\n",
      "Epoch: 220, Loss: 0.7886, Val: 0.7152, Test: 0.7032\n",
      "Epoch: 221, Loss: 0.7977, Val: 0.7168, Test: 0.7031\n",
      "Epoch: 222, Loss: 0.7923, Val: 0.7138, Test: 0.6992\n",
      "Epoch: 223, Loss: 0.7979, Val: 0.7146, Test: 0.6988\n",
      "Epoch: 224, Loss: 0.7941, Val: 0.7166, Test: 0.7025\n",
      "Epoch: 225, Loss: 0.7961, Val: 0.7168, Test: 0.7032\n",
      "Epoch: 226, Loss: 0.7929, Val: 0.7154, Test: 0.7007\n",
      "Epoch: 227, Loss: 0.7946, Val: 0.7141, Test: 0.6985\n",
      "Epoch: 228, Loss: 0.7916, Val: 0.7132, Test: 0.6994\n",
      "Epoch: 229, Loss: 0.7903, Val: 0.7145, Test: 0.7010\n",
      "Epoch: 230, Loss: 0.7917, Val: 0.7149, Test: 0.7018\n",
      "Epoch: 231, Loss: 0.7922, Val: 0.7141, Test: 0.6993\n",
      "Epoch: 232, Loss: 0.7893, Val: 0.7137, Test: 0.6981\n",
      "Epoch: 233, Loss: 0.7907, Val: 0.7147, Test: 0.6995\n",
      "Epoch: 234, Loss: 0.7906, Val: 0.7152, Test: 0.7008\n",
      "Epoch: 235, Loss: 0.7927, Val: 0.7158, Test: 0.7006\n",
      "Epoch: 236, Loss: 0.7888, Val: 0.7156, Test: 0.6994\n",
      "Epoch: 237, Loss: 0.7960, Val: 0.7154, Test: 0.7019\n",
      "Epoch: 238, Loss: 0.7919, Val: 0.7160, Test: 0.7028\n",
      "Epoch: 239, Loss: 0.7883, Val: 0.7152, Test: 0.7006\n",
      "Epoch: 240, Loss: 0.7900, Val: 0.7150, Test: 0.6985\n",
      "Epoch: 241, Loss: 0.7901, Val: 0.7143, Test: 0.6982\n",
      "Epoch: 242, Loss: 0.7900, Val: 0.7172, Test: 0.7031\n",
      "Epoch: 243, Loss: 0.7878, Val: 0.7170, Test: 0.7037\n",
      "Epoch: 244, Loss: 0.7909, Val: 0.7155, Test: 0.7007\n",
      "Epoch: 245, Loss: 0.7937, Val: 0.7135, Test: 0.6965\n",
      "Epoch: 246, Loss: 0.7876, Val: 0.7155, Test: 0.7018\n",
      "Epoch: 247, Loss: 0.7915, Val: 0.7161, Test: 0.7028\n",
      "Epoch: 248, Loss: 0.7885, Val: 0.7153, Test: 0.7010\n",
      "Epoch: 249, Loss: 0.7967, Val: 0.7166, Test: 0.7027\n",
      "Epoch: 250, Loss: 0.7869, Val: 0.7176, Test: 0.7058\n",
      "Epoch: 251, Loss: 0.7926, Val: 0.7167, Test: 0.7038\n",
      "Epoch: 252, Loss: 0.7873, Val: 0.7155, Test: 0.7004\n",
      "Epoch: 253, Loss: 0.7976, Val: 0.7141, Test: 0.7006\n",
      "Epoch: 254, Loss: 0.7916, Val: 0.7151, Test: 0.6988\n",
      "Epoch: 255, Loss: 0.7862, Val: 0.7172, Test: 0.7038\n",
      "Epoch: 256, Loss: 0.7905, Val: 0.7184, Test: 0.7068\n",
      "Epoch: 257, Loss: 0.7896, Val: 0.7171, Test: 0.7037\n",
      "Epoch: 258, Loss: 0.7913, Val: 0.7156, Test: 0.7009\n",
      "Epoch: 259, Loss: 0.7908, Val: 0.7148, Test: 0.7014\n",
      "Epoch: 260, Loss: 0.7848, Val: 0.7159, Test: 0.7026\n",
      "Epoch: 261, Loss: 0.7909, Val: 0.7161, Test: 0.7021\n",
      "Epoch: 262, Loss: 0.7901, Val: 0.7164, Test: 0.7032\n",
      "Epoch: 263, Loss: 0.7890, Val: 0.7176, Test: 0.7044\n",
      "Epoch: 264, Loss: 0.7883, Val: 0.7165, Test: 0.7029\n",
      "Epoch: 265, Loss: 0.7877, Val: 0.7154, Test: 0.7010\n",
      "Epoch: 266, Loss: 0.7901, Val: 0.7168, Test: 0.7039\n",
      "Epoch: 267, Loss: 0.7873, Val: 0.7176, Test: 0.7056\n",
      "Epoch: 268, Loss: 0.7877, Val: 0.7169, Test: 0.7029\n",
      "Epoch: 269, Loss: 0.7852, Val: 0.7143, Test: 0.6995\n",
      "Epoch: 270, Loss: 0.7897, Val: 0.7150, Test: 0.7034\n",
      "Epoch: 271, Loss: 0.7913, Val: 0.7168, Test: 0.7064\n",
      "Epoch: 272, Loss: 0.7887, Val: 0.7160, Test: 0.7030\n",
      "Epoch: 273, Loss: 0.7926, Val: 0.7162, Test: 0.7019\n",
      "Epoch: 274, Loss: 0.7873, Val: 0.7171, Test: 0.7051\n",
      "Epoch: 275, Loss: 0.7827, Val: 0.7176, Test: 0.7061\n",
      "Epoch: 276, Loss: 0.7905, Val: 0.7172, Test: 0.7027\n",
      "Epoch: 277, Loss: 0.7896, Val: 0.7171, Test: 0.7056\n",
      "Epoch: 278, Loss: 0.7920, Val: 0.7169, Test: 0.7072\n",
      "Epoch: 279, Loss: 0.7945, Val: 0.7168, Test: 0.7030\n",
      "Epoch: 280, Loss: 0.7902, Val: 0.7166, Test: 0.7021\n",
      "Epoch: 281, Loss: 0.7920, Val: 0.7178, Test: 0.7069\n",
      "Epoch: 282, Loss: 0.7949, Val: 0.7160, Test: 0.7026\n",
      "Epoch: 283, Loss: 0.7915, Val: 0.7143, Test: 0.6997\n",
      "Epoch: 284, Loss: 0.7925, Val: 0.7171, Test: 0.7044\n",
      "Epoch: 285, Loss: 0.7928, Val: 0.7178, Test: 0.7057\n",
      "Epoch: 286, Loss: 0.7932, Val: 0.7154, Test: 0.7009\n",
      "Epoch: 287, Loss: 0.7876, Val: 0.7154, Test: 0.7012\n",
      "Epoch: 288, Loss: 0.7886, Val: 0.7173, Test: 0.7054\n",
      "Epoch: 289, Loss: 0.7868, Val: 0.7182, Test: 0.7071\n",
      "Epoch: 290, Loss: 0.7868, Val: 0.7164, Test: 0.7040\n",
      "Epoch: 291, Loss: 0.7915, Val: 0.7153, Test: 0.7023\n",
      "Epoch: 292, Loss: 0.7847, Val: 0.7164, Test: 0.7032\n",
      "Epoch: 293, Loss: 0.7871, Val: 0.7174, Test: 0.7058\n",
      "Epoch: 294, Loss: 0.7866, Val: 0.7166, Test: 0.7028\n",
      "Epoch: 295, Loss: 0.7835, Val: 0.7158, Test: 0.7020\n",
      "Epoch: 296, Loss: 0.7913, Val: 0.7166, Test: 0.7045\n",
      "Epoch: 297, Loss: 0.7917, Val: 0.7184, Test: 0.7066\n",
      "Epoch: 298, Loss: 0.7879, Val: 0.7191, Test: 0.7063\n",
      "Epoch: 299, Loss: 0.7859, Val: 0.7174, Test: 0.7046\n",
      "Epoch: 300, Loss: 0.7852, Val: 0.7162, Test: 0.7020\n",
      "0.35000000000000003\n",
      "Label_rate: 0.35000000000000003\n",
      "Epoch: 001, Loss: 2.1430, Val: 0.4011, Test: 0.2931\n",
      "Epoch: 002, Loss: 1.6907, Val: 0.5502, Test: 0.5443\n",
      "Epoch: 003, Loss: 1.4824, Val: 0.5847, Test: 0.5844\n",
      "Epoch: 004, Loss: 1.3925, Val: 0.6122, Test: 0.6025\n",
      "Epoch: 005, Loss: 1.2435, Val: 0.6199, Test: 0.6111\n",
      "Epoch: 006, Loss: 1.2122, Val: 0.6457, Test: 0.6400\n",
      "Epoch: 007, Loss: 1.1391, Val: 0.6497, Test: 0.6516\n",
      "Epoch: 008, Loss: 1.1387, Val: 0.6574, Test: 0.6474\n",
      "Epoch: 009, Loss: 1.1144, Val: 0.6549, Test: 0.6366\n",
      "Epoch: 010, Loss: 1.0858, Val: 0.6590, Test: 0.6406\n",
      "Epoch: 011, Loss: 1.0606, Val: 0.6664, Test: 0.6515\n",
      "Epoch: 012, Loss: 1.0606, Val: 0.6767, Test: 0.6606\n",
      "Epoch: 013, Loss: 1.0409, Val: 0.6802, Test: 0.6646\n",
      "Epoch: 014, Loss: 1.0304, Val: 0.6825, Test: 0.6678\n",
      "Epoch: 015, Loss: 1.0215, Val: 0.6841, Test: 0.6674\n",
      "Epoch: 016, Loss: 1.0112, Val: 0.6852, Test: 0.6652\n",
      "Epoch: 017, Loss: 0.9960, Val: 0.6847, Test: 0.6636\n",
      "Epoch: 018, Loss: 0.9838, Val: 0.6867, Test: 0.6654\n",
      "Epoch: 019, Loss: 0.9785, Val: 0.6893, Test: 0.6682\n",
      "Epoch: 020, Loss: 0.9725, Val: 0.6903, Test: 0.6702\n",
      "Epoch: 021, Loss: 0.9562, Val: 0.6899, Test: 0.6705\n",
      "Epoch: 022, Loss: 0.9506, Val: 0.6905, Test: 0.6691\n",
      "Epoch: 023, Loss: 0.9473, Val: 0.6908, Test: 0.6687\n",
      "Epoch: 024, Loss: 0.9377, Val: 0.6923, Test: 0.6693\n",
      "Epoch: 025, Loss: 0.9268, Val: 0.6942, Test: 0.6706\n",
      "Epoch: 026, Loss: 0.9256, Val: 0.6935, Test: 0.6720\n",
      "Epoch: 027, Loss: 0.9160, Val: 0.6946, Test: 0.6733\n",
      "Epoch: 028, Loss: 0.9172, Val: 0.6957, Test: 0.6731\n",
      "Epoch: 029, Loss: 0.9122, Val: 0.6960, Test: 0.6728\n",
      "Epoch: 030, Loss: 0.9028, Val: 0.6976, Test: 0.6737\n",
      "Epoch: 031, Loss: 0.9005, Val: 0.6962, Test: 0.6754\n",
      "Epoch: 032, Loss: 0.8951, Val: 0.6995, Test: 0.6778\n",
      "Epoch: 033, Loss: 0.8946, Val: 0.7017, Test: 0.6810\n",
      "Epoch: 034, Loss: 0.8917, Val: 0.7027, Test: 0.6826\n",
      "Epoch: 035, Loss: 0.8874, Val: 0.7038, Test: 0.6856\n",
      "Epoch: 036, Loss: 0.8845, Val: 0.7025, Test: 0.6827\n",
      "Epoch: 037, Loss: 0.8861, Val: 0.7006, Test: 0.6799\n",
      "Epoch: 038, Loss: 0.8807, Val: 0.7024, Test: 0.6812\n",
      "Epoch: 039, Loss: 0.8766, Val: 0.7069, Test: 0.6871\n",
      "Epoch: 040, Loss: 0.8773, Val: 0.7095, Test: 0.6912\n",
      "Epoch: 041, Loss: 0.8692, Val: 0.7077, Test: 0.6901\n",
      "Epoch: 042, Loss: 0.8631, Val: 0.7057, Test: 0.6862\n",
      "Epoch: 043, Loss: 0.8678, Val: 0.7038, Test: 0.6849\n",
      "Epoch: 044, Loss: 0.8594, Val: 0.7065, Test: 0.6895\n",
      "Epoch: 045, Loss: 0.8679, Val: 0.7088, Test: 0.6922\n",
      "Epoch: 046, Loss: 0.8614, Val: 0.7098, Test: 0.6920\n",
      "Epoch: 047, Loss: 0.8550, Val: 0.7089, Test: 0.6914\n",
      "Epoch: 048, Loss: 0.8564, Val: 0.7099, Test: 0.6912\n",
      "Epoch: 049, Loss: 0.8583, Val: 0.7082, Test: 0.6884\n",
      "Epoch: 050, Loss: 0.8547, Val: 0.7078, Test: 0.6872\n",
      "Epoch: 051, Loss: 0.8528, Val: 0.7090, Test: 0.6907\n",
      "Epoch: 052, Loss: 0.8531, Val: 0.7114, Test: 0.6944\n",
      "Epoch: 053, Loss: 0.8475, Val: 0.7121, Test: 0.6938\n",
      "Epoch: 054, Loss: 0.8513, Val: 0.7096, Test: 0.6909\n",
      "Epoch: 055, Loss: 0.8392, Val: 0.7083, Test: 0.6885\n",
      "Epoch: 056, Loss: 0.8486, Val: 0.7081, Test: 0.6880\n",
      "Epoch: 057, Loss: 0.8450, Val: 0.7099, Test: 0.6918\n",
      "Epoch: 058, Loss: 0.8482, Val: 0.7116, Test: 0.6944\n",
      "Epoch: 059, Loss: 0.8399, Val: 0.7103, Test: 0.6935\n",
      "Epoch: 060, Loss: 0.8464, Val: 0.7097, Test: 0.6926\n",
      "Epoch: 061, Loss: 0.8358, Val: 0.7102, Test: 0.6930\n",
      "Epoch: 062, Loss: 0.8339, Val: 0.7102, Test: 0.6943\n",
      "Epoch: 063, Loss: 0.8366, Val: 0.7111, Test: 0.6947\n",
      "Epoch: 064, Loss: 0.8307, Val: 0.7117, Test: 0.6957\n",
      "Epoch: 065, Loss: 0.8337, Val: 0.7105, Test: 0.6934\n",
      "Epoch: 066, Loss: 0.8384, Val: 0.7094, Test: 0.6923\n",
      "Epoch: 067, Loss: 0.8259, Val: 0.7101, Test: 0.6945\n",
      "Epoch: 068, Loss: 0.8280, Val: 0.7104, Test: 0.6957\n",
      "Epoch: 069, Loss: 0.8265, Val: 0.7106, Test: 0.6957\n",
      "Epoch: 070, Loss: 0.8339, Val: 0.7107, Test: 0.6958\n",
      "Epoch: 071, Loss: 0.8346, Val: 0.7098, Test: 0.6944\n",
      "Epoch: 072, Loss: 0.8282, Val: 0.7088, Test: 0.6908\n",
      "Epoch: 073, Loss: 0.8310, Val: 0.7092, Test: 0.6916\n",
      "Epoch: 074, Loss: 0.8319, Val: 0.7111, Test: 0.6965\n",
      "Epoch: 075, Loss: 0.8298, Val: 0.7108, Test: 0.6964\n",
      "Epoch: 076, Loss: 0.8291, Val: 0.7097, Test: 0.6929\n",
      "Epoch: 077, Loss: 0.8281, Val: 0.7091, Test: 0.6894\n",
      "Epoch: 078, Loss: 0.8275, Val: 0.7102, Test: 0.6930\n",
      "Epoch: 079, Loss: 0.8203, Val: 0.7128, Test: 0.6985\n",
      "Epoch: 080, Loss: 0.8254, Val: 0.7131, Test: 0.6983\n",
      "Epoch: 081, Loss: 0.8228, Val: 0.7111, Test: 0.6945\n",
      "Epoch: 082, Loss: 0.8237, Val: 0.7106, Test: 0.6943\n",
      "Epoch: 083, Loss: 0.8298, Val: 0.7127, Test: 0.6989\n",
      "Epoch: 084, Loss: 0.8217, Val: 0.7144, Test: 0.6998\n",
      "Epoch: 085, Loss: 0.8232, Val: 0.7124, Test: 0.6973\n",
      "Epoch: 086, Loss: 0.8272, Val: 0.7102, Test: 0.6944\n",
      "Epoch: 087, Loss: 0.8204, Val: 0.7109, Test: 0.6951\n",
      "Epoch: 088, Loss: 0.8256, Val: 0.7116, Test: 0.6958\n",
      "Epoch: 089, Loss: 0.8196, Val: 0.7119, Test: 0.6946\n",
      "Epoch: 090, Loss: 0.8193, Val: 0.7115, Test: 0.6948\n",
      "Epoch: 091, Loss: 0.8180, Val: 0.7122, Test: 0.6951\n",
      "Epoch: 092, Loss: 0.8150, Val: 0.7104, Test: 0.6937\n",
      "Epoch: 093, Loss: 0.8181, Val: 0.7104, Test: 0.6933\n",
      "Epoch: 094, Loss: 0.8218, Val: 0.7120, Test: 0.6952\n",
      "Epoch: 095, Loss: 0.8176, Val: 0.7124, Test: 0.6954\n",
      "Epoch: 096, Loss: 0.8199, Val: 0.7127, Test: 0.6964\n",
      "Epoch: 097, Loss: 0.8149, Val: 0.7122, Test: 0.6956\n",
      "Epoch: 098, Loss: 0.8178, Val: 0.7112, Test: 0.6926\n",
      "Epoch: 099, Loss: 0.8120, Val: 0.7106, Test: 0.6923\n",
      "Epoch: 100, Loss: 0.8176, Val: 0.7119, Test: 0.6955\n",
      "Epoch: 101, Loss: 0.8201, Val: 0.7134, Test: 0.6984\n",
      "Epoch: 102, Loss: 0.8121, Val: 0.7131, Test: 0.6982\n",
      "Epoch: 103, Loss: 0.8212, Val: 0.7118, Test: 0.6949\n",
      "Epoch: 104, Loss: 0.8144, Val: 0.7104, Test: 0.6941\n",
      "Epoch: 105, Loss: 0.8118, Val: 0.7120, Test: 0.6974\n",
      "Epoch: 106, Loss: 0.8142, Val: 0.7122, Test: 0.6970\n",
      "Epoch: 107, Loss: 0.8156, Val: 0.7110, Test: 0.6938\n",
      "Epoch: 108, Loss: 0.8121, Val: 0.7118, Test: 0.6951\n",
      "Epoch: 109, Loss: 0.8106, Val: 0.7131, Test: 0.6980\n",
      "Epoch: 110, Loss: 0.8159, Val: 0.7137, Test: 0.6993\n",
      "Epoch: 111, Loss: 0.8101, Val: 0.7129, Test: 0.6960\n",
      "Epoch: 112, Loss: 0.8139, Val: 0.7127, Test: 0.6960\n",
      "Epoch: 113, Loss: 0.8114, Val: 0.7136, Test: 0.6982\n",
      "Epoch: 114, Loss: 0.8138, Val: 0.7146, Test: 0.6993\n",
      "Epoch: 115, Loss: 0.8086, Val: 0.7136, Test: 0.6987\n",
      "Epoch: 116, Loss: 0.8044, Val: 0.7117, Test: 0.6940\n",
      "Epoch: 117, Loss: 0.8112, Val: 0.7117, Test: 0.6934\n",
      "Epoch: 118, Loss: 0.8076, Val: 0.7133, Test: 0.6967\n",
      "Epoch: 119, Loss: 0.8127, Val: 0.7135, Test: 0.6965\n",
      "Epoch: 120, Loss: 0.8077, Val: 0.7122, Test: 0.6928\n",
      "Epoch: 121, Loss: 0.8113, Val: 0.7118, Test: 0.6926\n",
      "Epoch: 122, Loss: 0.8040, Val: 0.7126, Test: 0.6944\n",
      "Epoch: 123, Loss: 0.8079, Val: 0.7127, Test: 0.6954\n",
      "Epoch: 124, Loss: 0.8056, Val: 0.7135, Test: 0.6967\n",
      "Epoch: 125, Loss: 0.8052, Val: 0.7154, Test: 0.6981\n",
      "Epoch: 126, Loss: 0.8022, Val: 0.7146, Test: 0.6983\n",
      "Epoch: 127, Loss: 0.8067, Val: 0.7128, Test: 0.6938\n",
      "Epoch: 128, Loss: 0.8064, Val: 0.7108, Test: 0.6909\n",
      "Epoch: 129, Loss: 0.8121, Val: 0.7124, Test: 0.6936\n",
      "Epoch: 130, Loss: 0.8032, Val: 0.7138, Test: 0.6966\n",
      "Epoch: 131, Loss: 0.8028, Val: 0.7137, Test: 0.6957\n",
      "Epoch: 132, Loss: 0.8016, Val: 0.7105, Test: 0.6910\n",
      "Epoch: 133, Loss: 0.7986, Val: 0.7103, Test: 0.6904\n",
      "Epoch: 134, Loss: 0.8088, Val: 0.7120, Test: 0.6938\n",
      "Epoch: 135, Loss: 0.8052, Val: 0.7142, Test: 0.6960\n",
      "Epoch: 136, Loss: 0.7996, Val: 0.7140, Test: 0.6963\n",
      "Epoch: 137, Loss: 0.8057, Val: 0.7121, Test: 0.6944\n",
      "Epoch: 138, Loss: 0.7943, Val: 0.7123, Test: 0.6950\n",
      "Epoch: 139, Loss: 0.8012, Val: 0.7135, Test: 0.6990\n",
      "Epoch: 140, Loss: 0.8059, Val: 0.7158, Test: 0.7021\n",
      "Epoch: 141, Loss: 0.8025, Val: 0.7163, Test: 0.7003\n",
      "Epoch: 142, Loss: 0.8028, Val: 0.7137, Test: 0.6961\n",
      "Epoch: 143, Loss: 0.8014, Val: 0.7115, Test: 0.6940\n",
      "Epoch: 144, Loss: 0.8068, Val: 0.7137, Test: 0.6986\n",
      "Epoch: 145, Loss: 0.8019, Val: 0.7147, Test: 0.7009\n",
      "Epoch: 146, Loss: 0.8082, Val: 0.7153, Test: 0.6978\n",
      "Epoch: 147, Loss: 0.7963, Val: 0.7130, Test: 0.6958\n",
      "Epoch: 148, Loss: 0.8004, Val: 0.7124, Test: 0.6958\n",
      "Epoch: 149, Loss: 0.8033, Val: 0.7137, Test: 0.6980\n",
      "Epoch: 150, Loss: 0.8023, Val: 0.7155, Test: 0.7001\n",
      "Epoch: 151, Loss: 0.7997, Val: 0.7143, Test: 0.6968\n",
      "Epoch: 152, Loss: 0.8053, Val: 0.7115, Test: 0.6929\n",
      "Epoch: 153, Loss: 0.8014, Val: 0.7133, Test: 0.6957\n",
      "Epoch: 154, Loss: 0.7983, Val: 0.7143, Test: 0.6976\n",
      "Epoch: 155, Loss: 0.8001, Val: 0.7150, Test: 0.6978\n",
      "Epoch: 156, Loss: 0.8038, Val: 0.7141, Test: 0.6952\n",
      "Epoch: 157, Loss: 0.8026, Val: 0.7113, Test: 0.6923\n",
      "Epoch: 158, Loss: 0.8037, Val: 0.7128, Test: 0.6943\n",
      "Epoch: 159, Loss: 0.7977, Val: 0.7150, Test: 0.6978\n",
      "Epoch: 160, Loss: 0.7944, Val: 0.7154, Test: 0.6984\n",
      "Epoch: 161, Loss: 0.7982, Val: 0.7132, Test: 0.6952\n",
      "Epoch: 162, Loss: 0.7969, Val: 0.7133, Test: 0.6945\n",
      "Epoch: 163, Loss: 0.7910, Val: 0.7156, Test: 0.6999\n",
      "Epoch: 164, Loss: 0.7979, Val: 0.7164, Test: 0.7010\n",
      "Epoch: 165, Loss: 0.7941, Val: 0.7133, Test: 0.6964\n",
      "Epoch: 166, Loss: 0.8035, Val: 0.7121, Test: 0.6926\n",
      "Epoch: 167, Loss: 0.7935, Val: 0.7133, Test: 0.6941\n",
      "Epoch: 168, Loss: 0.8037, Val: 0.7161, Test: 0.7008\n",
      "Epoch: 169, Loss: 0.8007, Val: 0.7154, Test: 0.7002\n",
      "Epoch: 170, Loss: 0.7986, Val: 0.7123, Test: 0.6941\n",
      "Epoch: 171, Loss: 0.7991, Val: 0.7112, Test: 0.6916\n",
      "Epoch: 172, Loss: 0.8004, Val: 0.7152, Test: 0.6976\n",
      "Epoch: 173, Loss: 0.8016, Val: 0.7167, Test: 0.7014\n",
      "Epoch: 174, Loss: 0.7965, Val: 0.7148, Test: 0.6965\n",
      "Epoch: 175, Loss: 0.7931, Val: 0.7113, Test: 0.6929\n",
      "Epoch: 176, Loss: 0.7995, Val: 0.7112, Test: 0.6939\n",
      "Epoch: 177, Loss: 0.7977, Val: 0.7148, Test: 0.7000\n",
      "Epoch: 178, Loss: 0.7961, Val: 0.7143, Test: 0.6986\n",
      "Epoch: 179, Loss: 0.7940, Val: 0.7119, Test: 0.6928\n",
      "Epoch: 180, Loss: 0.7997, Val: 0.7108, Test: 0.6923\n",
      "Epoch: 181, Loss: 0.8030, Val: 0.7131, Test: 0.6980\n",
      "Epoch: 182, Loss: 0.7926, Val: 0.7152, Test: 0.6977\n",
      "Epoch: 183, Loss: 0.7955, Val: 0.7135, Test: 0.6955\n",
      "Epoch: 184, Loss: 0.7938, Val: 0.7135, Test: 0.6981\n",
      "Epoch: 185, Loss: 0.7976, Val: 0.7131, Test: 0.6992\n",
      "Epoch: 186, Loss: 0.7959, Val: 0.7132, Test: 0.6950\n",
      "Epoch: 187, Loss: 0.7935, Val: 0.7135, Test: 0.6949\n",
      "Epoch: 188, Loss: 0.7921, Val: 0.7160, Test: 0.7016\n",
      "Epoch: 189, Loss: 0.7980, Val: 0.7148, Test: 0.6985\n",
      "Epoch: 190, Loss: 0.7900, Val: 0.7138, Test: 0.6956\n",
      "Epoch: 191, Loss: 0.7948, Val: 0.7139, Test: 0.6981\n",
      "Epoch: 192, Loss: 0.7933, Val: 0.7158, Test: 0.7018\n",
      "Epoch: 193, Loss: 0.7975, Val: 0.7150, Test: 0.6989\n",
      "Epoch: 194, Loss: 0.7896, Val: 0.7132, Test: 0.6949\n",
      "Epoch: 195, Loss: 0.7883, Val: 0.7148, Test: 0.6970\n",
      "Epoch: 196, Loss: 0.7949, Val: 0.7161, Test: 0.6988\n",
      "Epoch: 197, Loss: 0.7929, Val: 0.7145, Test: 0.6981\n",
      "Epoch: 198, Loss: 0.7957, Val: 0.7132, Test: 0.6951\n",
      "Epoch: 199, Loss: 0.7910, Val: 0.7141, Test: 0.6981\n",
      "Epoch: 200, Loss: 0.7904, Val: 0.7159, Test: 0.7006\n",
      "Epoch: 201, Loss: 0.7914, Val: 0.7152, Test: 0.6983\n",
      "Epoch: 202, Loss: 0.7954, Val: 0.7137, Test: 0.6944\n",
      "Epoch: 203, Loss: 0.7884, Val: 0.7140, Test: 0.6966\n",
      "Epoch: 204, Loss: 0.7855, Val: 0.7157, Test: 0.7011\n",
      "Epoch: 205, Loss: 0.7938, Val: 0.7166, Test: 0.7029\n",
      "Epoch: 206, Loss: 0.7953, Val: 0.7143, Test: 0.6982\n",
      "Epoch: 207, Loss: 0.7910, Val: 0.7111, Test: 0.6939\n",
      "Epoch: 208, Loss: 0.7902, Val: 0.7124, Test: 0.6970\n",
      "Epoch: 209, Loss: 0.7846, Val: 0.7161, Test: 0.7002\n",
      "Epoch: 210, Loss: 0.7990, Val: 0.7160, Test: 0.6992\n",
      "Epoch: 211, Loss: 0.7969, Val: 0.7131, Test: 0.6967\n",
      "Epoch: 212, Loss: 0.7945, Val: 0.7126, Test: 0.6963\n",
      "Epoch: 213, Loss: 0.7819, Val: 0.7161, Test: 0.6986\n",
      "Epoch: 214, Loss: 0.7911, Val: 0.7175, Test: 0.7022\n",
      "Epoch: 215, Loss: 0.7935, Val: 0.7155, Test: 0.6969\n",
      "Epoch: 216, Loss: 0.7864, Val: 0.7122, Test: 0.6935\n",
      "Epoch: 217, Loss: 0.7868, Val: 0.7136, Test: 0.6971\n",
      "Epoch: 218, Loss: 0.7855, Val: 0.7167, Test: 0.7018\n",
      "Epoch: 219, Loss: 0.7927, Val: 0.7151, Test: 0.7000\n",
      "Epoch: 220, Loss: 0.7935, Val: 0.7125, Test: 0.6949\n",
      "Epoch: 221, Loss: 0.7917, Val: 0.7130, Test: 0.6960\n",
      "Epoch: 222, Loss: 0.7935, Val: 0.7168, Test: 0.7014\n",
      "Epoch: 223, Loss: 0.7861, Val: 0.7178, Test: 0.7028\n",
      "Epoch: 224, Loss: 0.7857, Val: 0.7158, Test: 0.7002\n",
      "Epoch: 225, Loss: 0.7888, Val: 0.7142, Test: 0.6965\n",
      "Epoch: 226, Loss: 0.7883, Val: 0.7155, Test: 0.6983\n",
      "Epoch: 227, Loss: 0.7946, Val: 0.7168, Test: 0.7006\n",
      "Epoch: 228, Loss: 0.7904, Val: 0.7162, Test: 0.7005\n",
      "Epoch: 229, Loss: 0.7875, Val: 0.7149, Test: 0.6980\n",
      "Epoch: 230, Loss: 0.7817, Val: 0.7133, Test: 0.6936\n",
      "Epoch: 231, Loss: 0.7892, Val: 0.7142, Test: 0.6963\n",
      "Epoch: 232, Loss: 0.7916, Val: 0.7152, Test: 0.6978\n",
      "Epoch: 233, Loss: 0.7898, Val: 0.7143, Test: 0.6980\n",
      "Epoch: 234, Loss: 0.7936, Val: 0.7146, Test: 0.7011\n",
      "Epoch: 235, Loss: 0.7834, Val: 0.7143, Test: 0.6992\n",
      "Epoch: 236, Loss: 0.7903, Val: 0.7121, Test: 0.6940\n",
      "Epoch: 237, Loss: 0.7856, Val: 0.7143, Test: 0.6971\n",
      "Epoch: 238, Loss: 0.7863, Val: 0.7165, Test: 0.7016\n",
      "Epoch: 239, Loss: 0.7809, Val: 0.7166, Test: 0.7025\n",
      "Epoch: 240, Loss: 0.7881, Val: 0.7142, Test: 0.6973\n",
      "Epoch: 241, Loss: 0.7906, Val: 0.7135, Test: 0.6962\n",
      "Epoch: 242, Loss: 0.7846, Val: 0.7159, Test: 0.7004\n",
      "Epoch: 243, Loss: 0.7920, Val: 0.7172, Test: 0.7026\n",
      "Epoch: 244, Loss: 0.7847, Val: 0.7175, Test: 0.7017\n",
      "Epoch: 245, Loss: 0.7857, Val: 0.7150, Test: 0.6970\n",
      "Epoch: 246, Loss: 0.7902, Val: 0.7139, Test: 0.6979\n",
      "Epoch: 247, Loss: 0.7896, Val: 0.7164, Test: 0.7017\n",
      "Epoch: 248, Loss: 0.7815, Val: 0.7173, Test: 0.7024\n",
      "Epoch: 249, Loss: 0.7844, Val: 0.7163, Test: 0.7027\n",
      "Epoch: 250, Loss: 0.7900, Val: 0.7136, Test: 0.6982\n",
      "Epoch: 251, Loss: 0.7867, Val: 0.7140, Test: 0.6979\n",
      "Epoch: 252, Loss: 0.7896, Val: 0.7155, Test: 0.6993\n",
      "Epoch: 253, Loss: 0.7842, Val: 0.7175, Test: 0.7013\n",
      "Epoch: 254, Loss: 0.7842, Val: 0.7169, Test: 0.7019\n",
      "Epoch: 255, Loss: 0.7867, Val: 0.7160, Test: 0.7002\n",
      "Epoch: 256, Loss: 0.7853, Val: 0.7151, Test: 0.6993\n",
      "Epoch: 257, Loss: 0.7903, Val: 0.7135, Test: 0.6975\n",
      "Epoch: 258, Loss: 0.7853, Val: 0.7144, Test: 0.6971\n",
      "Epoch: 259, Loss: 0.7857, Val: 0.7162, Test: 0.7009\n",
      "Epoch: 260, Loss: 0.7809, Val: 0.7175, Test: 0.7022\n",
      "Epoch: 261, Loss: 0.7847, Val: 0.7166, Test: 0.7019\n",
      "Epoch: 262, Loss: 0.7827, Val: 0.7158, Test: 0.6994\n",
      "Epoch: 263, Loss: 0.7877, Val: 0.7161, Test: 0.7008\n",
      "Epoch: 264, Loss: 0.7804, Val: 0.7165, Test: 0.7006\n",
      "Epoch: 265, Loss: 0.7813, Val: 0.7162, Test: 0.7018\n",
      "Epoch: 266, Loss: 0.7838, Val: 0.7153, Test: 0.7016\n",
      "Epoch: 267, Loss: 0.7853, Val: 0.7147, Test: 0.7006\n",
      "Epoch: 268, Loss: 0.7798, Val: 0.7160, Test: 0.6995\n",
      "Epoch: 269, Loss: 0.7862, Val: 0.7158, Test: 0.6968\n",
      "Epoch: 270, Loss: 0.7822, Val: 0.7160, Test: 0.6977\n",
      "Epoch: 271, Loss: 0.7846, Val: 0.7156, Test: 0.6997\n",
      "Epoch: 272, Loss: 0.7833, Val: 0.7160, Test: 0.6996\n",
      "Epoch: 273, Loss: 0.7827, Val: 0.7153, Test: 0.6985\n",
      "Epoch: 274, Loss: 0.7817, Val: 0.7155, Test: 0.7019\n",
      "Epoch: 275, Loss: 0.7860, Val: 0.7172, Test: 0.7042\n",
      "Epoch: 276, Loss: 0.7772, Val: 0.7169, Test: 0.7011\n",
      "Epoch: 277, Loss: 0.7810, Val: 0.7157, Test: 0.6981\n",
      "Epoch: 278, Loss: 0.7807, Val: 0.7146, Test: 0.7003\n",
      "Epoch: 279, Loss: 0.7865, Val: 0.7147, Test: 0.6977\n",
      "Epoch: 280, Loss: 0.7804, Val: 0.7150, Test: 0.6996\n",
      "Epoch: 281, Loss: 0.7859, Val: 0.7186, Test: 0.7041\n",
      "Epoch: 282, Loss: 0.7822, Val: 0.7160, Test: 0.7035\n",
      "Epoch: 283, Loss: 0.7819, Val: 0.7149, Test: 0.6969\n",
      "Epoch: 284, Loss: 0.7786, Val: 0.7123, Test: 0.6925\n",
      "Epoch: 285, Loss: 0.7880, Val: 0.7171, Test: 0.7013\n",
      "Epoch: 286, Loss: 0.7812, Val: 0.7182, Test: 0.7060\n",
      "Epoch: 287, Loss: 0.7807, Val: 0.7164, Test: 0.7007\n",
      "Epoch: 288, Loss: 0.7749, Val: 0.7134, Test: 0.6958\n",
      "Epoch: 289, Loss: 0.7794, Val: 0.7145, Test: 0.6994\n",
      "Epoch: 290, Loss: 0.7814, Val: 0.7181, Test: 0.7041\n",
      "Epoch: 291, Loss: 0.7849, Val: 0.7163, Test: 0.7004\n",
      "Epoch: 292, Loss: 0.7788, Val: 0.7133, Test: 0.6969\n",
      "Epoch: 293, Loss: 0.7837, Val: 0.7174, Test: 0.7025\n",
      "Epoch: 294, Loss: 0.7811, Val: 0.7188, Test: 0.7071\n",
      "Epoch: 295, Loss: 0.7794, Val: 0.7174, Test: 0.7023\n",
      "Epoch: 296, Loss: 0.7814, Val: 0.7149, Test: 0.6978\n",
      "Epoch: 297, Loss: 0.7838, Val: 0.7169, Test: 0.7030\n",
      "Epoch: 298, Loss: 0.7837, Val: 0.7170, Test: 0.7055\n",
      "Epoch: 299, Loss: 0.7771, Val: 0.7181, Test: 0.7029\n",
      "Epoch: 300, Loss: 0.7770, Val: 0.7151, Test: 0.6967\n",
      "0.4\n",
      "Label_rate: 0.4\n",
      "Epoch: 001, Loss: 2.9737, Val: 0.4318, Test: 0.3524\n",
      "Epoch: 002, Loss: 1.8942, Val: 0.5460, Test: 0.5217\n",
      "Epoch: 003, Loss: 1.6591, Val: 0.5974, Test: 0.5773\n",
      "Epoch: 004, Loss: 1.4131, Val: 0.6220, Test: 0.6022\n",
      "Epoch: 005, Loss: 1.3166, Val: 0.6360, Test: 0.6256\n",
      "Epoch: 006, Loss: 1.2868, Val: 0.6467, Test: 0.6401\n",
      "Epoch: 007, Loss: 1.2595, Val: 0.6650, Test: 0.6592\n",
      "Epoch: 008, Loss: 1.1997, Val: 0.6646, Test: 0.6590\n",
      "Epoch: 009, Loss: 1.1810, Val: 0.6693, Test: 0.6604\n",
      "Epoch: 010, Loss: 1.1239, Val: 0.6728, Test: 0.6647\n",
      "Epoch: 011, Loss: 1.1100, Val: 0.6814, Test: 0.6738\n",
      "Epoch: 012, Loss: 1.0765, Val: 0.6883, Test: 0.6805\n",
      "Epoch: 013, Loss: 1.0691, Val: 0.6871, Test: 0.6795\n",
      "Epoch: 014, Loss: 1.0547, Val: 0.6885, Test: 0.6796\n",
      "Epoch: 015, Loss: 1.0378, Val: 0.6881, Test: 0.6799\n",
      "Epoch: 016, Loss: 1.0259, Val: 0.6913, Test: 0.6836\n",
      "Epoch: 017, Loss: 1.0320, Val: 0.6979, Test: 0.6910\n",
      "Epoch: 018, Loss: 1.0166, Val: 0.7010, Test: 0.6929\n",
      "Epoch: 019, Loss: 0.9962, Val: 0.6993, Test: 0.6884\n",
      "Epoch: 020, Loss: 0.9957, Val: 0.6993, Test: 0.6885\n",
      "Epoch: 021, Loss: 0.9828, Val: 0.7011, Test: 0.6916\n",
      "Epoch: 022, Loss: 0.9792, Val: 0.7038, Test: 0.6967\n",
      "Epoch: 023, Loss: 0.9767, Val: 0.7070, Test: 0.7015\n",
      "Epoch: 024, Loss: 0.9702, Val: 0.7057, Test: 0.7011\n",
      "Epoch: 025, Loss: 0.9554, Val: 0.7045, Test: 0.6985\n",
      "Epoch: 026, Loss: 0.9497, Val: 0.7050, Test: 0.6999\n",
      "Epoch: 027, Loss: 0.9492, Val: 0.7053, Test: 0.7018\n",
      "Epoch: 028, Loss: 0.9449, Val: 0.7062, Test: 0.7028\n",
      "Epoch: 029, Loss: 0.9356, Val: 0.7061, Test: 0.7021\n",
      "Epoch: 030, Loss: 0.9296, Val: 0.7059, Test: 0.7030\n",
      "Epoch: 031, Loss: 0.9380, Val: 0.7081, Test: 0.7040\n",
      "Epoch: 032, Loss: 0.9324, Val: 0.7083, Test: 0.7054\n",
      "Epoch: 033, Loss: 0.9234, Val: 0.7100, Test: 0.7062\n",
      "Epoch: 034, Loss: 0.9167, Val: 0.7098, Test: 0.7065\n",
      "Epoch: 035, Loss: 0.9106, Val: 0.7104, Test: 0.7066\n",
      "Epoch: 036, Loss: 0.9059, Val: 0.7107, Test: 0.7068\n",
      "Epoch: 037, Loss: 0.9086, Val: 0.7101, Test: 0.7063\n",
      "Epoch: 038, Loss: 0.8977, Val: 0.7111, Test: 0.7084\n",
      "Epoch: 039, Loss: 0.9019, Val: 0.7129, Test: 0.7099\n",
      "Epoch: 040, Loss: 0.8986, Val: 0.7136, Test: 0.7096\n",
      "Epoch: 041, Loss: 0.8952, Val: 0.7131, Test: 0.7078\n",
      "Epoch: 042, Loss: 0.8947, Val: 0.7113, Test: 0.7071\n",
      "Epoch: 043, Loss: 0.8887, Val: 0.7128, Test: 0.7092\n",
      "Epoch: 044, Loss: 0.8816, Val: 0.7144, Test: 0.7115\n",
      "Epoch: 045, Loss: 0.8838, Val: 0.7150, Test: 0.7129\n",
      "Epoch: 046, Loss: 0.8807, Val: 0.7149, Test: 0.7123\n",
      "Epoch: 047, Loss: 0.8760, Val: 0.7137, Test: 0.7098\n",
      "Epoch: 048, Loss: 0.8787, Val: 0.7126, Test: 0.7080\n",
      "Epoch: 049, Loss: 0.8798, Val: 0.7139, Test: 0.7085\n",
      "Epoch: 050, Loss: 0.8795, Val: 0.7139, Test: 0.7084\n",
      "Epoch: 051, Loss: 0.8813, Val: 0.7147, Test: 0.7083\n",
      "Epoch: 052, Loss: 0.8688, Val: 0.7146, Test: 0.7084\n",
      "Epoch: 053, Loss: 0.8689, Val: 0.7146, Test: 0.7083\n",
      "Epoch: 054, Loss: 0.8711, Val: 0.7141, Test: 0.7068\n",
      "Epoch: 055, Loss: 0.8662, Val: 0.7142, Test: 0.7072\n",
      "Epoch: 056, Loss: 0.8623, Val: 0.7144, Test: 0.7081\n",
      "Epoch: 057, Loss: 0.8653, Val: 0.7146, Test: 0.7089\n",
      "Epoch: 058, Loss: 0.8679, Val: 0.7150, Test: 0.7089\n",
      "Epoch: 059, Loss: 0.8631, Val: 0.7144, Test: 0.7075\n",
      "Epoch: 060, Loss: 0.8641, Val: 0.7132, Test: 0.7068\n",
      "Epoch: 061, Loss: 0.8516, Val: 0.7145, Test: 0.7077\n",
      "Epoch: 062, Loss: 0.8569, Val: 0.7152, Test: 0.7085\n",
      "Epoch: 063, Loss: 0.8530, Val: 0.7149, Test: 0.7082\n",
      "Epoch: 064, Loss: 0.8573, Val: 0.7140, Test: 0.7071\n",
      "Epoch: 065, Loss: 0.8545, Val: 0.7137, Test: 0.7052\n",
      "Epoch: 066, Loss: 0.8548, Val: 0.7136, Test: 0.7048\n",
      "Epoch: 067, Loss: 0.8545, Val: 0.7152, Test: 0.7067\n",
      "Epoch: 068, Loss: 0.8477, Val: 0.7155, Test: 0.7075\n",
      "Epoch: 069, Loss: 0.8474, Val: 0.7146, Test: 0.7065\n",
      "Epoch: 070, Loss: 0.8449, Val: 0.7139, Test: 0.7052\n",
      "Epoch: 071, Loss: 0.8504, Val: 0.7132, Test: 0.7049\n",
      "Epoch: 072, Loss: 0.8460, Val: 0.7140, Test: 0.7057\n",
      "Epoch: 073, Loss: 0.8499, Val: 0.7146, Test: 0.7065\n",
      "Epoch: 074, Loss: 0.8402, Val: 0.7147, Test: 0.7069\n",
      "Epoch: 075, Loss: 0.8434, Val: 0.7154, Test: 0.7074\n",
      "Epoch: 076, Loss: 0.8458, Val: 0.7163, Test: 0.7074\n",
      "Epoch: 077, Loss: 0.8448, Val: 0.7151, Test: 0.7072\n",
      "Epoch: 078, Loss: 0.8434, Val: 0.7146, Test: 0.7062\n",
      "Epoch: 079, Loss: 0.8477, Val: 0.7142, Test: 0.7060\n",
      "Epoch: 080, Loss: 0.8425, Val: 0.7153, Test: 0.7057\n",
      "Epoch: 081, Loss: 0.8371, Val: 0.7158, Test: 0.7061\n",
      "Epoch: 082, Loss: 0.8451, Val: 0.7167, Test: 0.7077\n",
      "Epoch: 083, Loss: 0.8456, Val: 0.7157, Test: 0.7064\n",
      "Epoch: 084, Loss: 0.8374, Val: 0.7136, Test: 0.7047\n",
      "Epoch: 085, Loss: 0.8393, Val: 0.7154, Test: 0.7064\n",
      "Epoch: 086, Loss: 0.8407, Val: 0.7170, Test: 0.7084\n",
      "Epoch: 087, Loss: 0.8491, Val: 0.7172, Test: 0.7093\n",
      "Epoch: 088, Loss: 0.8384, Val: 0.7165, Test: 0.7078\n",
      "Epoch: 089, Loss: 0.8364, Val: 0.7154, Test: 0.7056\n",
      "Epoch: 090, Loss: 0.8314, Val: 0.7158, Test: 0.7055\n",
      "Epoch: 091, Loss: 0.8360, Val: 0.7157, Test: 0.7062\n",
      "Epoch: 092, Loss: 0.8348, Val: 0.7173, Test: 0.7080\n",
      "Epoch: 093, Loss: 0.8350, Val: 0.7182, Test: 0.7103\n",
      "Epoch: 094, Loss: 0.8303, Val: 0.7177, Test: 0.7082\n",
      "Epoch: 095, Loss: 0.8287, Val: 0.7152, Test: 0.7055\n",
      "Epoch: 096, Loss: 0.8247, Val: 0.7172, Test: 0.7077\n",
      "Epoch: 097, Loss: 0.8294, Val: 0.7196, Test: 0.7105\n",
      "Epoch: 098, Loss: 0.8266, Val: 0.7182, Test: 0.7094\n",
      "Epoch: 099, Loss: 0.8293, Val: 0.7175, Test: 0.7071\n",
      "Epoch: 100, Loss: 0.8329, Val: 0.7162, Test: 0.7056\n",
      "Epoch: 101, Loss: 0.8336, Val: 0.7170, Test: 0.7063\n",
      "Epoch: 102, Loss: 0.8339, Val: 0.7182, Test: 0.7090\n",
      "Epoch: 103, Loss: 0.8293, Val: 0.7188, Test: 0.7093\n",
      "Epoch: 104, Loss: 0.8248, Val: 0.7168, Test: 0.7066\n",
      "Epoch: 105, Loss: 0.8297, Val: 0.7157, Test: 0.7041\n",
      "Epoch: 106, Loss: 0.8222, Val: 0.7176, Test: 0.7064\n",
      "Epoch: 107, Loss: 0.8294, Val: 0.7189, Test: 0.7092\n",
      "Epoch: 108, Loss: 0.8244, Val: 0.7192, Test: 0.7098\n",
      "Epoch: 109, Loss: 0.8256, Val: 0.7180, Test: 0.7067\n",
      "Epoch: 110, Loss: 0.8211, Val: 0.7156, Test: 0.7047\n",
      "Epoch: 111, Loss: 0.8245, Val: 0.7184, Test: 0.7079\n",
      "Epoch: 112, Loss: 0.8177, Val: 0.7184, Test: 0.7098\n",
      "Epoch: 113, Loss: 0.8210, Val: 0.7186, Test: 0.7074\n",
      "Epoch: 114, Loss: 0.8217, Val: 0.7143, Test: 0.7022\n",
      "Epoch: 115, Loss: 0.8230, Val: 0.7165, Test: 0.7044\n",
      "Epoch: 116, Loss: 0.8194, Val: 0.7187, Test: 0.7098\n",
      "Epoch: 117, Loss: 0.8261, Val: 0.7192, Test: 0.7114\n",
      "Epoch: 118, Loss: 0.8196, Val: 0.7197, Test: 0.7093\n",
      "Epoch: 119, Loss: 0.8218, Val: 0.7174, Test: 0.7055\n",
      "Epoch: 120, Loss: 0.8194, Val: 0.7190, Test: 0.7075\n",
      "Epoch: 121, Loss: 0.8163, Val: 0.7182, Test: 0.7087\n",
      "Epoch: 122, Loss: 0.8179, Val: 0.7180, Test: 0.7068\n",
      "Epoch: 123, Loss: 0.8209, Val: 0.7183, Test: 0.7061\n",
      "Epoch: 124, Loss: 0.8214, Val: 0.7192, Test: 0.7070\n",
      "Epoch: 125, Loss: 0.8129, Val: 0.7198, Test: 0.7075\n",
      "Epoch: 126, Loss: 0.8202, Val: 0.7192, Test: 0.7068\n",
      "Epoch: 127, Loss: 0.8122, Val: 0.7186, Test: 0.7057\n",
      "Epoch: 128, Loss: 0.8170, Val: 0.7181, Test: 0.7058\n",
      "Epoch: 129, Loss: 0.8196, Val: 0.7179, Test: 0.7061\n",
      "Epoch: 130, Loss: 0.8110, Val: 0.7176, Test: 0.7069\n",
      "Epoch: 131, Loss: 0.8266, Val: 0.7179, Test: 0.7060\n",
      "Epoch: 132, Loss: 0.8144, Val: 0.7180, Test: 0.7057\n",
      "Epoch: 133, Loss: 0.8171, Val: 0.7194, Test: 0.7064\n",
      "Epoch: 134, Loss: 0.8167, Val: 0.7194, Test: 0.7078\n",
      "Epoch: 135, Loss: 0.8055, Val: 0.7188, Test: 0.7069\n",
      "Epoch: 136, Loss: 0.8169, Val: 0.7171, Test: 0.7049\n",
      "Epoch: 137, Loss: 0.8170, Val: 0.7164, Test: 0.7037\n",
      "Epoch: 138, Loss: 0.8256, Val: 0.7166, Test: 0.7043\n",
      "Epoch: 139, Loss: 0.8137, Val: 0.7189, Test: 0.7074\n",
      "Epoch: 140, Loss: 0.8155, Val: 0.7186, Test: 0.7085\n",
      "Epoch: 141, Loss: 0.8158, Val: 0.7173, Test: 0.7069\n",
      "Epoch: 142, Loss: 0.8113, Val: 0.7179, Test: 0.7067\n",
      "Epoch: 143, Loss: 0.8169, Val: 0.7181, Test: 0.7068\n",
      "Epoch: 144, Loss: 0.8098, Val: 0.7187, Test: 0.7077\n",
      "Epoch: 145, Loss: 0.8086, Val: 0.7168, Test: 0.7052\n",
      "Epoch: 146, Loss: 0.8138, Val: 0.7158, Test: 0.7034\n",
      "Epoch: 147, Loss: 0.8078, Val: 0.7189, Test: 0.7078\n",
      "Epoch: 148, Loss: 0.8143, Val: 0.7186, Test: 0.7103\n",
      "Epoch: 149, Loss: 0.8130, Val: 0.7178, Test: 0.7077\n",
      "Epoch: 150, Loss: 0.8115, Val: 0.7165, Test: 0.7063\n",
      "Epoch: 151, Loss: 0.8117, Val: 0.7178, Test: 0.7081\n",
      "Epoch: 152, Loss: 0.8122, Val: 0.7204, Test: 0.7104\n",
      "Epoch: 153, Loss: 0.8192, Val: 0.7202, Test: 0.7090\n",
      "Epoch: 154, Loss: 0.8100, Val: 0.7182, Test: 0.7064\n",
      "Epoch: 155, Loss: 0.8046, Val: 0.7189, Test: 0.7079\n",
      "Epoch: 156, Loss: 0.8034, Val: 0.7195, Test: 0.7104\n",
      "Epoch: 157, Loss: 0.8079, Val: 0.7200, Test: 0.7100\n",
      "Epoch: 158, Loss: 0.8130, Val: 0.7199, Test: 0.7088\n",
      "Epoch: 159, Loss: 0.8028, Val: 0.7191, Test: 0.7071\n",
      "Epoch: 160, Loss: 0.8100, Val: 0.7182, Test: 0.7067\n",
      "Epoch: 161, Loss: 0.8114, Val: 0.7195, Test: 0.7094\n",
      "Epoch: 162, Loss: 0.8127, Val: 0.7198, Test: 0.7096\n",
      "Epoch: 163, Loss: 0.8079, Val: 0.7187, Test: 0.7079\n",
      "Epoch: 164, Loss: 0.8125, Val: 0.7165, Test: 0.7054\n",
      "Epoch: 165, Loss: 0.8086, Val: 0.7172, Test: 0.7062\n",
      "Epoch: 166, Loss: 0.8047, Val: 0.7195, Test: 0.7095\n",
      "Epoch: 167, Loss: 0.8050, Val: 0.7196, Test: 0.7106\n",
      "Epoch: 168, Loss: 0.8027, Val: 0.7181, Test: 0.7085\n",
      "Epoch: 169, Loss: 0.8070, Val: 0.7191, Test: 0.7084\n",
      "Epoch: 170, Loss: 0.8058, Val: 0.7203, Test: 0.7097\n",
      "Epoch: 171, Loss: 0.8054, Val: 0.7202, Test: 0.7084\n",
      "Epoch: 172, Loss: 0.8068, Val: 0.7167, Test: 0.7057\n",
      "Epoch: 173, Loss: 0.8004, Val: 0.7185, Test: 0.7075\n",
      "Epoch: 174, Loss: 0.8053, Val: 0.7205, Test: 0.7108\n",
      "Epoch: 175, Loss: 0.8027, Val: 0.7202, Test: 0.7106\n",
      "Epoch: 176, Loss: 0.8029, Val: 0.7177, Test: 0.7068\n",
      "Epoch: 177, Loss: 0.8044, Val: 0.7157, Test: 0.7051\n",
      "Epoch: 178, Loss: 0.8000, Val: 0.7186, Test: 0.7086\n",
      "Epoch: 179, Loss: 0.8050, Val: 0.7199, Test: 0.7107\n",
      "Epoch: 180, Loss: 0.8098, Val: 0.7195, Test: 0.7094\n",
      "Epoch: 181, Loss: 0.8043, Val: 0.7181, Test: 0.7076\n",
      "Epoch: 182, Loss: 0.8006, Val: 0.7187, Test: 0.7086\n",
      "Epoch: 183, Loss: 0.8032, Val: 0.7186, Test: 0.7096\n",
      "Epoch: 184, Loss: 0.8101, Val: 0.7195, Test: 0.7088\n",
      "Epoch: 185, Loss: 0.8049, Val: 0.7195, Test: 0.7084\n",
      "Epoch: 186, Loss: 0.8004, Val: 0.7175, Test: 0.7072\n",
      "Epoch: 187, Loss: 0.8039, Val: 0.7190, Test: 0.7074\n",
      "Epoch: 188, Loss: 0.8043, Val: 0.7193, Test: 0.7095\n",
      "Epoch: 189, Loss: 0.8014, Val: 0.7188, Test: 0.7082\n",
      "Epoch: 190, Loss: 0.7951, Val: 0.7178, Test: 0.7076\n",
      "Epoch: 191, Loss: 0.8049, Val: 0.7186, Test: 0.7079\n",
      "Epoch: 192, Loss: 0.7957, Val: 0.7201, Test: 0.7084\n",
      "Epoch: 193, Loss: 0.8091, Val: 0.7192, Test: 0.7081\n",
      "Epoch: 194, Loss: 0.7997, Val: 0.7190, Test: 0.7088\n",
      "Epoch: 195, Loss: 0.7963, Val: 0.7178, Test: 0.7077\n",
      "Epoch: 196, Loss: 0.7929, Val: 0.7190, Test: 0.7083\n",
      "Epoch: 197, Loss: 0.7932, Val: 0.7196, Test: 0.7088\n",
      "Epoch: 198, Loss: 0.7960, Val: 0.7200, Test: 0.7078\n",
      "Epoch: 199, Loss: 0.8024, Val: 0.7198, Test: 0.7079\n",
      "Epoch: 200, Loss: 0.7992, Val: 0.7190, Test: 0.7075\n",
      "Epoch: 201, Loss: 0.8049, Val: 0.7196, Test: 0.7079\n",
      "Epoch: 202, Loss: 0.8058, Val: 0.7195, Test: 0.7079\n",
      "Epoch: 203, Loss: 0.7972, Val: 0.7196, Test: 0.7081\n",
      "Epoch: 204, Loss: 0.7973, Val: 0.7204, Test: 0.7106\n",
      "Epoch: 205, Loss: 0.7964, Val: 0.7213, Test: 0.7120\n",
      "Epoch: 206, Loss: 0.7988, Val: 0.7187, Test: 0.7088\n",
      "Epoch: 207, Loss: 0.8008, Val: 0.7165, Test: 0.7050\n",
      "Epoch: 208, Loss: 0.8054, Val: 0.7174, Test: 0.7066\n",
      "Epoch: 209, Loss: 0.7989, Val: 0.7198, Test: 0.7099\n",
      "Epoch: 210, Loss: 0.7978, Val: 0.7206, Test: 0.7107\n",
      "Epoch: 211, Loss: 0.7980, Val: 0.7195, Test: 0.7089\n",
      "Epoch: 212, Loss: 0.7988, Val: 0.7177, Test: 0.7059\n",
      "Epoch: 213, Loss: 0.7964, Val: 0.7185, Test: 0.7069\n",
      "Epoch: 214, Loss: 0.7980, Val: 0.7207, Test: 0.7115\n",
      "Epoch: 215, Loss: 0.7940, Val: 0.7214, Test: 0.7119\n",
      "Epoch: 216, Loss: 0.7965, Val: 0.7184, Test: 0.7077\n",
      "Epoch: 217, Loss: 0.7944, Val: 0.7185, Test: 0.7076\n",
      "Epoch: 218, Loss: 0.7993, Val: 0.7195, Test: 0.7092\n",
      "Epoch: 219, Loss: 0.8005, Val: 0.7190, Test: 0.7093\n",
      "Epoch: 220, Loss: 0.8000, Val: 0.7183, Test: 0.7077\n",
      "Epoch: 221, Loss: 0.7893, Val: 0.7177, Test: 0.7083\n",
      "Epoch: 222, Loss: 0.8011, Val: 0.7203, Test: 0.7109\n",
      "Epoch: 223, Loss: 0.7859, Val: 0.7214, Test: 0.7120\n",
      "Epoch: 224, Loss: 0.7977, Val: 0.7197, Test: 0.7081\n",
      "Epoch: 225, Loss: 0.7938, Val: 0.7193, Test: 0.7072\n",
      "Epoch: 226, Loss: 0.7917, Val: 0.7206, Test: 0.7100\n",
      "Epoch: 227, Loss: 0.7922, Val: 0.7215, Test: 0.7126\n",
      "Epoch: 228, Loss: 0.7873, Val: 0.7206, Test: 0.7103\n",
      "Epoch: 229, Loss: 0.7882, Val: 0.7180, Test: 0.7060\n",
      "Epoch: 230, Loss: 0.7947, Val: 0.7174, Test: 0.7060\n",
      "Epoch: 231, Loss: 0.8004, Val: 0.7213, Test: 0.7110\n",
      "Epoch: 232, Loss: 0.7970, Val: 0.7220, Test: 0.7123\n",
      "Epoch: 233, Loss: 0.7953, Val: 0.7205, Test: 0.7093\n",
      "Epoch: 234, Loss: 0.7929, Val: 0.7196, Test: 0.7076\n",
      "Epoch: 235, Loss: 0.7915, Val: 0.7207, Test: 0.7097\n",
      "Epoch: 236, Loss: 0.7945, Val: 0.7209, Test: 0.7108\n",
      "Epoch: 237, Loss: 0.7932, Val: 0.7199, Test: 0.7092\n",
      "Epoch: 238, Loss: 0.7884, Val: 0.7185, Test: 0.7083\n",
      "Epoch: 239, Loss: 0.7907, Val: 0.7202, Test: 0.7100\n",
      "Epoch: 240, Loss: 0.7888, Val: 0.7208, Test: 0.7102\n",
      "Epoch: 241, Loss: 0.7937, Val: 0.7197, Test: 0.7097\n",
      "Epoch: 242, Loss: 0.7980, Val: 0.7197, Test: 0.7094\n",
      "Epoch: 243, Loss: 0.7919, Val: 0.7195, Test: 0.7091\n",
      "Epoch: 244, Loss: 0.7865, Val: 0.7187, Test: 0.7101\n",
      "Epoch: 245, Loss: 0.7945, Val: 0.7201, Test: 0.7097\n",
      "Epoch: 246, Loss: 0.7878, Val: 0.7183, Test: 0.7083\n",
      "Epoch: 247, Loss: 0.7937, Val: 0.7211, Test: 0.7110\n",
      "Epoch: 248, Loss: 0.7967, Val: 0.7207, Test: 0.7132\n",
      "Epoch: 249, Loss: 0.7969, Val: 0.7199, Test: 0.7115\n",
      "Epoch: 250, Loss: 0.8037, Val: 0.7165, Test: 0.7047\n",
      "Epoch: 251, Loss: 0.7938, Val: 0.7173, Test: 0.7051\n",
      "Epoch: 252, Loss: 0.7912, Val: 0.7205, Test: 0.7110\n",
      "Epoch: 253, Loss: 0.7930, Val: 0.7204, Test: 0.7116\n",
      "Epoch: 254, Loss: 0.8013, Val: 0.7182, Test: 0.7078\n",
      "Epoch: 255, Loss: 0.7877, Val: 0.7189, Test: 0.7083\n",
      "Epoch: 256, Loss: 0.7940, Val: 0.7197, Test: 0.7097\n",
      "Epoch: 257, Loss: 0.7857, Val: 0.7195, Test: 0.7092\n",
      "Epoch: 258, Loss: 0.7956, Val: 0.7196, Test: 0.7078\n",
      "Epoch: 259, Loss: 0.7931, Val: 0.7210, Test: 0.7104\n",
      "Epoch: 260, Loss: 0.7888, Val: 0.7217, Test: 0.7120\n",
      "Epoch: 261, Loss: 0.7879, Val: 0.7209, Test: 0.7107\n",
      "Epoch: 262, Loss: 0.7939, Val: 0.7183, Test: 0.7075\n",
      "Epoch: 263, Loss: 0.7916, Val: 0.7196, Test: 0.7080\n",
      "Epoch: 264, Loss: 0.7872, Val: 0.7211, Test: 0.7107\n",
      "Epoch: 265, Loss: 0.7911, Val: 0.7217, Test: 0.7107\n",
      "Epoch: 266, Loss: 0.7842, Val: 0.7174, Test: 0.7060\n",
      "Epoch: 267, Loss: 0.7872, Val: 0.7198, Test: 0.7074\n",
      "Epoch: 268, Loss: 0.8010, Val: 0.7185, Test: 0.7096\n",
      "Epoch: 269, Loss: 0.7929, Val: 0.7187, Test: 0.7070\n",
      "Epoch: 270, Loss: 0.7928, Val: 0.7175, Test: 0.7051\n",
      "Epoch: 271, Loss: 0.7848, Val: 0.7194, Test: 0.7081\n",
      "Epoch: 272, Loss: 0.7968, Val: 0.7206, Test: 0.7108\n",
      "Epoch: 273, Loss: 0.7890, Val: 0.7190, Test: 0.7077\n",
      "Epoch: 274, Loss: 0.7886, Val: 0.7178, Test: 0.7055\n",
      "Epoch: 275, Loss: 0.7928, Val: 0.7184, Test: 0.7074\n",
      "Epoch: 276, Loss: 0.7908, Val: 0.7208, Test: 0.7111\n",
      "Epoch: 277, Loss: 0.7865, Val: 0.7196, Test: 0.7077\n",
      "Epoch: 278, Loss: 0.7880, Val: 0.7173, Test: 0.7050\n",
      "Epoch: 279, Loss: 0.7917, Val: 0.7187, Test: 0.7088\n",
      "Epoch: 280, Loss: 0.7847, Val: 0.7201, Test: 0.7103\n",
      "Epoch: 281, Loss: 0.7889, Val: 0.7200, Test: 0.7111\n",
      "Epoch: 282, Loss: 0.7870, Val: 0.7190, Test: 0.7089\n",
      "Epoch: 283, Loss: 0.7895, Val: 0.7171, Test: 0.7055\n",
      "Epoch: 284, Loss: 0.7938, Val: 0.7184, Test: 0.7081\n",
      "Epoch: 285, Loss: 0.7802, Val: 0.7207, Test: 0.7110\n",
      "Epoch: 286, Loss: 0.7954, Val: 0.7201, Test: 0.7092\n",
      "Epoch: 287, Loss: 0.7906, Val: 0.7190, Test: 0.7072\n",
      "Epoch: 288, Loss: 0.7901, Val: 0.7201, Test: 0.7088\n",
      "Epoch: 289, Loss: 0.7849, Val: 0.7218, Test: 0.7120\n",
      "Epoch: 290, Loss: 0.7878, Val: 0.7200, Test: 0.7091\n",
      "Epoch: 291, Loss: 0.7834, Val: 0.7178, Test: 0.7057\n",
      "Epoch: 292, Loss: 0.7905, Val: 0.7191, Test: 0.7077\n",
      "Epoch: 293, Loss: 0.7885, Val: 0.7213, Test: 0.7113\n",
      "Epoch: 294, Loss: 0.7908, Val: 0.7204, Test: 0.7102\n",
      "Epoch: 295, Loss: 0.7862, Val: 0.7193, Test: 0.7079\n",
      "Epoch: 296, Loss: 0.7842, Val: 0.7175, Test: 0.7052\n",
      "Epoch: 297, Loss: 0.7898, Val: 0.7187, Test: 0.7081\n",
      "Epoch: 298, Loss: 0.7808, Val: 0.7202, Test: 0.7108\n",
      "Epoch: 299, Loss: 0.7891, Val: 0.7200, Test: 0.7102\n",
      "Epoch: 300, Loss: 0.7852, Val: 0.7178, Test: 0.7066\n",
      "0.45\n",
      "Label_rate: 0.45\n",
      "Epoch: 001, Loss: 2.0978, Val: 0.3866, Test: 0.2748\n",
      "Epoch: 002, Loss: 1.6756, Val: 0.5408, Test: 0.4791\n",
      "Epoch: 003, Loss: 1.4551, Val: 0.5532, Test: 0.5111\n",
      "Epoch: 004, Loss: 1.3836, Val: 0.5730, Test: 0.5361\n",
      "Epoch: 005, Loss: 1.3020, Val: 0.6240, Test: 0.6036\n",
      "Epoch: 006, Loss: 1.1891, Val: 0.6399, Test: 0.6284\n",
      "Epoch: 007, Loss: 1.1817, Val: 0.6499, Test: 0.6375\n",
      "Epoch: 008, Loss: 1.1221, Val: 0.6516, Test: 0.6422\n",
      "Epoch: 009, Loss: 1.0944, Val: 0.6631, Test: 0.6544\n",
      "Epoch: 010, Loss: 1.0873, Val: 0.6721, Test: 0.6625\n",
      "Epoch: 011, Loss: 1.0534, Val: 0.6722, Test: 0.6636\n",
      "Epoch: 012, Loss: 1.0577, Val: 0.6828, Test: 0.6697\n",
      "Epoch: 013, Loss: 1.0099, Val: 0.6853, Test: 0.6738\n",
      "Epoch: 014, Loss: 1.0162, Val: 0.6876, Test: 0.6785\n",
      "Epoch: 015, Loss: 1.0019, Val: 0.6915, Test: 0.6836\n",
      "Epoch: 016, Loss: 0.9885, Val: 0.6912, Test: 0.6815\n",
      "Epoch: 017, Loss: 0.9816, Val: 0.6882, Test: 0.6750\n",
      "Epoch: 018, Loss: 0.9913, Val: 0.6921, Test: 0.6802\n",
      "Epoch: 019, Loss: 0.9625, Val: 0.6974, Test: 0.6875\n",
      "Epoch: 020, Loss: 0.9567, Val: 0.7008, Test: 0.6894\n",
      "Epoch: 021, Loss: 0.9549, Val: 0.7019, Test: 0.6890\n",
      "Epoch: 022, Loss: 0.9484, Val: 0.7004, Test: 0.6872\n",
      "Epoch: 023, Loss: 0.9345, Val: 0.7008, Test: 0.6854\n",
      "Epoch: 024, Loss: 0.9316, Val: 0.7009, Test: 0.6869\n",
      "Epoch: 025, Loss: 0.9305, Val: 0.7010, Test: 0.6871\n",
      "Epoch: 026, Loss: 0.9230, Val: 0.7011, Test: 0.6861\n",
      "Epoch: 027, Loss: 0.9169, Val: 0.7017, Test: 0.6872\n",
      "Epoch: 028, Loss: 0.9088, Val: 0.7016, Test: 0.6878\n",
      "Epoch: 029, Loss: 0.9172, Val: 0.7024, Test: 0.6904\n",
      "Epoch: 030, Loss: 0.9056, Val: 0.7038, Test: 0.6924\n",
      "Epoch: 031, Loss: 0.9054, Val: 0.7040, Test: 0.6934\n",
      "Epoch: 032, Loss: 0.8970, Val: 0.7018, Test: 0.6913\n",
      "Epoch: 033, Loss: 0.8941, Val: 0.7020, Test: 0.6903\n",
      "Epoch: 034, Loss: 0.8920, Val: 0.7045, Test: 0.6927\n",
      "Epoch: 035, Loss: 0.8952, Val: 0.7068, Test: 0.6958\n",
      "Epoch: 036, Loss: 0.8813, Val: 0.7074, Test: 0.6975\n",
      "Epoch: 037, Loss: 0.8851, Val: 0.7077, Test: 0.6960\n",
      "Epoch: 038, Loss: 0.8839, Val: 0.7064, Test: 0.6945\n",
      "Epoch: 039, Loss: 0.8838, Val: 0.7058, Test: 0.6932\n",
      "Epoch: 040, Loss: 0.8776, Val: 0.7055, Test: 0.6938\n",
      "Epoch: 041, Loss: 0.8755, Val: 0.7067, Test: 0.6953\n",
      "Epoch: 042, Loss: 0.8728, Val: 0.7081, Test: 0.6977\n",
      "Epoch: 043, Loss: 0.8710, Val: 0.7084, Test: 0.6982\n",
      "Epoch: 044, Loss: 0.8749, Val: 0.7083, Test: 0.6983\n",
      "Epoch: 045, Loss: 0.8641, Val: 0.7079, Test: 0.6979\n",
      "Epoch: 046, Loss: 0.8691, Val: 0.7065, Test: 0.6966\n",
      "Epoch: 047, Loss: 0.8646, Val: 0.7067, Test: 0.6973\n",
      "Epoch: 048, Loss: 0.8598, Val: 0.7075, Test: 0.6992\n",
      "Epoch: 049, Loss: 0.8534, Val: 0.7105, Test: 0.7019\n",
      "Epoch: 050, Loss: 0.8558, Val: 0.7106, Test: 0.7037\n",
      "Epoch: 051, Loss: 0.8583, Val: 0.7101, Test: 0.7029\n",
      "Epoch: 052, Loss: 0.8586, Val: 0.7090, Test: 0.7006\n",
      "Epoch: 053, Loss: 0.8555, Val: 0.7104, Test: 0.7006\n",
      "Epoch: 054, Loss: 0.8561, Val: 0.7106, Test: 0.7018\n",
      "Epoch: 055, Loss: 0.8579, Val: 0.7114, Test: 0.7024\n",
      "Epoch: 056, Loss: 0.8494, Val: 0.7117, Test: 0.7031\n",
      "Epoch: 057, Loss: 0.8522, Val: 0.7113, Test: 0.7028\n",
      "Epoch: 058, Loss: 0.8549, Val: 0.7113, Test: 0.7027\n",
      "Epoch: 059, Loss: 0.8373, Val: 0.7112, Test: 0.7028\n",
      "Epoch: 060, Loss: 0.8474, Val: 0.7123, Test: 0.7035\n",
      "Epoch: 061, Loss: 0.8426, Val: 0.7129, Test: 0.7033\n",
      "Epoch: 062, Loss: 0.8423, Val: 0.7131, Test: 0.7039\n",
      "Epoch: 063, Loss: 0.8408, Val: 0.7127, Test: 0.7036\n",
      "Epoch: 064, Loss: 0.8371, Val: 0.7116, Test: 0.7032\n",
      "Epoch: 065, Loss: 0.8410, Val: 0.7102, Test: 0.7021\n",
      "Epoch: 066, Loss: 0.8415, Val: 0.7113, Test: 0.7031\n",
      "Epoch: 067, Loss: 0.8374, Val: 0.7131, Test: 0.7044\n",
      "Epoch: 068, Loss: 0.8429, Val: 0.7125, Test: 0.7048\n",
      "Epoch: 069, Loss: 0.8311, Val: 0.7118, Test: 0.7026\n",
      "Epoch: 070, Loss: 0.8406, Val: 0.7116, Test: 0.7018\n",
      "Epoch: 071, Loss: 0.8392, Val: 0.7118, Test: 0.7020\n",
      "Epoch: 072, Loss: 0.8344, Val: 0.7118, Test: 0.7026\n",
      "Epoch: 073, Loss: 0.8336, Val: 0.7119, Test: 0.7017\n",
      "Epoch: 074, Loss: 0.8383, Val: 0.7095, Test: 0.7002\n",
      "Epoch: 075, Loss: 0.8379, Val: 0.7095, Test: 0.7005\n",
      "Epoch: 076, Loss: 0.8402, Val: 0.7106, Test: 0.7017\n",
      "Epoch: 077, Loss: 0.8357, Val: 0.7132, Test: 0.7042\n",
      "Epoch: 078, Loss: 0.8297, Val: 0.7147, Test: 0.7065\n",
      "Epoch: 079, Loss: 0.8301, Val: 0.7133, Test: 0.7049\n",
      "Epoch: 080, Loss: 0.8332, Val: 0.7102, Test: 0.7006\n",
      "Epoch: 081, Loss: 0.8268, Val: 0.7087, Test: 0.6986\n",
      "Epoch: 082, Loss: 0.8292, Val: 0.7122, Test: 0.7029\n",
      "Epoch: 083, Loss: 0.8342, Val: 0.7144, Test: 0.7060\n",
      "Epoch: 084, Loss: 0.8204, Val: 0.7137, Test: 0.7069\n",
      "Epoch: 085, Loss: 0.8319, Val: 0.7138, Test: 0.7045\n",
      "Epoch: 086, Loss: 0.8263, Val: 0.7123, Test: 0.7016\n",
      "Epoch: 087, Loss: 0.8280, Val: 0.7120, Test: 0.7020\n",
      "Epoch: 088, Loss: 0.8398, Val: 0.7131, Test: 0.7050\n",
      "Epoch: 089, Loss: 0.8303, Val: 0.7132, Test: 0.7065\n",
      "Epoch: 090, Loss: 0.8202, Val: 0.7113, Test: 0.7045\n",
      "Epoch: 091, Loss: 0.8315, Val: 0.7111, Test: 0.7032\n",
      "Epoch: 092, Loss: 0.8303, Val: 0.7131, Test: 0.7043\n",
      "Epoch: 093, Loss: 0.8234, Val: 0.7143, Test: 0.7064\n",
      "Epoch: 094, Loss: 0.8192, Val: 0.7141, Test: 0.7064\n",
      "Epoch: 095, Loss: 0.8214, Val: 0.7127, Test: 0.7047\n",
      "Epoch: 096, Loss: 0.8320, Val: 0.7125, Test: 0.7042\n",
      "Epoch: 097, Loss: 0.8269, Val: 0.7138, Test: 0.7047\n",
      "Epoch: 098, Loss: 0.8253, Val: 0.7144, Test: 0.7049\n",
      "Epoch: 099, Loss: 0.8283, Val: 0.7140, Test: 0.7034\n",
      "Epoch: 100, Loss: 0.8150, Val: 0.7115, Test: 0.7022\n",
      "Epoch: 101, Loss: 0.8265, Val: 0.7132, Test: 0.7048\n",
      "Epoch: 102, Loss: 0.8228, Val: 0.7128, Test: 0.7057\n",
      "Epoch: 103, Loss: 0.8210, Val: 0.7124, Test: 0.7037\n",
      "Epoch: 104, Loss: 0.8152, Val: 0.7119, Test: 0.7027\n",
      "Epoch: 105, Loss: 0.8184, Val: 0.7113, Test: 0.7018\n",
      "Epoch: 106, Loss: 0.8235, Val: 0.7138, Test: 0.7052\n",
      "Epoch: 107, Loss: 0.8244, Val: 0.7134, Test: 0.7075\n",
      "Epoch: 108, Loss: 0.8253, Val: 0.7134, Test: 0.7052\n",
      "Epoch: 109, Loss: 0.8213, Val: 0.7116, Test: 0.7005\n",
      "Epoch: 110, Loss: 0.8119, Val: 0.7118, Test: 0.7003\n",
      "Epoch: 111, Loss: 0.8194, Val: 0.7146, Test: 0.7059\n",
      "Epoch: 112, Loss: 0.8148, Val: 0.7159, Test: 0.7092\n",
      "Epoch: 113, Loss: 0.8215, Val: 0.7152, Test: 0.7081\n",
      "Epoch: 114, Loss: 0.8199, Val: 0.7113, Test: 0.7028\n",
      "Epoch: 115, Loss: 0.8154, Val: 0.7107, Test: 0.7009\n",
      "Epoch: 116, Loss: 0.8179, Val: 0.7112, Test: 0.7014\n",
      "Epoch: 117, Loss: 0.8121, Val: 0.7140, Test: 0.7051\n",
      "Epoch: 118, Loss: 0.8280, Val: 0.7145, Test: 0.7052\n",
      "Epoch: 119, Loss: 0.8148, Val: 0.7132, Test: 0.7045\n",
      "Epoch: 120, Loss: 0.8154, Val: 0.7127, Test: 0.7052\n",
      "Epoch: 121, Loss: 0.8132, Val: 0.7129, Test: 0.7066\n",
      "Epoch: 122, Loss: 0.8158, Val: 0.7121, Test: 0.7066\n",
      "Epoch: 123, Loss: 0.8116, Val: 0.7136, Test: 0.7051\n",
      "Epoch: 124, Loss: 0.8166, Val: 0.7130, Test: 0.7025\n",
      "Epoch: 125, Loss: 0.8179, Val: 0.7144, Test: 0.7047\n",
      "Epoch: 126, Loss: 0.8125, Val: 0.7148, Test: 0.7059\n",
      "Epoch: 127, Loss: 0.8194, Val: 0.7138, Test: 0.7054\n",
      "Epoch: 128, Loss: 0.8219, Val: 0.7128, Test: 0.7040\n",
      "Epoch: 129, Loss: 0.8144, Val: 0.7136, Test: 0.7048\n",
      "Epoch: 130, Loss: 0.8132, Val: 0.7151, Test: 0.7070\n",
      "Epoch: 131, Loss: 0.8048, Val: 0.7152, Test: 0.7069\n",
      "Epoch: 132, Loss: 0.8047, Val: 0.7148, Test: 0.7060\n",
      "Epoch: 133, Loss: 0.8167, Val: 0.7135, Test: 0.7043\n",
      "Epoch: 134, Loss: 0.8067, Val: 0.7132, Test: 0.7043\n",
      "Epoch: 135, Loss: 0.8045, Val: 0.7151, Test: 0.7055\n",
      "Epoch: 136, Loss: 0.8109, Val: 0.7157, Test: 0.7081\n",
      "Epoch: 137, Loss: 0.8139, Val: 0.7166, Test: 0.7074\n",
      "Epoch: 138, Loss: 0.8080, Val: 0.7164, Test: 0.7067\n",
      "Epoch: 139, Loss: 0.8076, Val: 0.7147, Test: 0.7045\n",
      "Epoch: 140, Loss: 0.8100, Val: 0.7151, Test: 0.7053\n",
      "Epoch: 141, Loss: 0.8148, Val: 0.7148, Test: 0.7064\n",
      "Epoch: 142, Loss: 0.8079, Val: 0.7146, Test: 0.7050\n",
      "Epoch: 143, Loss: 0.8082, Val: 0.7146, Test: 0.7044\n",
      "Epoch: 144, Loss: 0.8059, Val: 0.7146, Test: 0.7042\n",
      "Epoch: 145, Loss: 0.8032, Val: 0.7146, Test: 0.7038\n",
      "Epoch: 146, Loss: 0.8106, Val: 0.7148, Test: 0.7038\n",
      "Epoch: 147, Loss: 0.8143, Val: 0.7150, Test: 0.7045\n",
      "Epoch: 148, Loss: 0.8038, Val: 0.7152, Test: 0.7059\n",
      "Epoch: 149, Loss: 0.8075, Val: 0.7158, Test: 0.7074\n",
      "Epoch: 150, Loss: 0.8070, Val: 0.7153, Test: 0.7067\n",
      "Epoch: 151, Loss: 0.8106, Val: 0.7152, Test: 0.7060\n",
      "Epoch: 152, Loss: 0.8054, Val: 0.7138, Test: 0.7043\n",
      "Epoch: 153, Loss: 0.8138, Val: 0.7134, Test: 0.7038\n",
      "Epoch: 154, Loss: 0.8068, Val: 0.7142, Test: 0.7050\n",
      "Epoch: 155, Loss: 0.8088, Val: 0.7142, Test: 0.7044\n",
      "Epoch: 156, Loss: 0.8070, Val: 0.7146, Test: 0.7047\n",
      "Epoch: 157, Loss: 0.7984, Val: 0.7147, Test: 0.7058\n",
      "Epoch: 158, Loss: 0.8119, Val: 0.7153, Test: 0.7072\n",
      "Epoch: 159, Loss: 0.8078, Val: 0.7152, Test: 0.7069\n",
      "Epoch: 160, Loss: 0.8058, Val: 0.7155, Test: 0.7064\n",
      "Epoch: 161, Loss: 0.8121, Val: 0.7151, Test: 0.7066\n",
      "Epoch: 162, Loss: 0.8003, Val: 0.7135, Test: 0.7058\n",
      "Epoch: 163, Loss: 0.8042, Val: 0.7155, Test: 0.7079\n",
      "Epoch: 164, Loss: 0.8029, Val: 0.7168, Test: 0.7095\n",
      "Epoch: 165, Loss: 0.7965, Val: 0.7171, Test: 0.7084\n",
      "Epoch: 166, Loss: 0.8069, Val: 0.7167, Test: 0.7067\n",
      "Epoch: 167, Loss: 0.8082, Val: 0.7153, Test: 0.7049\n",
      "Epoch: 168, Loss: 0.8008, Val: 0.7157, Test: 0.7058\n",
      "Epoch: 169, Loss: 0.8001, Val: 0.7157, Test: 0.7087\n",
      "Epoch: 170, Loss: 0.8019, Val: 0.7171, Test: 0.7075\n",
      "Epoch: 171, Loss: 0.8020, Val: 0.7154, Test: 0.7042\n",
      "Epoch: 172, Loss: 0.7995, Val: 0.7149, Test: 0.7035\n",
      "Epoch: 173, Loss: 0.7988, Val: 0.7164, Test: 0.7057\n",
      "Epoch: 174, Loss: 0.7994, Val: 0.7156, Test: 0.7058\n",
      "Epoch: 175, Loss: 0.8012, Val: 0.7151, Test: 0.7042\n",
      "Epoch: 176, Loss: 0.8018, Val: 0.7157, Test: 0.7049\n",
      "Epoch: 177, Loss: 0.8006, Val: 0.7166, Test: 0.7082\n",
      "Epoch: 178, Loss: 0.8045, Val: 0.7168, Test: 0.7071\n",
      "Epoch: 179, Loss: 0.8029, Val: 0.7169, Test: 0.7070\n",
      "Epoch: 180, Loss: 0.8061, Val: 0.7165, Test: 0.7069\n",
      "Epoch: 181, Loss: 0.7960, Val: 0.7166, Test: 0.7063\n",
      "Epoch: 182, Loss: 0.7942, Val: 0.7157, Test: 0.7042\n",
      "Epoch: 183, Loss: 0.8020, Val: 0.7152, Test: 0.7032\n",
      "Epoch: 184, Loss: 0.7949, Val: 0.7177, Test: 0.7051\n",
      "Epoch: 185, Loss: 0.8068, Val: 0.7188, Test: 0.7077\n",
      "Epoch: 186, Loss: 0.8030, Val: 0.7183, Test: 0.7079\n",
      "Epoch: 187, Loss: 0.8001, Val: 0.7161, Test: 0.7050\n",
      "Epoch: 188, Loss: 0.7982, Val: 0.7147, Test: 0.7029\n",
      "Epoch: 189, Loss: 0.8042, Val: 0.7157, Test: 0.7047\n",
      "Epoch: 190, Loss: 0.7942, Val: 0.7179, Test: 0.7075\n",
      "Epoch: 191, Loss: 0.8017, Val: 0.7177, Test: 0.7094\n",
      "Epoch: 192, Loss: 0.7985, Val: 0.7178, Test: 0.7070\n",
      "Epoch: 193, Loss: 0.7960, Val: 0.7153, Test: 0.7031\n",
      "Epoch: 194, Loss: 0.7973, Val: 0.7141, Test: 0.7022\n",
      "Epoch: 195, Loss: 0.8071, Val: 0.7168, Test: 0.7062\n",
      "Epoch: 196, Loss: 0.7972, Val: 0.7169, Test: 0.7070\n",
      "Epoch: 197, Loss: 0.7980, Val: 0.7172, Test: 0.7066\n",
      "Epoch: 198, Loss: 0.8006, Val: 0.7165, Test: 0.7039\n",
      "Epoch: 199, Loss: 0.8022, Val: 0.7166, Test: 0.7022\n",
      "Epoch: 200, Loss: 0.7911, Val: 0.7169, Test: 0.7047\n",
      "Epoch: 201, Loss: 0.7918, Val: 0.7172, Test: 0.7061\n",
      "Epoch: 202, Loss: 0.7941, Val: 0.7162, Test: 0.7057\n",
      "Epoch: 203, Loss: 0.7924, Val: 0.7165, Test: 0.7052\n",
      "Epoch: 204, Loss: 0.7968, Val: 0.7178, Test: 0.7064\n",
      "Epoch: 205, Loss: 0.7926, Val: 0.7179, Test: 0.7067\n",
      "Epoch: 206, Loss: 0.8004, Val: 0.7179, Test: 0.7064\n",
      "Epoch: 207, Loss: 0.7946, Val: 0.7169, Test: 0.7055\n",
      "Epoch: 208, Loss: 0.7993, Val: 0.7172, Test: 0.7061\n",
      "Epoch: 209, Loss: 0.7942, Val: 0.7182, Test: 0.7074\n",
      "Epoch: 210, Loss: 0.7944, Val: 0.7175, Test: 0.7070\n",
      "Epoch: 211, Loss: 0.7934, Val: 0.7170, Test: 0.7046\n",
      "Epoch: 212, Loss: 0.7955, Val: 0.7166, Test: 0.7048\n",
      "Epoch: 213, Loss: 0.7950, Val: 0.7186, Test: 0.7078\n",
      "Epoch: 214, Loss: 0.7932, Val: 0.7194, Test: 0.7097\n",
      "Epoch: 215, Loss: 0.7956, Val: 0.7185, Test: 0.7071\n",
      "Epoch: 216, Loss: 0.7903, Val: 0.7161, Test: 0.7037\n",
      "Epoch: 217, Loss: 0.7965, Val: 0.7166, Test: 0.7048\n",
      "Epoch: 218, Loss: 0.7923, Val: 0.7175, Test: 0.7064\n",
      "Epoch: 219, Loss: 0.7976, Val: 0.7175, Test: 0.7061\n",
      "Epoch: 220, Loss: 0.7978, Val: 0.7162, Test: 0.7053\n",
      "Epoch: 221, Loss: 0.7914, Val: 0.7178, Test: 0.7073\n",
      "Epoch: 222, Loss: 0.7975, Val: 0.7180, Test: 0.7077\n",
      "Epoch: 223, Loss: 0.7919, Val: 0.7162, Test: 0.7039\n",
      "Epoch: 224, Loss: 0.7948, Val: 0.7161, Test: 0.7042\n",
      "Epoch: 225, Loss: 0.7933, Val: 0.7181, Test: 0.7078\n",
      "Epoch: 226, Loss: 0.7925, Val: 0.7182, Test: 0.7082\n",
      "Epoch: 227, Loss: 0.7944, Val: 0.7174, Test: 0.7066\n",
      "Epoch: 228, Loss: 0.7898, Val: 0.7168, Test: 0.7045\n",
      "Epoch: 229, Loss: 0.7844, Val: 0.7179, Test: 0.7061\n",
      "Epoch: 230, Loss: 0.7876, Val: 0.7185, Test: 0.7073\n",
      "Epoch: 231, Loss: 0.7903, Val: 0.7166, Test: 0.7041\n",
      "Epoch: 232, Loss: 0.7896, Val: 0.7154, Test: 0.7030\n",
      "Epoch: 233, Loss: 0.7896, Val: 0.7165, Test: 0.7041\n",
      "Epoch: 234, Loss: 0.7860, Val: 0.7186, Test: 0.7087\n",
      "Epoch: 235, Loss: 0.7954, Val: 0.7193, Test: 0.7088\n",
      "Epoch: 236, Loss: 0.7883, Val: 0.7183, Test: 0.7061\n",
      "Epoch: 237, Loss: 0.7939, Val: 0.7172, Test: 0.7047\n",
      "Epoch: 238, Loss: 0.7921, Val: 0.7175, Test: 0.7060\n",
      "Epoch: 239, Loss: 0.7888, Val: 0.7167, Test: 0.7075\n",
      "Epoch: 240, Loss: 0.7939, Val: 0.7178, Test: 0.7077\n",
      "Epoch: 241, Loss: 0.7898, Val: 0.7178, Test: 0.7073\n",
      "Epoch: 242, Loss: 0.7981, Val: 0.7190, Test: 0.7095\n",
      "Epoch: 243, Loss: 0.7779, Val: 0.7190, Test: 0.7097\n",
      "Epoch: 244, Loss: 0.7887, Val: 0.7176, Test: 0.7082\n",
      "Epoch: 245, Loss: 0.7835, Val: 0.7159, Test: 0.7057\n",
      "Epoch: 246, Loss: 0.7936, Val: 0.7157, Test: 0.7039\n",
      "Epoch: 247, Loss: 0.7902, Val: 0.7175, Test: 0.7060\n",
      "Epoch: 248, Loss: 0.7843, Val: 0.7192, Test: 0.7097\n",
      "Epoch: 249, Loss: 0.7872, Val: 0.7193, Test: 0.7118\n",
      "Epoch: 250, Loss: 0.7906, Val: 0.7187, Test: 0.7106\n",
      "Epoch: 251, Loss: 0.7937, Val: 0.7169, Test: 0.7057\n",
      "Epoch: 252, Loss: 0.7846, Val: 0.7171, Test: 0.7054\n",
      "Epoch: 253, Loss: 0.7927, Val: 0.7184, Test: 0.7075\n",
      "Epoch: 254, Loss: 0.7799, Val: 0.7188, Test: 0.7089\n",
      "Epoch: 255, Loss: 0.7840, Val: 0.7188, Test: 0.7084\n",
      "Epoch: 256, Loss: 0.7866, Val: 0.7180, Test: 0.7072\n",
      "Epoch: 257, Loss: 0.7824, Val: 0.7178, Test: 0.7083\n",
      "Epoch: 258, Loss: 0.7814, Val: 0.7179, Test: 0.7097\n",
      "Epoch: 259, Loss: 0.7929, Val: 0.7176, Test: 0.7086\n",
      "Epoch: 260, Loss: 0.7811, Val: 0.7179, Test: 0.7078\n",
      "Epoch: 261, Loss: 0.7799, Val: 0.7181, Test: 0.7075\n",
      "Epoch: 262, Loss: 0.7873, Val: 0.7182, Test: 0.7073\n",
      "Epoch: 263, Loss: 0.7803, Val: 0.7186, Test: 0.7085\n",
      "Epoch: 264, Loss: 0.7838, Val: 0.7176, Test: 0.7068\n",
      "Epoch: 265, Loss: 0.7878, Val: 0.7180, Test: 0.7068\n",
      "Epoch: 266, Loss: 0.7909, Val: 0.7177, Test: 0.7059\n",
      "Epoch: 267, Loss: 0.7881, Val: 0.7176, Test: 0.7047\n",
      "Epoch: 268, Loss: 0.7863, Val: 0.7168, Test: 0.7044\n",
      "Epoch: 269, Loss: 0.7790, Val: 0.7180, Test: 0.7075\n",
      "Epoch: 270, Loss: 0.7801, Val: 0.7190, Test: 0.7087\n",
      "Epoch: 271, Loss: 0.7875, Val: 0.7177, Test: 0.7065\n",
      "Epoch: 272, Loss: 0.7833, Val: 0.7170, Test: 0.7044\n",
      "Epoch: 273, Loss: 0.7830, Val: 0.7184, Test: 0.7049\n",
      "Epoch: 274, Loss: 0.7822, Val: 0.7208, Test: 0.7103\n",
      "Epoch: 275, Loss: 0.7858, Val: 0.7201, Test: 0.7101\n",
      "Epoch: 276, Loss: 0.7858, Val: 0.7176, Test: 0.7065\n",
      "Epoch: 277, Loss: 0.7728, Val: 0.7167, Test: 0.7028\n",
      "Epoch: 278, Loss: 0.7845, Val: 0.7185, Test: 0.7063\n",
      "Epoch: 279, Loss: 0.7821, Val: 0.7205, Test: 0.7102\n",
      "Epoch: 280, Loss: 0.7905, Val: 0.7203, Test: 0.7102\n",
      "Epoch: 281, Loss: 0.7900, Val: 0.7174, Test: 0.7060\n",
      "Epoch: 282, Loss: 0.7784, Val: 0.7166, Test: 0.7037\n",
      "Epoch: 283, Loss: 0.7838, Val: 0.7190, Test: 0.7083\n",
      "Epoch: 284, Loss: 0.7832, Val: 0.7201, Test: 0.7103\n",
      "Epoch: 285, Loss: 0.7822, Val: 0.7187, Test: 0.7064\n",
      "Epoch: 286, Loss: 0.7822, Val: 0.7188, Test: 0.7066\n",
      "Epoch: 287, Loss: 0.7865, Val: 0.7207, Test: 0.7107\n",
      "Epoch: 288, Loss: 0.7849, Val: 0.7205, Test: 0.7111\n",
      "Epoch: 289, Loss: 0.7838, Val: 0.7179, Test: 0.7048\n",
      "Epoch: 290, Loss: 0.7848, Val: 0.7154, Test: 0.7023\n",
      "Epoch: 291, Loss: 0.7886, Val: 0.7173, Test: 0.7059\n",
      "Epoch: 292, Loss: 0.7841, Val: 0.7209, Test: 0.7118\n",
      "Epoch: 293, Loss: 0.7928, Val: 0.7198, Test: 0.7106\n",
      "Epoch: 294, Loss: 0.7883, Val: 0.7172, Test: 0.7060\n",
      "Epoch: 295, Loss: 0.7915, Val: 0.7178, Test: 0.7059\n",
      "Epoch: 296, Loss: 0.7816, Val: 0.7190, Test: 0.7083\n",
      "Epoch: 297, Loss: 0.7802, Val: 0.7189, Test: 0.7077\n",
      "Epoch: 298, Loss: 0.7841, Val: 0.7174, Test: 0.7025\n",
      "Epoch: 299, Loss: 0.7868, Val: 0.7181, Test: 0.7042\n",
      "Epoch: 300, Loss: 0.7895, Val: 0.7205, Test: 0.7101\n",
      "0.5\n",
      "Label_rate: 0.5\n",
      "Epoch: 001, Loss: 2.3980, Val: 0.4322, Test: 0.3703\n",
      "Epoch: 002, Loss: 1.8803, Val: 0.5134, Test: 0.4903\n",
      "Epoch: 003, Loss: 1.7049, Val: 0.5459, Test: 0.5347\n",
      "Epoch: 004, Loss: 1.5423, Val: 0.5495, Test: 0.5428\n",
      "Epoch: 005, Loss: 1.4831, Val: 0.6033, Test: 0.5997\n",
      "Epoch: 006, Loss: 1.3420, Val: 0.6399, Test: 0.6369\n",
      "Epoch: 007, Loss: 1.2564, Val: 0.6371, Test: 0.6322\n",
      "Epoch: 008, Loss: 1.2233, Val: 0.6503, Test: 0.6425\n",
      "Epoch: 009, Loss: 1.1631, Val: 0.6628, Test: 0.6547\n",
      "Epoch: 010, Loss: 1.1319, Val: 0.6641, Test: 0.6606\n",
      "Epoch: 011, Loss: 1.1269, Val: 0.6726, Test: 0.6730\n",
      "Epoch: 012, Loss: 1.0923, Val: 0.6846, Test: 0.6858\n",
      "Epoch: 013, Loss: 1.0652, Val: 0.6911, Test: 0.6918\n",
      "Epoch: 014, Loss: 1.0564, Val: 0.6954, Test: 0.6930\n",
      "Epoch: 015, Loss: 1.0612, Val: 0.6987, Test: 0.6969\n",
      "Epoch: 016, Loss: 1.0438, Val: 0.7019, Test: 0.7006\n",
      "Epoch: 017, Loss: 1.0269, Val: 0.7036, Test: 0.7022\n",
      "Epoch: 018, Loss: 1.0181, Val: 0.7019, Test: 0.7000\n",
      "Epoch: 019, Loss: 1.0100, Val: 0.7015, Test: 0.6982\n",
      "Epoch: 020, Loss: 1.0072, Val: 0.7020, Test: 0.6987\n",
      "Epoch: 021, Loss: 0.9792, Val: 0.7035, Test: 0.6997\n",
      "Epoch: 022, Loss: 0.9668, Val: 0.7047, Test: 0.6996\n",
      "Epoch: 023, Loss: 0.9674, Val: 0.7051, Test: 0.7013\n",
      "Epoch: 024, Loss: 0.9598, Val: 0.7081, Test: 0.7042\n",
      "Epoch: 025, Loss: 0.9524, Val: 0.7094, Test: 0.7044\n",
      "Epoch: 026, Loss: 0.9489, Val: 0.7079, Test: 0.7031\n",
      "Epoch: 027, Loss: 0.9454, Val: 0.7082, Test: 0.7022\n",
      "Epoch: 028, Loss: 0.9291, Val: 0.7086, Test: 0.7022\n",
      "Epoch: 029, Loss: 0.9278, Val: 0.7088, Test: 0.7043\n",
      "Epoch: 030, Loss: 0.9370, Val: 0.7112, Test: 0.7075\n",
      "Epoch: 031, Loss: 0.9230, Val: 0.7127, Test: 0.7105\n",
      "Epoch: 032, Loss: 0.9124, Val: 0.7134, Test: 0.7114\n",
      "Epoch: 033, Loss: 0.9110, Val: 0.7126, Test: 0.7096\n",
      "Epoch: 034, Loss: 0.9079, Val: 0.7098, Test: 0.7066\n",
      "Epoch: 035, Loss: 0.9036, Val: 0.7083, Test: 0.7064\n",
      "Epoch: 036, Loss: 0.8978, Val: 0.7091, Test: 0.7083\n",
      "Epoch: 037, Loss: 0.9001, Val: 0.7102, Test: 0.7109\n",
      "Epoch: 038, Loss: 0.8987, Val: 0.7119, Test: 0.7119\n",
      "Epoch: 039, Loss: 0.8957, Val: 0.7128, Test: 0.7120\n",
      "Epoch: 040, Loss: 0.8896, Val: 0.7136, Test: 0.7108\n",
      "Epoch: 041, Loss: 0.8857, Val: 0.7143, Test: 0.7091\n",
      "Epoch: 042, Loss: 0.8886, Val: 0.7139, Test: 0.7088\n",
      "Epoch: 043, Loss: 0.8910, Val: 0.7147, Test: 0.7090\n",
      "Epoch: 044, Loss: 0.8818, Val: 0.7151, Test: 0.7102\n",
      "Epoch: 045, Loss: 0.8744, Val: 0.7145, Test: 0.7091\n",
      "Epoch: 046, Loss: 0.8813, Val: 0.7136, Test: 0.7084\n",
      "Epoch: 047, Loss: 0.8823, Val: 0.7144, Test: 0.7083\n",
      "Epoch: 048, Loss: 0.8846, Val: 0.7144, Test: 0.7085\n",
      "Epoch: 049, Loss: 0.8728, Val: 0.7151, Test: 0.7085\n",
      "Epoch: 050, Loss: 0.8712, Val: 0.7157, Test: 0.7078\n",
      "Epoch: 051, Loss: 0.8705, Val: 0.7146, Test: 0.7070\n",
      "Epoch: 052, Loss: 0.8744, Val: 0.7144, Test: 0.7066\n",
      "Epoch: 053, Loss: 0.8637, Val: 0.7142, Test: 0.7075\n",
      "Epoch: 054, Loss: 0.8669, Val: 0.7147, Test: 0.7086\n",
      "Epoch: 055, Loss: 0.8684, Val: 0.7148, Test: 0.7087\n",
      "Epoch: 056, Loss: 0.8619, Val: 0.7144, Test: 0.7091\n",
      "Epoch: 057, Loss: 0.8660, Val: 0.7155, Test: 0.7093\n",
      "Epoch: 058, Loss: 0.8548, Val: 0.7155, Test: 0.7079\n",
      "Epoch: 059, Loss: 0.8516, Val: 0.7166, Test: 0.7081\n",
      "Epoch: 060, Loss: 0.8540, Val: 0.7173, Test: 0.7092\n",
      "Epoch: 061, Loss: 0.8454, Val: 0.7177, Test: 0.7109\n",
      "Epoch: 062, Loss: 0.8470, Val: 0.7174, Test: 0.7107\n",
      "Epoch: 063, Loss: 0.8497, Val: 0.7169, Test: 0.7085\n",
      "Epoch: 064, Loss: 0.8493, Val: 0.7161, Test: 0.7082\n",
      "Epoch: 065, Loss: 0.8520, Val: 0.7164, Test: 0.7092\n",
      "Epoch: 066, Loss: 0.8491, Val: 0.7172, Test: 0.7116\n",
      "Epoch: 067, Loss: 0.8479, Val: 0.7188, Test: 0.7138\n",
      "Epoch: 068, Loss: 0.8556, Val: 0.7191, Test: 0.7139\n",
      "Epoch: 069, Loss: 0.8456, Val: 0.7182, Test: 0.7125\n",
      "Epoch: 070, Loss: 0.8457, Val: 0.7170, Test: 0.7090\n",
      "Epoch: 071, Loss: 0.8390, Val: 0.7158, Test: 0.7076\n",
      "Epoch: 072, Loss: 0.8516, Val: 0.7170, Test: 0.7089\n",
      "Epoch: 073, Loss: 0.8396, Val: 0.7180, Test: 0.7114\n",
      "Epoch: 074, Loss: 0.8259, Val: 0.7179, Test: 0.7138\n",
      "Epoch: 075, Loss: 0.8366, Val: 0.7182, Test: 0.7127\n",
      "Epoch: 076, Loss: 0.8373, Val: 0.7181, Test: 0.7114\n",
      "Epoch: 077, Loss: 0.8323, Val: 0.7178, Test: 0.7111\n",
      "Epoch: 078, Loss: 0.8355, Val: 0.7183, Test: 0.7119\n",
      "Epoch: 079, Loss: 0.8350, Val: 0.7189, Test: 0.7130\n",
      "Epoch: 080, Loss: 0.8360, Val: 0.7192, Test: 0.7134\n",
      "Epoch: 081, Loss: 0.8339, Val: 0.7191, Test: 0.7131\n",
      "Epoch: 082, Loss: 0.8353, Val: 0.7189, Test: 0.7132\n",
      "Epoch: 083, Loss: 0.8305, Val: 0.7193, Test: 0.7132\n",
      "Epoch: 084, Loss: 0.8283, Val: 0.7199, Test: 0.7129\n",
      "Epoch: 085, Loss: 0.8301, Val: 0.7196, Test: 0.7115\n",
      "Epoch: 086, Loss: 0.8357, Val: 0.7195, Test: 0.7107\n",
      "Epoch: 087, Loss: 0.8344, Val: 0.7191, Test: 0.7125\n",
      "Epoch: 088, Loss: 0.8322, Val: 0.7195, Test: 0.7127\n",
      "Epoch: 089, Loss: 0.8250, Val: 0.7191, Test: 0.7117\n",
      "Epoch: 090, Loss: 0.8361, Val: 0.7193, Test: 0.7119\n",
      "Epoch: 091, Loss: 0.8273, Val: 0.7193, Test: 0.7125\n",
      "Epoch: 092, Loss: 0.8371, Val: 0.7198, Test: 0.7136\n",
      "Epoch: 093, Loss: 0.8248, Val: 0.7184, Test: 0.7122\n",
      "Epoch: 094, Loss: 0.8326, Val: 0.7183, Test: 0.7114\n",
      "Epoch: 095, Loss: 0.8231, Val: 0.7179, Test: 0.7092\n",
      "Epoch: 096, Loss: 0.8316, Val: 0.7194, Test: 0.7113\n",
      "Epoch: 097, Loss: 0.8324, Val: 0.7204, Test: 0.7132\n",
      "Epoch: 098, Loss: 0.8230, Val: 0.7207, Test: 0.7142\n",
      "Epoch: 099, Loss: 0.8308, Val: 0.7201, Test: 0.7120\n",
      "Epoch: 100, Loss: 0.8245, Val: 0.7191, Test: 0.7106\n",
      "Epoch: 101, Loss: 0.8219, Val: 0.7192, Test: 0.7110\n",
      "Epoch: 102, Loss: 0.8246, Val: 0.7195, Test: 0.7113\n",
      "Epoch: 103, Loss: 0.8213, Val: 0.7201, Test: 0.7133\n",
      "Epoch: 104, Loss: 0.8219, Val: 0.7208, Test: 0.7145\n",
      "Epoch: 105, Loss: 0.8265, Val: 0.7204, Test: 0.7138\n",
      "Epoch: 106, Loss: 0.8211, Val: 0.7205, Test: 0.7104\n",
      "Epoch: 107, Loss: 0.8138, Val: 0.7200, Test: 0.7104\n",
      "Epoch: 108, Loss: 0.8226, Val: 0.7206, Test: 0.7135\n",
      "Epoch: 109, Loss: 0.8223, Val: 0.7209, Test: 0.7148\n",
      "Epoch: 110, Loss: 0.8129, Val: 0.7205, Test: 0.7132\n",
      "Epoch: 111, Loss: 0.8250, Val: 0.7200, Test: 0.7115\n",
      "Epoch: 112, Loss: 0.8221, Val: 0.7197, Test: 0.7113\n",
      "Epoch: 113, Loss: 0.8118, Val: 0.7198, Test: 0.7102\n",
      "Epoch: 114, Loss: 0.8236, Val: 0.7203, Test: 0.7108\n",
      "Epoch: 115, Loss: 0.8107, Val: 0.7207, Test: 0.7129\n",
      "Epoch: 116, Loss: 0.8173, Val: 0.7215, Test: 0.7142\n",
      "Epoch: 117, Loss: 0.8177, Val: 0.7223, Test: 0.7154\n",
      "Epoch: 118, Loss: 0.8089, Val: 0.7218, Test: 0.7156\n",
      "Epoch: 119, Loss: 0.8170, Val: 0.7204, Test: 0.7136\n",
      "Epoch: 120, Loss: 0.8243, Val: 0.7194, Test: 0.7105\n",
      "Epoch: 121, Loss: 0.8085, Val: 0.7193, Test: 0.7080\n",
      "Epoch: 122, Loss: 0.8133, Val: 0.7199, Test: 0.7091\n",
      "Epoch: 123, Loss: 0.8122, Val: 0.7213, Test: 0.7123\n",
      "Epoch: 124, Loss: 0.8160, Val: 0.7213, Test: 0.7154\n",
      "Epoch: 125, Loss: 0.8131, Val: 0.7208, Test: 0.7143\n",
      "Epoch: 126, Loss: 0.8080, Val: 0.7201, Test: 0.7114\n",
      "Epoch: 127, Loss: 0.8095, Val: 0.7198, Test: 0.7090\n",
      "Epoch: 128, Loss: 0.8126, Val: 0.7204, Test: 0.7098\n",
      "Epoch: 129, Loss: 0.8122, Val: 0.7216, Test: 0.7120\n",
      "Epoch: 130, Loss: 0.8123, Val: 0.7228, Test: 0.7145\n",
      "Epoch: 131, Loss: 0.8140, Val: 0.7223, Test: 0.7134\n",
      "Epoch: 132, Loss: 0.8143, Val: 0.7208, Test: 0.7114\n",
      "Epoch: 133, Loss: 0.8127, Val: 0.7207, Test: 0.7112\n",
      "Epoch: 134, Loss: 0.8098, Val: 0.7210, Test: 0.7129\n",
      "Epoch: 135, Loss: 0.8116, Val: 0.7208, Test: 0.7138\n",
      "Epoch: 136, Loss: 0.8141, Val: 0.7224, Test: 0.7143\n",
      "Epoch: 137, Loss: 0.8115, Val: 0.7216, Test: 0.7137\n",
      "Epoch: 138, Loss: 0.8104, Val: 0.7215, Test: 0.7132\n",
      "Epoch: 139, Loss: 0.8066, Val: 0.7221, Test: 0.7147\n",
      "Epoch: 140, Loss: 0.8087, Val: 0.7215, Test: 0.7140\n",
      "Epoch: 141, Loss: 0.8101, Val: 0.7211, Test: 0.7136\n",
      "Epoch: 142, Loss: 0.8140, Val: 0.7207, Test: 0.7127\n",
      "Epoch: 143, Loss: 0.8094, Val: 0.7217, Test: 0.7133\n",
      "Epoch: 144, Loss: 0.8019, Val: 0.7215, Test: 0.7138\n",
      "Epoch: 145, Loss: 0.8061, Val: 0.7222, Test: 0.7138\n",
      "Epoch: 146, Loss: 0.8187, Val: 0.7224, Test: 0.7139\n",
      "Epoch: 147, Loss: 0.8094, Val: 0.7220, Test: 0.7134\n",
      "Epoch: 148, Loss: 0.7991, Val: 0.7212, Test: 0.7132\n",
      "Epoch: 149, Loss: 0.8065, Val: 0.7201, Test: 0.7136\n",
      "Epoch: 150, Loss: 0.8083, Val: 0.7204, Test: 0.7128\n",
      "Epoch: 151, Loss: 0.7973, Val: 0.7207, Test: 0.7135\n",
      "Epoch: 152, Loss: 0.7970, Val: 0.7210, Test: 0.7145\n",
      "Epoch: 153, Loss: 0.8015, Val: 0.7211, Test: 0.7142\n",
      "Epoch: 154, Loss: 0.8037, Val: 0.7215, Test: 0.7124\n",
      "Epoch: 155, Loss: 0.8018, Val: 0.7219, Test: 0.7133\n",
      "Epoch: 156, Loss: 0.7982, Val: 0.7225, Test: 0.7149\n",
      "Epoch: 157, Loss: 0.7988, Val: 0.7227, Test: 0.7166\n",
      "Epoch: 158, Loss: 0.7957, Val: 0.7223, Test: 0.7161\n",
      "Epoch: 159, Loss: 0.8036, Val: 0.7223, Test: 0.7134\n",
      "Epoch: 160, Loss: 0.8000, Val: 0.7217, Test: 0.7132\n",
      "Epoch: 161, Loss: 0.7915, Val: 0.7227, Test: 0.7157\n",
      "Epoch: 162, Loss: 0.8020, Val: 0.7228, Test: 0.7175\n",
      "Epoch: 163, Loss: 0.7989, Val: 0.7227, Test: 0.7174\n",
      "Epoch: 164, Loss: 0.8027, Val: 0.7230, Test: 0.7152\n",
      "Epoch: 165, Loss: 0.8027, Val: 0.7227, Test: 0.7128\n",
      "Epoch: 166, Loss: 0.7962, Val: 0.7222, Test: 0.7131\n",
      "Epoch: 167, Loss: 0.8008, Val: 0.7224, Test: 0.7154\n",
      "Epoch: 168, Loss: 0.8059, Val: 0.7221, Test: 0.7158\n",
      "Epoch: 169, Loss: 0.8004, Val: 0.7210, Test: 0.7142\n",
      "Epoch: 170, Loss: 0.8032, Val: 0.7210, Test: 0.7126\n",
      "Epoch: 171, Loss: 0.7957, Val: 0.7214, Test: 0.7119\n",
      "Epoch: 172, Loss: 0.8014, Val: 0.7222, Test: 0.7147\n",
      "Epoch: 173, Loss: 0.8048, Val: 0.7222, Test: 0.7163\n",
      "Epoch: 174, Loss: 0.8015, Val: 0.7225, Test: 0.7159\n",
      "Epoch: 175, Loss: 0.7983, Val: 0.7211, Test: 0.7135\n",
      "Epoch: 176, Loss: 0.7944, Val: 0.7210, Test: 0.7136\n",
      "Epoch: 177, Loss: 0.7924, Val: 0.7228, Test: 0.7161\n",
      "Epoch: 178, Loss: 0.7949, Val: 0.7228, Test: 0.7162\n",
      "Epoch: 179, Loss: 0.8000, Val: 0.7229, Test: 0.7154\n",
      "Epoch: 180, Loss: 0.7937, Val: 0.7233, Test: 0.7173\n",
      "Epoch: 181, Loss: 0.8000, Val: 0.7238, Test: 0.7189\n",
      "Epoch: 182, Loss: 0.7932, Val: 0.7240, Test: 0.7186\n",
      "Epoch: 183, Loss: 0.7973, Val: 0.7216, Test: 0.7149\n",
      "Epoch: 184, Loss: 0.7964, Val: 0.7206, Test: 0.7128\n",
      "Epoch: 185, Loss: 0.7921, Val: 0.7219, Test: 0.7143\n",
      "Epoch: 186, Loss: 0.7897, Val: 0.7231, Test: 0.7170\n",
      "Epoch: 187, Loss: 0.7943, Val: 0.7238, Test: 0.7183\n",
      "Epoch: 188, Loss: 0.7994, Val: 0.7228, Test: 0.7161\n",
      "Epoch: 189, Loss: 0.7898, Val: 0.7219, Test: 0.7131\n",
      "Epoch: 190, Loss: 0.7948, Val: 0.7217, Test: 0.7133\n",
      "Epoch: 191, Loss: 0.7948, Val: 0.7229, Test: 0.7175\n",
      "Epoch: 192, Loss: 0.7913, Val: 0.7230, Test: 0.7193\n",
      "Epoch: 193, Loss: 0.7937, Val: 0.7224, Test: 0.7178\n",
      "Epoch: 194, Loss: 0.7951, Val: 0.7210, Test: 0.7142\n",
      "Epoch: 195, Loss: 0.8074, Val: 0.7198, Test: 0.7127\n",
      "Epoch: 196, Loss: 0.7919, Val: 0.7221, Test: 0.7160\n",
      "Epoch: 197, Loss: 0.7862, Val: 0.7231, Test: 0.7185\n",
      "Epoch: 198, Loss: 0.7918, Val: 0.7222, Test: 0.7185\n",
      "Epoch: 199, Loss: 0.7917, Val: 0.7222, Test: 0.7168\n",
      "Epoch: 200, Loss: 0.7998, Val: 0.7224, Test: 0.7158\n",
      "Epoch: 201, Loss: 0.7864, Val: 0.7220, Test: 0.7151\n",
      "Epoch: 202, Loss: 0.7951, Val: 0.7215, Test: 0.7141\n",
      "Epoch: 203, Loss: 0.7928, Val: 0.7210, Test: 0.7142\n",
      "Epoch: 204, Loss: 0.7951, Val: 0.7214, Test: 0.7164\n",
      "Epoch: 205, Loss: 0.7879, Val: 0.7224, Test: 0.7171\n",
      "Epoch: 206, Loss: 0.7930, Val: 0.7225, Test: 0.7158\n",
      "Epoch: 207, Loss: 0.7908, Val: 0.7204, Test: 0.7128\n",
      "Epoch: 208, Loss: 0.7924, Val: 0.7209, Test: 0.7140\n",
      "Epoch: 209, Loss: 0.7866, Val: 0.7230, Test: 0.7174\n",
      "Epoch: 210, Loss: 0.7845, Val: 0.7227, Test: 0.7192\n",
      "Epoch: 211, Loss: 0.7830, Val: 0.7227, Test: 0.7174\n",
      "Epoch: 212, Loss: 0.7917, Val: 0.7208, Test: 0.7136\n",
      "Epoch: 213, Loss: 0.7836, Val: 0.7204, Test: 0.7142\n",
      "Epoch: 214, Loss: 0.7912, Val: 0.7217, Test: 0.7164\n",
      "Epoch: 215, Loss: 0.7962, Val: 0.7217, Test: 0.7171\n",
      "Epoch: 216, Loss: 0.7871, Val: 0.7227, Test: 0.7171\n",
      "Epoch: 217, Loss: 0.7894, Val: 0.7217, Test: 0.7163\n",
      "Epoch: 218, Loss: 0.7863, Val: 0.7217, Test: 0.7169\n",
      "Epoch: 219, Loss: 0.7896, Val: 0.7224, Test: 0.7179\n",
      "Epoch: 220, Loss: 0.7840, Val: 0.7225, Test: 0.7185\n",
      "Epoch: 221, Loss: 0.7885, Val: 0.7226, Test: 0.7177\n",
      "Epoch: 222, Loss: 0.7930, Val: 0.7223, Test: 0.7159\n",
      "Epoch: 223, Loss: 0.7878, Val: 0.7226, Test: 0.7161\n",
      "Epoch: 224, Loss: 0.7812, Val: 0.7231, Test: 0.7181\n",
      "Epoch: 225, Loss: 0.7910, Val: 0.7244, Test: 0.7179\n",
      "Epoch: 226, Loss: 0.7862, Val: 0.7241, Test: 0.7180\n",
      "Epoch: 227, Loss: 0.7877, Val: 0.7232, Test: 0.7167\n",
      "Epoch: 228, Loss: 0.7844, Val: 0.7223, Test: 0.7172\n",
      "Epoch: 229, Loss: 0.7833, Val: 0.7226, Test: 0.7176\n",
      "Epoch: 230, Loss: 0.7830, Val: 0.7238, Test: 0.7192\n",
      "Epoch: 231, Loss: 0.7864, Val: 0.7238, Test: 0.7186\n",
      "Epoch: 232, Loss: 0.7825, Val: 0.7228, Test: 0.7165\n",
      "Epoch: 233, Loss: 0.7828, Val: 0.7228, Test: 0.7158\n",
      "Epoch: 234, Loss: 0.7882, Val: 0.7228, Test: 0.7176\n",
      "Epoch: 235, Loss: 0.7812, Val: 0.7220, Test: 0.7172\n",
      "Epoch: 236, Loss: 0.7860, Val: 0.7216, Test: 0.7155\n",
      "Epoch: 237, Loss: 0.7822, Val: 0.7215, Test: 0.7138\n",
      "Epoch: 238, Loss: 0.7888, Val: 0.7229, Test: 0.7168\n",
      "Epoch: 239, Loss: 0.7805, Val: 0.7240, Test: 0.7203\n",
      "Epoch: 240, Loss: 0.7728, Val: 0.7241, Test: 0.7198\n",
      "Epoch: 241, Loss: 0.7872, Val: 0.7222, Test: 0.7168\n",
      "Epoch: 242, Loss: 0.7879, Val: 0.7204, Test: 0.7150\n",
      "Epoch: 243, Loss: 0.7859, Val: 0.7228, Test: 0.7175\n",
      "Epoch: 244, Loss: 0.7864, Val: 0.7231, Test: 0.7182\n",
      "Epoch: 245, Loss: 0.7877, Val: 0.7232, Test: 0.7161\n",
      "Epoch: 246, Loss: 0.7865, Val: 0.7221, Test: 0.7157\n",
      "Epoch: 247, Loss: 0.7828, Val: 0.7222, Test: 0.7151\n",
      "Epoch: 248, Loss: 0.7830, Val: 0.7226, Test: 0.7168\n",
      "Epoch: 249, Loss: 0.7773, Val: 0.7232, Test: 0.7181\n",
      "Epoch: 250, Loss: 0.7861, Val: 0.7242, Test: 0.7182\n",
      "Epoch: 251, Loss: 0.7831, Val: 0.7228, Test: 0.7157\n",
      "Epoch: 252, Loss: 0.7800, Val: 0.7219, Test: 0.7147\n",
      "Epoch: 253, Loss: 0.7884, Val: 0.7216, Test: 0.7153\n",
      "Epoch: 254, Loss: 0.7796, Val: 0.7228, Test: 0.7168\n",
      "Epoch: 255, Loss: 0.7799, Val: 0.7217, Test: 0.7166\n",
      "Epoch: 256, Loss: 0.7792, Val: 0.7226, Test: 0.7184\n",
      "Epoch: 257, Loss: 0.7767, Val: 0.7239, Test: 0.7201\n",
      "Epoch: 258, Loss: 0.7818, Val: 0.7244, Test: 0.7192\n",
      "Epoch: 259, Loss: 0.7699, Val: 0.7233, Test: 0.7186\n",
      "Epoch: 260, Loss: 0.7838, Val: 0.7242, Test: 0.7189\n",
      "Epoch: 261, Loss: 0.7795, Val: 0.7253, Test: 0.7197\n",
      "Epoch: 262, Loss: 0.7849, Val: 0.7244, Test: 0.7194\n",
      "Epoch: 263, Loss: 0.7783, Val: 0.7235, Test: 0.7187\n",
      "Epoch: 264, Loss: 0.7749, Val: 0.7229, Test: 0.7175\n",
      "Epoch: 265, Loss: 0.7863, Val: 0.7228, Test: 0.7158\n",
      "Epoch: 266, Loss: 0.7808, Val: 0.7240, Test: 0.7163\n",
      "Epoch: 267, Loss: 0.7788, Val: 0.7232, Test: 0.7173\n",
      "Epoch: 268, Loss: 0.7735, Val: 0.7244, Test: 0.7195\n",
      "Epoch: 269, Loss: 0.7720, Val: 0.7236, Test: 0.7185\n",
      "Epoch: 270, Loss: 0.7830, Val: 0.7223, Test: 0.7166\n",
      "Epoch: 271, Loss: 0.7901, Val: 0.7223, Test: 0.7168\n",
      "Epoch: 272, Loss: 0.7834, Val: 0.7237, Test: 0.7178\n",
      "Epoch: 273, Loss: 0.7776, Val: 0.7240, Test: 0.7187\n",
      "Epoch: 274, Loss: 0.7771, Val: 0.7230, Test: 0.7169\n",
      "Epoch: 275, Loss: 0.7741, Val: 0.7228, Test: 0.7165\n",
      "Epoch: 276, Loss: 0.7802, Val: 0.7224, Test: 0.7164\n",
      "Epoch: 277, Loss: 0.7809, Val: 0.7229, Test: 0.7168\n",
      "Epoch: 278, Loss: 0.7788, Val: 0.7228, Test: 0.7172\n",
      "Epoch: 279, Loss: 0.7780, Val: 0.7235, Test: 0.7181\n",
      "Epoch: 280, Loss: 0.7778, Val: 0.7223, Test: 0.7161\n",
      "Epoch: 281, Loss: 0.7818, Val: 0.7226, Test: 0.7164\n",
      "Epoch: 282, Loss: 0.7790, Val: 0.7240, Test: 0.7181\n",
      "Epoch: 283, Loss: 0.7775, Val: 0.7242, Test: 0.7173\n",
      "Epoch: 284, Loss: 0.7763, Val: 0.7242, Test: 0.7189\n",
      "Epoch: 285, Loss: 0.7800, Val: 0.7233, Test: 0.7170\n",
      "Epoch: 286, Loss: 0.7821, Val: 0.7216, Test: 0.7150\n",
      "Epoch: 287, Loss: 0.7764, Val: 0.7212, Test: 0.7145\n",
      "Epoch: 288, Loss: 0.7737, Val: 0.7229, Test: 0.7165\n",
      "Epoch: 289, Loss: 0.7781, Val: 0.7236, Test: 0.7163\n",
      "Epoch: 290, Loss: 0.7802, Val: 0.7233, Test: 0.7166\n",
      "Epoch: 291, Loss: 0.7754, Val: 0.7232, Test: 0.7161\n",
      "Epoch: 292, Loss: 0.7797, Val: 0.7222, Test: 0.7143\n",
      "Epoch: 293, Loss: 0.7797, Val: 0.7222, Test: 0.7160\n",
      "Epoch: 294, Loss: 0.7770, Val: 0.7231, Test: 0.7168\n",
      "Epoch: 295, Loss: 0.7737, Val: 0.7233, Test: 0.7174\n",
      "Epoch: 296, Loss: 0.7712, Val: 0.7239, Test: 0.7180\n",
      "Epoch: 297, Loss: 0.7787, Val: 0.7234, Test: 0.7180\n",
      "Epoch: 298, Loss: 0.7779, Val: 0.7226, Test: 0.7170\n",
      "Epoch: 299, Loss: 0.7830, Val: 0.7220, Test: 0.7166\n",
      "Epoch: 300, Loss: 0.7763, Val: 0.7225, Test: 0.7166\n",
      "0.55\n",
      "Label_rate: 0.55\n",
      "Epoch: 001, Loss: 1.9850, Val: 0.4107, Test: 0.3158\n",
      "Epoch: 002, Loss: 1.6289, Val: 0.5607, Test: 0.5240\n",
      "Epoch: 003, Loss: 1.4096, Val: 0.5979, Test: 0.5688\n",
      "Epoch: 004, Loss: 1.2732, Val: 0.6076, Test: 0.5846\n",
      "Epoch: 005, Loss: 1.2479, Val: 0.6395, Test: 0.6313\n",
      "Epoch: 006, Loss: 1.1502, Val: 0.6517, Test: 0.6479\n",
      "Epoch: 007, Loss: 1.1035, Val: 0.6609, Test: 0.6554\n",
      "Epoch: 008, Loss: 1.0698, Val: 0.6680, Test: 0.6641\n",
      "Epoch: 009, Loss: 1.0463, Val: 0.6791, Test: 0.6754\n",
      "Epoch: 010, Loss: 1.0373, Val: 0.6846, Test: 0.6850\n",
      "Epoch: 011, Loss: 1.0108, Val: 0.6864, Test: 0.6859\n",
      "Epoch: 012, Loss: 1.0001, Val: 0.6899, Test: 0.6877\n",
      "Epoch: 013, Loss: 0.9856, Val: 0.6939, Test: 0.6916\n",
      "Epoch: 014, Loss: 0.9713, Val: 0.6965, Test: 0.6944\n",
      "Epoch: 015, Loss: 0.9693, Val: 0.6966, Test: 0.6948\n",
      "Epoch: 016, Loss: 0.9713, Val: 0.6989, Test: 0.6950\n",
      "Epoch: 017, Loss: 0.9645, Val: 0.7013, Test: 0.6964\n",
      "Epoch: 018, Loss: 0.9497, Val: 0.7042, Test: 0.6998\n",
      "Epoch: 019, Loss: 0.9429, Val: 0.7047, Test: 0.7015\n",
      "Epoch: 020, Loss: 0.9253, Val: 0.7047, Test: 0.7004\n",
      "Epoch: 021, Loss: 0.9244, Val: 0.7050, Test: 0.6999\n",
      "Epoch: 022, Loss: 0.9175, Val: 0.7069, Test: 0.6986\n",
      "Epoch: 023, Loss: 0.9144, Val: 0.7076, Test: 0.6987\n",
      "Epoch: 024, Loss: 0.9157, Val: 0.7086, Test: 0.7006\n",
      "Epoch: 025, Loss: 0.9024, Val: 0.7112, Test: 0.7032\n",
      "Epoch: 026, Loss: 0.8990, Val: 0.7112, Test: 0.7022\n",
      "Epoch: 027, Loss: 0.8904, Val: 0.7096, Test: 0.7000\n",
      "Epoch: 028, Loss: 0.8968, Val: 0.7056, Test: 0.6960\n",
      "Epoch: 029, Loss: 0.8946, Val: 0.7051, Test: 0.6950\n",
      "Epoch: 030, Loss: 0.8938, Val: 0.7061, Test: 0.6965\n",
      "Epoch: 031, Loss: 0.8783, Val: 0.7092, Test: 0.6995\n",
      "Epoch: 032, Loss: 0.8738, Val: 0.7100, Test: 0.7017\n",
      "Epoch: 033, Loss: 0.8793, Val: 0.7109, Test: 0.7022\n",
      "Epoch: 034, Loss: 0.8754, Val: 0.7109, Test: 0.7029\n",
      "Epoch: 035, Loss: 0.8813, Val: 0.7117, Test: 0.7038\n",
      "Epoch: 036, Loss: 0.8740, Val: 0.7123, Test: 0.7054\n",
      "Epoch: 037, Loss: 0.8673, Val: 0.7130, Test: 0.7061\n",
      "Epoch: 038, Loss: 0.8696, Val: 0.7123, Test: 0.7049\n",
      "Epoch: 039, Loss: 0.8573, Val: 0.7113, Test: 0.7041\n",
      "Epoch: 040, Loss: 0.8571, Val: 0.7126, Test: 0.7043\n",
      "Epoch: 041, Loss: 0.8568, Val: 0.7137, Test: 0.7058\n",
      "Epoch: 042, Loss: 0.8511, Val: 0.7137, Test: 0.7070\n",
      "Epoch: 043, Loss: 0.8522, Val: 0.7135, Test: 0.7075\n",
      "Epoch: 044, Loss: 0.8443, Val: 0.7134, Test: 0.7078\n",
      "Epoch: 045, Loss: 0.8426, Val: 0.7132, Test: 0.7076\n",
      "Epoch: 046, Loss: 0.8423, Val: 0.7130, Test: 0.7065\n",
      "Epoch: 047, Loss: 0.8422, Val: 0.7131, Test: 0.7079\n",
      "Epoch: 048, Loss: 0.8480, Val: 0.7146, Test: 0.7100\n",
      "Epoch: 049, Loss: 0.8446, Val: 0.7157, Test: 0.7110\n",
      "Epoch: 050, Loss: 0.8459, Val: 0.7160, Test: 0.7120\n",
      "Epoch: 051, Loss: 0.8435, Val: 0.7152, Test: 0.7105\n",
      "Epoch: 052, Loss: 0.8403, Val: 0.7142, Test: 0.7077\n",
      "Epoch: 053, Loss: 0.8426, Val: 0.7138, Test: 0.7061\n",
      "Epoch: 054, Loss: 0.8372, Val: 0.7155, Test: 0.7080\n",
      "Epoch: 055, Loss: 0.8374, Val: 0.7164, Test: 0.7101\n",
      "Epoch: 056, Loss: 0.8371, Val: 0.7165, Test: 0.7104\n",
      "Epoch: 057, Loss: 0.8378, Val: 0.7159, Test: 0.7092\n",
      "Epoch: 058, Loss: 0.8314, Val: 0.7162, Test: 0.7078\n",
      "Epoch: 059, Loss: 0.8363, Val: 0.7162, Test: 0.7078\n",
      "Epoch: 060, Loss: 0.8289, Val: 0.7170, Test: 0.7104\n",
      "Epoch: 061, Loss: 0.8382, Val: 0.7172, Test: 0.7116\n",
      "Epoch: 062, Loss: 0.8258, Val: 0.7175, Test: 0.7122\n",
      "Epoch: 063, Loss: 0.8285, Val: 0.7171, Test: 0.7110\n",
      "Epoch: 064, Loss: 0.8304, Val: 0.7169, Test: 0.7099\n",
      "Epoch: 065, Loss: 0.8314, Val: 0.7166, Test: 0.7103\n",
      "Epoch: 066, Loss: 0.8226, Val: 0.7159, Test: 0.7100\n",
      "Epoch: 067, Loss: 0.8252, Val: 0.7162, Test: 0.7103\n",
      "Epoch: 068, Loss: 0.8320, Val: 0.7162, Test: 0.7107\n",
      "Epoch: 069, Loss: 0.8307, Val: 0.7165, Test: 0.7118\n",
      "Epoch: 070, Loss: 0.8279, Val: 0.7156, Test: 0.7123\n",
      "Epoch: 071, Loss: 0.8248, Val: 0.7159, Test: 0.7126\n",
      "Epoch: 072, Loss: 0.8238, Val: 0.7165, Test: 0.7116\n",
      "Epoch: 073, Loss: 0.8213, Val: 0.7167, Test: 0.7106\n",
      "Epoch: 074, Loss: 0.8207, Val: 0.7168, Test: 0.7104\n",
      "Epoch: 075, Loss: 0.8160, Val: 0.7170, Test: 0.7111\n",
      "Epoch: 076, Loss: 0.8229, Val: 0.7176, Test: 0.7131\n",
      "Epoch: 077, Loss: 0.8215, Val: 0.7182, Test: 0.7145\n",
      "Epoch: 078, Loss: 0.8190, Val: 0.7179, Test: 0.7141\n",
      "Epoch: 079, Loss: 0.8083, Val: 0.7162, Test: 0.7121\n",
      "Epoch: 080, Loss: 0.8161, Val: 0.7162, Test: 0.7109\n",
      "Epoch: 081, Loss: 0.8172, Val: 0.7162, Test: 0.7104\n",
      "Epoch: 082, Loss: 0.8159, Val: 0.7162, Test: 0.7117\n",
      "Epoch: 083, Loss: 0.8087, Val: 0.7164, Test: 0.7131\n",
      "Epoch: 084, Loss: 0.8242, Val: 0.7175, Test: 0.7146\n",
      "Epoch: 085, Loss: 0.8106, Val: 0.7182, Test: 0.7149\n",
      "Epoch: 086, Loss: 0.8204, Val: 0.7183, Test: 0.7148\n",
      "Epoch: 087, Loss: 0.8107, Val: 0.7183, Test: 0.7138\n",
      "Epoch: 088, Loss: 0.8159, Val: 0.7167, Test: 0.7123\n",
      "Epoch: 089, Loss: 0.8171, Val: 0.7167, Test: 0.7118\n",
      "Epoch: 090, Loss: 0.8133, Val: 0.7177, Test: 0.7140\n",
      "Epoch: 091, Loss: 0.8116, Val: 0.7182, Test: 0.7158\n",
      "Epoch: 092, Loss: 0.8175, Val: 0.7187, Test: 0.7160\n",
      "Epoch: 093, Loss: 0.8046, Val: 0.7188, Test: 0.7157\n",
      "Epoch: 094, Loss: 0.8176, Val: 0.7179, Test: 0.7142\n",
      "Epoch: 095, Loss: 0.8145, Val: 0.7180, Test: 0.7134\n",
      "Epoch: 096, Loss: 0.8080, Val: 0.7174, Test: 0.7148\n",
      "Epoch: 097, Loss: 0.8110, Val: 0.7190, Test: 0.7174\n",
      "Epoch: 098, Loss: 0.8029, Val: 0.7184, Test: 0.7159\n",
      "Epoch: 099, Loss: 0.8123, Val: 0.7188, Test: 0.7145\n",
      "Epoch: 100, Loss: 0.8062, Val: 0.7194, Test: 0.7147\n",
      "Epoch: 101, Loss: 0.8134, Val: 0.7193, Test: 0.7157\n",
      "Epoch: 102, Loss: 0.8126, Val: 0.7181, Test: 0.7150\n",
      "Epoch: 103, Loss: 0.8058, Val: 0.7182, Test: 0.7141\n",
      "Epoch: 104, Loss: 0.8001, Val: 0.7185, Test: 0.7129\n",
      "Epoch: 105, Loss: 0.8116, Val: 0.7185, Test: 0.7132\n",
      "Epoch: 106, Loss: 0.8039, Val: 0.7186, Test: 0.7158\n",
      "Epoch: 107, Loss: 0.8157, Val: 0.7195, Test: 0.7170\n",
      "Epoch: 108, Loss: 0.8106, Val: 0.7197, Test: 0.7161\n",
      "Epoch: 109, Loss: 0.8110, Val: 0.7193, Test: 0.7146\n",
      "Epoch: 110, Loss: 0.8113, Val: 0.7183, Test: 0.7135\n",
      "Epoch: 111, Loss: 0.8096, Val: 0.7183, Test: 0.7130\n",
      "Epoch: 112, Loss: 0.7981, Val: 0.7189, Test: 0.7132\n",
      "Epoch: 113, Loss: 0.8061, Val: 0.7190, Test: 0.7147\n",
      "Epoch: 114, Loss: 0.7964, Val: 0.7199, Test: 0.7154\n",
      "Epoch: 115, Loss: 0.8000, Val: 0.7210, Test: 0.7174\n",
      "Epoch: 116, Loss: 0.8066, Val: 0.7204, Test: 0.7183\n",
      "Epoch: 117, Loss: 0.8024, Val: 0.7204, Test: 0.7174\n",
      "Epoch: 118, Loss: 0.7941, Val: 0.7195, Test: 0.7147\n",
      "Epoch: 119, Loss: 0.7978, Val: 0.7187, Test: 0.7134\n",
      "Epoch: 120, Loss: 0.7950, Val: 0.7193, Test: 0.7136\n",
      "Epoch: 121, Loss: 0.8020, Val: 0.7199, Test: 0.7145\n",
      "Epoch: 122, Loss: 0.8042, Val: 0.7199, Test: 0.7154\n",
      "Epoch: 123, Loss: 0.8099, Val: 0.7194, Test: 0.7154\n",
      "Epoch: 124, Loss: 0.7933, Val: 0.7190, Test: 0.7148\n",
      "Epoch: 125, Loss: 0.8026, Val: 0.7188, Test: 0.7154\n",
      "Epoch: 126, Loss: 0.8097, Val: 0.7201, Test: 0.7162\n",
      "Epoch: 127, Loss: 0.7974, Val: 0.7213, Test: 0.7179\n",
      "Epoch: 128, Loss: 0.7947, Val: 0.7204, Test: 0.7171\n",
      "Epoch: 129, Loss: 0.8017, Val: 0.7201, Test: 0.7150\n",
      "Epoch: 130, Loss: 0.7991, Val: 0.7183, Test: 0.7131\n",
      "Epoch: 131, Loss: 0.7924, Val: 0.7196, Test: 0.7146\n",
      "Epoch: 132, Loss: 0.7881, Val: 0.7207, Test: 0.7182\n",
      "Epoch: 133, Loss: 0.7977, Val: 0.7213, Test: 0.7190\n",
      "Epoch: 134, Loss: 0.8031, Val: 0.7199, Test: 0.7164\n",
      "Epoch: 135, Loss: 0.7940, Val: 0.7189, Test: 0.7140\n",
      "Epoch: 136, Loss: 0.8042, Val: 0.7199, Test: 0.7149\n",
      "Epoch: 137, Loss: 0.8104, Val: 0.7205, Test: 0.7178\n",
      "Epoch: 138, Loss: 0.7933, Val: 0.7217, Test: 0.7208\n",
      "Epoch: 139, Loss: 0.7995, Val: 0.7220, Test: 0.7194\n",
      "Epoch: 140, Loss: 0.7986, Val: 0.7200, Test: 0.7154\n",
      "Epoch: 141, Loss: 0.7893, Val: 0.7192, Test: 0.7149\n",
      "Epoch: 142, Loss: 0.7937, Val: 0.7209, Test: 0.7180\n",
      "Epoch: 143, Loss: 0.7958, Val: 0.7216, Test: 0.7203\n",
      "Epoch: 144, Loss: 0.7877, Val: 0.7211, Test: 0.7180\n",
      "Epoch: 145, Loss: 0.7990, Val: 0.7199, Test: 0.7155\n",
      "Epoch: 146, Loss: 0.7971, Val: 0.7192, Test: 0.7154\n",
      "Epoch: 147, Loss: 0.7827, Val: 0.7207, Test: 0.7177\n",
      "Epoch: 148, Loss: 0.7951, Val: 0.7209, Test: 0.7188\n",
      "Epoch: 149, Loss: 0.7887, Val: 0.7220, Test: 0.7179\n",
      "Epoch: 150, Loss: 0.7898, Val: 0.7212, Test: 0.7172\n",
      "Epoch: 151, Loss: 0.7867, Val: 0.7218, Test: 0.7186\n",
      "Epoch: 152, Loss: 0.7912, Val: 0.7218, Test: 0.7187\n",
      "Epoch: 153, Loss: 0.7917, Val: 0.7208, Test: 0.7174\n",
      "Epoch: 154, Loss: 0.7950, Val: 0.7207, Test: 0.7164\n",
      "Epoch: 155, Loss: 0.7893, Val: 0.7210, Test: 0.7170\n",
      "Epoch: 156, Loss: 0.7898, Val: 0.7229, Test: 0.7195\n",
      "Epoch: 157, Loss: 0.7984, Val: 0.7228, Test: 0.7209\n",
      "Epoch: 158, Loss: 0.7901, Val: 0.7222, Test: 0.7190\n",
      "Epoch: 159, Loss: 0.7865, Val: 0.7218, Test: 0.7163\n",
      "Epoch: 160, Loss: 0.7854, Val: 0.7202, Test: 0.7162\n",
      "Epoch: 161, Loss: 0.7875, Val: 0.7209, Test: 0.7173\n",
      "Epoch: 162, Loss: 0.7897, Val: 0.7228, Test: 0.7207\n",
      "Epoch: 163, Loss: 0.7799, Val: 0.7236, Test: 0.7214\n",
      "Epoch: 164, Loss: 0.7914, Val: 0.7228, Test: 0.7194\n",
      "Epoch: 165, Loss: 0.7900, Val: 0.7217, Test: 0.7171\n",
      "Epoch: 166, Loss: 0.7821, Val: 0.7213, Test: 0.7169\n",
      "Epoch: 167, Loss: 0.7910, Val: 0.7218, Test: 0.7181\n",
      "Epoch: 168, Loss: 0.7842, Val: 0.7223, Test: 0.7200\n",
      "Epoch: 169, Loss: 0.7911, Val: 0.7234, Test: 0.7199\n",
      "Epoch: 170, Loss: 0.7914, Val: 0.7221, Test: 0.7186\n",
      "Epoch: 171, Loss: 0.7783, Val: 0.7213, Test: 0.7176\n",
      "Epoch: 172, Loss: 0.7803, Val: 0.7226, Test: 0.7182\n",
      "Epoch: 173, Loss: 0.7895, Val: 0.7241, Test: 0.7203\n",
      "Epoch: 174, Loss: 0.7912, Val: 0.7230, Test: 0.7204\n",
      "Epoch: 175, Loss: 0.7887, Val: 0.7223, Test: 0.7185\n",
      "Epoch: 176, Loss: 0.7890, Val: 0.7216, Test: 0.7175\n",
      "Epoch: 177, Loss: 0.7796, Val: 0.7228, Test: 0.7197\n",
      "Epoch: 178, Loss: 0.7837, Val: 0.7234, Test: 0.7209\n",
      "Epoch: 179, Loss: 0.7835, Val: 0.7226, Test: 0.7211\n",
      "Epoch: 180, Loss: 0.7917, Val: 0.7220, Test: 0.7195\n",
      "Epoch: 181, Loss: 0.7784, Val: 0.7230, Test: 0.7201\n",
      "Epoch: 182, Loss: 0.7866, Val: 0.7239, Test: 0.7213\n",
      "Epoch: 183, Loss: 0.7828, Val: 0.7229, Test: 0.7208\n",
      "Epoch: 184, Loss: 0.7869, Val: 0.7230, Test: 0.7193\n",
      "Epoch: 185, Loss: 0.7896, Val: 0.7219, Test: 0.7176\n",
      "Epoch: 186, Loss: 0.7819, Val: 0.7218, Test: 0.7173\n",
      "Epoch: 187, Loss: 0.7921, Val: 0.7227, Test: 0.7188\n",
      "Epoch: 188, Loss: 0.7803, Val: 0.7235, Test: 0.7206\n",
      "Epoch: 189, Loss: 0.7909, Val: 0.7231, Test: 0.7206\n",
      "Epoch: 190, Loss: 0.7929, Val: 0.7230, Test: 0.7197\n",
      "Epoch: 191, Loss: 0.7840, Val: 0.7229, Test: 0.7195\n",
      "Epoch: 192, Loss: 0.7870, Val: 0.7231, Test: 0.7208\n",
      "Epoch: 193, Loss: 0.7833, Val: 0.7231, Test: 0.7203\n",
      "Epoch: 194, Loss: 0.7755, Val: 0.7213, Test: 0.7175\n",
      "Epoch: 195, Loss: 0.7746, Val: 0.7215, Test: 0.7172\n",
      "Epoch: 196, Loss: 0.7878, Val: 0.7224, Test: 0.7186\n",
      "Epoch: 197, Loss: 0.7842, Val: 0.7235, Test: 0.7214\n",
      "Epoch: 198, Loss: 0.7840, Val: 0.7236, Test: 0.7226\n",
      "Epoch: 199, Loss: 0.7843, Val: 0.7223, Test: 0.7201\n",
      "Epoch: 200, Loss: 0.7869, Val: 0.7218, Test: 0.7184\n",
      "Epoch: 201, Loss: 0.7813, Val: 0.7227, Test: 0.7203\n",
      "Epoch: 202, Loss: 0.7749, Val: 0.7231, Test: 0.7222\n",
      "Epoch: 203, Loss: 0.7799, Val: 0.7233, Test: 0.7216\n",
      "Epoch: 204, Loss: 0.7834, Val: 0.7234, Test: 0.7201\n",
      "Epoch: 205, Loss: 0.7778, Val: 0.7231, Test: 0.7189\n",
      "Epoch: 206, Loss: 0.7785, Val: 0.7225, Test: 0.7187\n",
      "Epoch: 207, Loss: 0.7836, Val: 0.7220, Test: 0.7187\n",
      "Epoch: 208, Loss: 0.7815, Val: 0.7228, Test: 0.7206\n",
      "Epoch: 209, Loss: 0.7703, Val: 0.7238, Test: 0.7214\n",
      "Epoch: 210, Loss: 0.7775, Val: 0.7243, Test: 0.7209\n",
      "Epoch: 211, Loss: 0.7792, Val: 0.7232, Test: 0.7187\n",
      "Epoch: 212, Loss: 0.7901, Val: 0.7228, Test: 0.7184\n",
      "Epoch: 213, Loss: 0.7809, Val: 0.7233, Test: 0.7191\n",
      "Epoch: 214, Loss: 0.7866, Val: 0.7221, Test: 0.7186\n",
      "Epoch: 215, Loss: 0.7863, Val: 0.7216, Test: 0.7187\n",
      "Epoch: 216, Loss: 0.7863, Val: 0.7211, Test: 0.7182\n",
      "Epoch: 217, Loss: 0.7856, Val: 0.7233, Test: 0.7200\n",
      "Epoch: 218, Loss: 0.7874, Val: 0.7233, Test: 0.7203\n",
      "Epoch: 219, Loss: 0.7799, Val: 0.7232, Test: 0.7203\n",
      "Epoch: 220, Loss: 0.7769, Val: 0.7226, Test: 0.7201\n",
      "Epoch: 221, Loss: 0.7810, Val: 0.7221, Test: 0.7200\n",
      "Epoch: 222, Loss: 0.7737, Val: 0.7235, Test: 0.7216\n",
      "Epoch: 223, Loss: 0.7808, Val: 0.7235, Test: 0.7220\n",
      "Epoch: 224, Loss: 0.7830, Val: 0.7234, Test: 0.7207\n",
      "Epoch: 225, Loss: 0.7759, Val: 0.7228, Test: 0.7189\n",
      "Epoch: 226, Loss: 0.7857, Val: 0.7223, Test: 0.7177\n",
      "Epoch: 227, Loss: 0.7771, Val: 0.7235, Test: 0.7195\n",
      "Epoch: 228, Loss: 0.7789, Val: 0.7246, Test: 0.7217\n",
      "Epoch: 229, Loss: 0.7809, Val: 0.7244, Test: 0.7215\n",
      "Epoch: 230, Loss: 0.7790, Val: 0.7227, Test: 0.7195\n",
      "Epoch: 231, Loss: 0.7744, Val: 0.7223, Test: 0.7195\n",
      "Epoch: 232, Loss: 0.7768, Val: 0.7229, Test: 0.7198\n",
      "Epoch: 233, Loss: 0.7802, Val: 0.7223, Test: 0.7199\n",
      "Epoch: 234, Loss: 0.7718, Val: 0.7230, Test: 0.7195\n",
      "Epoch: 235, Loss: 0.7734, Val: 0.7218, Test: 0.7182\n",
      "Epoch: 236, Loss: 0.7825, Val: 0.7225, Test: 0.7200\n",
      "Epoch: 237, Loss: 0.7729, Val: 0.7237, Test: 0.7226\n",
      "Epoch: 238, Loss: 0.7742, Val: 0.7237, Test: 0.7223\n",
      "Epoch: 239, Loss: 0.7772, Val: 0.7221, Test: 0.7190\n",
      "Epoch: 240, Loss: 0.7887, Val: 0.7224, Test: 0.7191\n",
      "Epoch: 241, Loss: 0.7831, Val: 0.7227, Test: 0.7200\n",
      "Epoch: 242, Loss: 0.7767, Val: 0.7231, Test: 0.7207\n",
      "Epoch: 243, Loss: 0.7730, Val: 0.7228, Test: 0.7207\n",
      "Epoch: 244, Loss: 0.7774, Val: 0.7228, Test: 0.7197\n",
      "Epoch: 245, Loss: 0.7778, Val: 0.7228, Test: 0.7196\n",
      "Epoch: 246, Loss: 0.7772, Val: 0.7233, Test: 0.7209\n",
      "Epoch: 247, Loss: 0.7742, Val: 0.7244, Test: 0.7218\n",
      "Epoch: 248, Loss: 0.7831, Val: 0.7237, Test: 0.7211\n",
      "Epoch: 249, Loss: 0.7699, Val: 0.7230, Test: 0.7198\n",
      "Epoch: 250, Loss: 0.7737, Val: 0.7249, Test: 0.7205\n",
      "Epoch: 251, Loss: 0.7834, Val: 0.7241, Test: 0.7213\n",
      "Epoch: 252, Loss: 0.7816, Val: 0.7236, Test: 0.7213\n",
      "Epoch: 253, Loss: 0.7832, Val: 0.7227, Test: 0.7197\n",
      "Epoch: 254, Loss: 0.7735, Val: 0.7221, Test: 0.7188\n",
      "Epoch: 255, Loss: 0.7815, Val: 0.7222, Test: 0.7206\n",
      "Epoch: 256, Loss: 0.7787, Val: 0.7235, Test: 0.7213\n",
      "Epoch: 257, Loss: 0.7786, Val: 0.7239, Test: 0.7222\n",
      "Epoch: 258, Loss: 0.7839, Val: 0.7243, Test: 0.7217\n",
      "Epoch: 259, Loss: 0.7788, Val: 0.7230, Test: 0.7201\n",
      "Epoch: 260, Loss: 0.7761, Val: 0.7212, Test: 0.7191\n",
      "Epoch: 261, Loss: 0.7724, Val: 0.7234, Test: 0.7204\n",
      "Epoch: 262, Loss: 0.7698, Val: 0.7244, Test: 0.7214\n",
      "Epoch: 263, Loss: 0.7741, Val: 0.7247, Test: 0.7218\n",
      "Epoch: 264, Loss: 0.7680, Val: 0.7234, Test: 0.7194\n",
      "Epoch: 265, Loss: 0.7680, Val: 0.7217, Test: 0.7185\n",
      "Epoch: 266, Loss: 0.7691, Val: 0.7212, Test: 0.7179\n",
      "Epoch: 267, Loss: 0.7658, Val: 0.7234, Test: 0.7216\n",
      "Epoch: 268, Loss: 0.7718, Val: 0.7234, Test: 0.7236\n",
      "Epoch: 269, Loss: 0.7765, Val: 0.7235, Test: 0.7211\n",
      "Epoch: 270, Loss: 0.7752, Val: 0.7203, Test: 0.7156\n",
      "Epoch: 271, Loss: 0.7714, Val: 0.7216, Test: 0.7182\n",
      "Epoch: 272, Loss: 0.7773, Val: 0.7232, Test: 0.7209\n",
      "Epoch: 273, Loss: 0.7688, Val: 0.7238, Test: 0.7216\n",
      "Epoch: 274, Loss: 0.7692, Val: 0.7245, Test: 0.7208\n",
      "Epoch: 275, Loss: 0.7685, Val: 0.7241, Test: 0.7207\n",
      "Epoch: 276, Loss: 0.7781, Val: 0.7242, Test: 0.7211\n",
      "Epoch: 277, Loss: 0.7713, Val: 0.7245, Test: 0.7204\n",
      "Epoch: 278, Loss: 0.7748, Val: 0.7230, Test: 0.7193\n",
      "Epoch: 279, Loss: 0.7734, Val: 0.7238, Test: 0.7204\n",
      "Epoch: 280, Loss: 0.7750, Val: 0.7251, Test: 0.7224\n",
      "Epoch: 281, Loss: 0.7696, Val: 0.7245, Test: 0.7221\n",
      "Epoch: 282, Loss: 0.7694, Val: 0.7231, Test: 0.7196\n",
      "Epoch: 283, Loss: 0.7757, Val: 0.7202, Test: 0.7167\n",
      "Epoch: 284, Loss: 0.7799, Val: 0.7219, Test: 0.7190\n",
      "Epoch: 285, Loss: 0.7749, Val: 0.7230, Test: 0.7218\n",
      "Epoch: 286, Loss: 0.7672, Val: 0.7249, Test: 0.7230\n",
      "Epoch: 287, Loss: 0.7724, Val: 0.7243, Test: 0.7211\n",
      "Epoch: 288, Loss: 0.7679, Val: 0.7226, Test: 0.7197\n",
      "Epoch: 289, Loss: 0.7692, Val: 0.7236, Test: 0.7208\n",
      "Epoch: 290, Loss: 0.7690, Val: 0.7242, Test: 0.7220\n",
      "Epoch: 291, Loss: 0.7668, Val: 0.7242, Test: 0.7213\n",
      "Epoch: 292, Loss: 0.7709, Val: 0.7241, Test: 0.7208\n",
      "Epoch: 293, Loss: 0.7785, Val: 0.7240, Test: 0.7203\n",
      "Epoch: 294, Loss: 0.7679, Val: 0.7227, Test: 0.7206\n",
      "Epoch: 295, Loss: 0.7704, Val: 0.7243, Test: 0.7220\n",
      "Epoch: 296, Loss: 0.7728, Val: 0.7247, Test: 0.7213\n",
      "Epoch: 297, Loss: 0.7675, Val: 0.7239, Test: 0.7204\n",
      "Epoch: 298, Loss: 0.7683, Val: 0.7232, Test: 0.7202\n",
      "Epoch: 299, Loss: 0.7717, Val: 0.7217, Test: 0.7188\n",
      "Epoch: 300, Loss: 0.7748, Val: 0.7224, Test: 0.7198\n",
      "0.6000000000000001\n",
      "Label_rate: 0.6000000000000001\n",
      "Epoch: 001, Loss: 2.5165, Val: 0.4441, Test: 0.4090\n",
      "Epoch: 002, Loss: 1.8733, Val: 0.5273, Test: 0.4987\n",
      "Epoch: 003, Loss: 1.6377, Val: 0.5664, Test: 0.5587\n",
      "Epoch: 004, Loss: 1.5044, Val: 0.5535, Test: 0.5320\n",
      "Epoch: 005, Loss: 1.4570, Val: 0.6078, Test: 0.5867\n",
      "Epoch: 006, Loss: 1.2749, Val: 0.6300, Test: 0.6196\n",
      "Epoch: 007, Loss: 1.2718, Val: 0.6471, Test: 0.6388\n",
      "Epoch: 008, Loss: 1.2271, Val: 0.6514, Test: 0.6372\n",
      "Epoch: 009, Loss: 1.1868, Val: 0.6456, Test: 0.6317\n",
      "Epoch: 010, Loss: 1.1787, Val: 0.6681, Test: 0.6585\n",
      "Epoch: 011, Loss: 1.1205, Val: 0.6772, Test: 0.6725\n",
      "Epoch: 012, Loss: 1.0868, Val: 0.6807, Test: 0.6793\n",
      "Epoch: 013, Loss: 1.0917, Val: 0.6844, Test: 0.6802\n",
      "Epoch: 014, Loss: 1.0588, Val: 0.6836, Test: 0.6753\n",
      "Epoch: 015, Loss: 1.0144, Val: 0.6819, Test: 0.6722\n",
      "Epoch: 016, Loss: 1.0254, Val: 0.6856, Test: 0.6778\n",
      "Epoch: 017, Loss: 1.0155, Val: 0.6915, Test: 0.6847\n",
      "Epoch: 018, Loss: 0.9965, Val: 0.6930, Test: 0.6888\n",
      "Epoch: 019, Loss: 0.9869, Val: 0.6936, Test: 0.6903\n",
      "Epoch: 020, Loss: 0.9909, Val: 0.6949, Test: 0.6903\n",
      "Epoch: 021, Loss: 0.9668, Val: 0.6967, Test: 0.6908\n",
      "Epoch: 022, Loss: 0.9798, Val: 0.6978, Test: 0.6919\n",
      "Epoch: 023, Loss: 0.9658, Val: 0.7003, Test: 0.6930\n",
      "Epoch: 024, Loss: 0.9631, Val: 0.7032, Test: 0.6971\n",
      "Epoch: 025, Loss: 0.9478, Val: 0.7078, Test: 0.7006\n",
      "Epoch: 026, Loss: 0.9472, Val: 0.7073, Test: 0.7018\n",
      "Epoch: 027, Loss: 0.9385, Val: 0.7078, Test: 0.7019\n",
      "Epoch: 028, Loss: 0.9322, Val: 0.7072, Test: 0.7018\n",
      "Epoch: 029, Loss: 0.9201, Val: 0.7068, Test: 0.7023\n",
      "Epoch: 030, Loss: 0.9282, Val: 0.7070, Test: 0.7027\n",
      "Epoch: 031, Loss: 0.9197, Val: 0.7062, Test: 0.7018\n",
      "Epoch: 032, Loss: 0.9157, Val: 0.7072, Test: 0.7034\n",
      "Epoch: 033, Loss: 0.9023, Val: 0.7081, Test: 0.7062\n",
      "Epoch: 034, Loss: 0.9156, Val: 0.7093, Test: 0.7082\n",
      "Epoch: 035, Loss: 0.8965, Val: 0.7097, Test: 0.7095\n",
      "Epoch: 036, Loss: 0.8938, Val: 0.7114, Test: 0.7104\n",
      "Epoch: 037, Loss: 0.8953, Val: 0.7110, Test: 0.7107\n",
      "Epoch: 038, Loss: 0.8947, Val: 0.7113, Test: 0.7095\n",
      "Epoch: 039, Loss: 0.8775, Val: 0.7120, Test: 0.7110\n",
      "Epoch: 040, Loss: 0.8916, Val: 0.7134, Test: 0.7130\n",
      "Epoch: 041, Loss: 0.8864, Val: 0.7138, Test: 0.7143\n",
      "Epoch: 042, Loss: 0.8964, Val: 0.7142, Test: 0.7135\n",
      "Epoch: 043, Loss: 0.8661, Val: 0.7138, Test: 0.7117\n",
      "Epoch: 044, Loss: 0.8785, Val: 0.7127, Test: 0.7091\n",
      "Epoch: 045, Loss: 0.8786, Val: 0.7120, Test: 0.7075\n",
      "Epoch: 046, Loss: 0.8693, Val: 0.7141, Test: 0.7081\n",
      "Epoch: 047, Loss: 0.8669, Val: 0.7142, Test: 0.7093\n",
      "Epoch: 048, Loss: 0.8691, Val: 0.7152, Test: 0.7103\n",
      "Epoch: 049, Loss: 0.8626, Val: 0.7149, Test: 0.7101\n",
      "Epoch: 050, Loss: 0.8699, Val: 0.7159, Test: 0.7107\n",
      "Epoch: 051, Loss: 0.8728, Val: 0.7157, Test: 0.7096\n",
      "Epoch: 052, Loss: 0.8673, Val: 0.7148, Test: 0.7092\n",
      "Epoch: 053, Loss: 0.8660, Val: 0.7149, Test: 0.7096\n",
      "Epoch: 054, Loss: 0.8668, Val: 0.7160, Test: 0.7111\n",
      "Epoch: 055, Loss: 0.8597, Val: 0.7167, Test: 0.7144\n",
      "Epoch: 056, Loss: 0.8614, Val: 0.7174, Test: 0.7161\n",
      "Epoch: 057, Loss: 0.8528, Val: 0.7176, Test: 0.7158\n",
      "Epoch: 058, Loss: 0.8592, Val: 0.7173, Test: 0.7141\n",
      "Epoch: 059, Loss: 0.8576, Val: 0.7173, Test: 0.7137\n",
      "Epoch: 060, Loss: 0.8435, Val: 0.7172, Test: 0.7141\n",
      "Epoch: 061, Loss: 0.8583, Val: 0.7181, Test: 0.7147\n",
      "Epoch: 062, Loss: 0.8606, Val: 0.7181, Test: 0.7152\n",
      "Epoch: 063, Loss: 0.8467, Val: 0.7182, Test: 0.7143\n",
      "Epoch: 064, Loss: 0.8480, Val: 0.7174, Test: 0.7128\n",
      "Epoch: 065, Loss: 0.8484, Val: 0.7166, Test: 0.7117\n",
      "Epoch: 066, Loss: 0.8432, Val: 0.7166, Test: 0.7114\n",
      "Epoch: 067, Loss: 0.8481, Val: 0.7177, Test: 0.7123\n",
      "Epoch: 068, Loss: 0.8476, Val: 0.7192, Test: 0.7141\n",
      "Epoch: 069, Loss: 0.8487, Val: 0.7194, Test: 0.7150\n",
      "Epoch: 070, Loss: 0.8386, Val: 0.7189, Test: 0.7136\n",
      "Epoch: 071, Loss: 0.8359, Val: 0.7179, Test: 0.7113\n",
      "Epoch: 072, Loss: 0.8382, Val: 0.7169, Test: 0.7087\n",
      "Epoch: 073, Loss: 0.8481, Val: 0.7170, Test: 0.7092\n",
      "Epoch: 074, Loss: 0.8412, Val: 0.7177, Test: 0.7108\n",
      "Epoch: 075, Loss: 0.8359, Val: 0.7191, Test: 0.7114\n",
      "Epoch: 076, Loss: 0.8356, Val: 0.7195, Test: 0.7116\n",
      "Epoch: 077, Loss: 0.8349, Val: 0.7184, Test: 0.7112\n",
      "Epoch: 078, Loss: 0.8349, Val: 0.7176, Test: 0.7102\n",
      "Epoch: 079, Loss: 0.8289, Val: 0.7175, Test: 0.7089\n",
      "Epoch: 080, Loss: 0.8380, Val: 0.7184, Test: 0.7102\n",
      "Epoch: 081, Loss: 0.8352, Val: 0.7187, Test: 0.7120\n",
      "Epoch: 082, Loss: 0.8375, Val: 0.7184, Test: 0.7121\n",
      "Epoch: 083, Loss: 0.8321, Val: 0.7191, Test: 0.7138\n",
      "Epoch: 084, Loss: 0.8316, Val: 0.7206, Test: 0.7148\n",
      "Epoch: 085, Loss: 0.8328, Val: 0.7206, Test: 0.7157\n",
      "Epoch: 086, Loss: 0.8334, Val: 0.7205, Test: 0.7142\n",
      "Epoch: 087, Loss: 0.8303, Val: 0.7191, Test: 0.7129\n",
      "Epoch: 088, Loss: 0.8241, Val: 0.7183, Test: 0.7119\n",
      "Epoch: 089, Loss: 0.8343, Val: 0.7191, Test: 0.7141\n",
      "Epoch: 090, Loss: 0.8335, Val: 0.7201, Test: 0.7154\n",
      "Epoch: 091, Loss: 0.8305, Val: 0.7201, Test: 0.7143\n",
      "Epoch: 092, Loss: 0.8220, Val: 0.7195, Test: 0.7129\n",
      "Epoch: 093, Loss: 0.8236, Val: 0.7186, Test: 0.7124\n",
      "Epoch: 094, Loss: 0.8193, Val: 0.7193, Test: 0.7134\n",
      "Epoch: 095, Loss: 0.8284, Val: 0.7202, Test: 0.7147\n",
      "Epoch: 096, Loss: 0.8191, Val: 0.7203, Test: 0.7154\n",
      "Epoch: 097, Loss: 0.8238, Val: 0.7206, Test: 0.7150\n",
      "Epoch: 098, Loss: 0.8215, Val: 0.7199, Test: 0.7146\n",
      "Epoch: 099, Loss: 0.8311, Val: 0.7193, Test: 0.7142\n",
      "Epoch: 100, Loss: 0.8219, Val: 0.7183, Test: 0.7121\n",
      "Epoch: 101, Loss: 0.8231, Val: 0.7188, Test: 0.7132\n",
      "Epoch: 102, Loss: 0.8193, Val: 0.7198, Test: 0.7151\n",
      "Epoch: 103, Loss: 0.8216, Val: 0.7199, Test: 0.7159\n",
      "Epoch: 104, Loss: 0.8165, Val: 0.7198, Test: 0.7145\n",
      "Epoch: 105, Loss: 0.8278, Val: 0.7205, Test: 0.7140\n",
      "Epoch: 106, Loss: 0.8099, Val: 0.7206, Test: 0.7139\n",
      "Epoch: 107, Loss: 0.8293, Val: 0.7200, Test: 0.7135\n",
      "Epoch: 108, Loss: 0.8159, Val: 0.7193, Test: 0.7125\n",
      "Epoch: 109, Loss: 0.8165, Val: 0.7194, Test: 0.7144\n",
      "Epoch: 110, Loss: 0.8145, Val: 0.7202, Test: 0.7162\n",
      "Epoch: 111, Loss: 0.8117, Val: 0.7210, Test: 0.7173\n",
      "Epoch: 112, Loss: 0.8193, Val: 0.7204, Test: 0.7172\n",
      "Epoch: 113, Loss: 0.8183, Val: 0.7202, Test: 0.7149\n",
      "Epoch: 114, Loss: 0.8151, Val: 0.7191, Test: 0.7123\n",
      "Epoch: 115, Loss: 0.8099, Val: 0.7192, Test: 0.7127\n",
      "Epoch: 116, Loss: 0.8170, Val: 0.7197, Test: 0.7143\n",
      "Epoch: 117, Loss: 0.8105, Val: 0.7211, Test: 0.7176\n",
      "Epoch: 118, Loss: 0.8133, Val: 0.7213, Test: 0.7191\n",
      "Epoch: 119, Loss: 0.8175, Val: 0.7215, Test: 0.7186\n",
      "Epoch: 120, Loss: 0.8162, Val: 0.7206, Test: 0.7154\n",
      "Epoch: 121, Loss: 0.8135, Val: 0.7200, Test: 0.7114\n",
      "Epoch: 122, Loss: 0.8093, Val: 0.7196, Test: 0.7119\n",
      "Epoch: 123, Loss: 0.8014, Val: 0.7208, Test: 0.7153\n",
      "Epoch: 124, Loss: 0.8142, Val: 0.7215, Test: 0.7165\n",
      "Epoch: 125, Loss: 0.8216, Val: 0.7214, Test: 0.7163\n",
      "Epoch: 126, Loss: 0.8187, Val: 0.7199, Test: 0.7136\n",
      "Epoch: 127, Loss: 0.8136, Val: 0.7187, Test: 0.7120\n",
      "Epoch: 128, Loss: 0.8070, Val: 0.7192, Test: 0.7128\n",
      "Epoch: 129, Loss: 0.8127, Val: 0.7199, Test: 0.7148\n",
      "Epoch: 130, Loss: 0.8049, Val: 0.7223, Test: 0.7180\n",
      "Epoch: 131, Loss: 0.8139, Val: 0.7228, Test: 0.7192\n",
      "Epoch: 132, Loss: 0.8082, Val: 0.7217, Test: 0.7178\n",
      "Epoch: 133, Loss: 0.8070, Val: 0.7213, Test: 0.7156\n",
      "Epoch: 134, Loss: 0.8070, Val: 0.7207, Test: 0.7136\n",
      "Epoch: 135, Loss: 0.8066, Val: 0.7203, Test: 0.7129\n",
      "Epoch: 136, Loss: 0.8004, Val: 0.7213, Test: 0.7142\n",
      "Epoch: 137, Loss: 0.8081, Val: 0.7221, Test: 0.7161\n",
      "Epoch: 138, Loss: 0.8114, Val: 0.7228, Test: 0.7178\n",
      "Epoch: 139, Loss: 0.8015, Val: 0.7225, Test: 0.7166\n",
      "Epoch: 140, Loss: 0.8083, Val: 0.7211, Test: 0.7134\n",
      "Epoch: 141, Loss: 0.8036, Val: 0.7184, Test: 0.7118\n",
      "Epoch: 142, Loss: 0.8049, Val: 0.7203, Test: 0.7141\n",
      "Epoch: 143, Loss: 0.8041, Val: 0.7222, Test: 0.7167\n",
      "Epoch: 144, Loss: 0.7946, Val: 0.7229, Test: 0.7178\n",
      "Epoch: 145, Loss: 0.8083, Val: 0.7230, Test: 0.7174\n",
      "Epoch: 146, Loss: 0.8079, Val: 0.7217, Test: 0.7144\n",
      "Epoch: 147, Loss: 0.8015, Val: 0.7198, Test: 0.7118\n",
      "Epoch: 148, Loss: 0.8005, Val: 0.7209, Test: 0.7140\n",
      "Epoch: 149, Loss: 0.8055, Val: 0.7217, Test: 0.7160\n",
      "Epoch: 150, Loss: 0.8090, Val: 0.7217, Test: 0.7156\n",
      "Epoch: 151, Loss: 0.7942, Val: 0.7213, Test: 0.7140\n",
      "Epoch: 152, Loss: 0.8031, Val: 0.7204, Test: 0.7135\n",
      "Epoch: 153, Loss: 0.8024, Val: 0.7215, Test: 0.7137\n",
      "Epoch: 154, Loss: 0.8091, Val: 0.7217, Test: 0.7155\n",
      "Epoch: 155, Loss: 0.7949, Val: 0.7218, Test: 0.7158\n",
      "Epoch: 156, Loss: 0.8001, Val: 0.7227, Test: 0.7158\n",
      "Epoch: 157, Loss: 0.7977, Val: 0.7222, Test: 0.7159\n",
      "Epoch: 158, Loss: 0.8050, Val: 0.7211, Test: 0.7153\n",
      "Epoch: 159, Loss: 0.8027, Val: 0.7223, Test: 0.7155\n",
      "Epoch: 160, Loss: 0.7979, Val: 0.7222, Test: 0.7161\n",
      "Epoch: 161, Loss: 0.7981, Val: 0.7217, Test: 0.7167\n",
      "Epoch: 162, Loss: 0.7930, Val: 0.7219, Test: 0.7166\n",
      "Epoch: 163, Loss: 0.8018, Val: 0.7219, Test: 0.7167\n",
      "Epoch: 164, Loss: 0.7957, Val: 0.7224, Test: 0.7168\n",
      "Epoch: 165, Loss: 0.8030, Val: 0.7222, Test: 0.7178\n",
      "Epoch: 166, Loss: 0.7989, Val: 0.7221, Test: 0.7188\n",
      "Epoch: 167, Loss: 0.7958, Val: 0.7227, Test: 0.7179\n",
      "Epoch: 168, Loss: 0.7930, Val: 0.7222, Test: 0.7154\n",
      "Epoch: 169, Loss: 0.7923, Val: 0.7215, Test: 0.7144\n",
      "Epoch: 170, Loss: 0.7965, Val: 0.7240, Test: 0.7184\n",
      "Epoch: 171, Loss: 0.7996, Val: 0.7249, Test: 0.7205\n",
      "Epoch: 172, Loss: 0.7974, Val: 0.7240, Test: 0.7182\n",
      "Epoch: 173, Loss: 0.7954, Val: 0.7224, Test: 0.7153\n",
      "Epoch: 174, Loss: 0.7947, Val: 0.7210, Test: 0.7133\n",
      "Epoch: 175, Loss: 0.7948, Val: 0.7213, Test: 0.7143\n",
      "Epoch: 176, Loss: 0.7879, Val: 0.7229, Test: 0.7168\n",
      "Epoch: 177, Loss: 0.7929, Val: 0.7230, Test: 0.7159\n",
      "Epoch: 178, Loss: 0.7914, Val: 0.7240, Test: 0.7167\n",
      "Epoch: 179, Loss: 0.7914, Val: 0.7234, Test: 0.7154\n",
      "Epoch: 180, Loss: 0.7970, Val: 0.7212, Test: 0.7144\n",
      "Epoch: 181, Loss: 0.7979, Val: 0.7211, Test: 0.7149\n",
      "Epoch: 182, Loss: 0.7961, Val: 0.7224, Test: 0.7158\n",
      "Epoch: 183, Loss: 0.8014, Val: 0.7225, Test: 0.7164\n",
      "Epoch: 184, Loss: 0.8006, Val: 0.7225, Test: 0.7158\n",
      "Epoch: 185, Loss: 0.7898, Val: 0.7233, Test: 0.7159\n",
      "Epoch: 186, Loss: 0.7816, Val: 0.7231, Test: 0.7162\n",
      "Epoch: 187, Loss: 0.7965, Val: 0.7226, Test: 0.7156\n",
      "Epoch: 188, Loss: 0.7915, Val: 0.7213, Test: 0.7146\n",
      "Epoch: 189, Loss: 0.7898, Val: 0.7210, Test: 0.7145\n",
      "Epoch: 190, Loss: 0.7913, Val: 0.7211, Test: 0.7139\n",
      "Epoch: 191, Loss: 0.7936, Val: 0.7220, Test: 0.7153\n",
      "Epoch: 192, Loss: 0.7974, Val: 0.7234, Test: 0.7173\n",
      "Epoch: 193, Loss: 0.8019, Val: 0.7231, Test: 0.7174\n",
      "Epoch: 194, Loss: 0.7861, Val: 0.7225, Test: 0.7165\n",
      "Epoch: 195, Loss: 0.7947, Val: 0.7226, Test: 0.7167\n",
      "Epoch: 196, Loss: 0.7877, Val: 0.7234, Test: 0.7173\n",
      "Epoch: 197, Loss: 0.7957, Val: 0.7225, Test: 0.7159\n",
      "Epoch: 198, Loss: 0.7955, Val: 0.7210, Test: 0.7136\n",
      "Epoch: 199, Loss: 0.7845, Val: 0.7217, Test: 0.7138\n",
      "Epoch: 200, Loss: 0.7960, Val: 0.7223, Test: 0.7149\n",
      "Epoch: 201, Loss: 0.7888, Val: 0.7229, Test: 0.7158\n",
      "Epoch: 202, Loss: 0.7964, Val: 0.7228, Test: 0.7166\n",
      "Epoch: 203, Loss: 0.7961, Val: 0.7228, Test: 0.7166\n",
      "Epoch: 204, Loss: 0.7924, Val: 0.7224, Test: 0.7160\n",
      "Epoch: 205, Loss: 0.7860, Val: 0.7227, Test: 0.7150\n",
      "Epoch: 206, Loss: 0.7886, Val: 0.7235, Test: 0.7159\n",
      "Epoch: 207, Loss: 0.7919, Val: 0.7234, Test: 0.7156\n",
      "Epoch: 208, Loss: 0.7844, Val: 0.7222, Test: 0.7145\n",
      "Epoch: 209, Loss: 0.7915, Val: 0.7211, Test: 0.7142\n",
      "Epoch: 210, Loss: 0.7914, Val: 0.7217, Test: 0.7151\n",
      "Epoch: 211, Loss: 0.7941, Val: 0.7233, Test: 0.7157\n",
      "Epoch: 212, Loss: 0.7896, Val: 0.7243, Test: 0.7172\n",
      "Epoch: 213, Loss: 0.7802, Val: 0.7251, Test: 0.7172\n",
      "Epoch: 214, Loss: 0.7887, Val: 0.7238, Test: 0.7150\n",
      "Epoch: 215, Loss: 0.7949, Val: 0.7219, Test: 0.7134\n",
      "Epoch: 216, Loss: 0.7802, Val: 0.7206, Test: 0.7124\n",
      "Epoch: 217, Loss: 0.7813, Val: 0.7229, Test: 0.7149\n",
      "Epoch: 218, Loss: 0.7826, Val: 0.7241, Test: 0.7164\n",
      "Epoch: 219, Loss: 0.7871, Val: 0.7242, Test: 0.7170\n",
      "Epoch: 220, Loss: 0.7933, Val: 0.7239, Test: 0.7171\n",
      "Epoch: 221, Loss: 0.7800, Val: 0.7229, Test: 0.7161\n",
      "Epoch: 222, Loss: 0.7916, Val: 0.7229, Test: 0.7161\n",
      "Epoch: 223, Loss: 0.7848, Val: 0.7243, Test: 0.7179\n",
      "Epoch: 224, Loss: 0.7845, Val: 0.7250, Test: 0.7181\n",
      "Epoch: 225, Loss: 0.7793, Val: 0.7251, Test: 0.7188\n",
      "Epoch: 226, Loss: 0.7804, Val: 0.7249, Test: 0.7187\n",
      "Epoch: 227, Loss: 0.7831, Val: 0.7248, Test: 0.7178\n",
      "Epoch: 228, Loss: 0.7782, Val: 0.7231, Test: 0.7156\n",
      "Epoch: 229, Loss: 0.7839, Val: 0.7223, Test: 0.7150\n",
      "Epoch: 230, Loss: 0.7826, Val: 0.7225, Test: 0.7160\n",
      "Epoch: 231, Loss: 0.7959, Val: 0.7226, Test: 0.7166\n",
      "Epoch: 232, Loss: 0.7804, Val: 0.7228, Test: 0.7161\n",
      "Epoch: 233, Loss: 0.7851, Val: 0.7217, Test: 0.7143\n",
      "Epoch: 234, Loss: 0.7869, Val: 0.7216, Test: 0.7135\n",
      "Epoch: 235, Loss: 0.7799, Val: 0.7233, Test: 0.7160\n",
      "Epoch: 236, Loss: 0.7801, Val: 0.7246, Test: 0.7167\n",
      "Epoch: 237, Loss: 0.7824, Val: 0.7230, Test: 0.7150\n",
      "Epoch: 238, Loss: 0.7814, Val: 0.7209, Test: 0.7126\n",
      "Epoch: 239, Loss: 0.7820, Val: 0.7209, Test: 0.7128\n",
      "Epoch: 240, Loss: 0.7808, Val: 0.7225, Test: 0.7156\n",
      "Epoch: 241, Loss: 0.7762, Val: 0.7240, Test: 0.7182\n",
      "Epoch: 242, Loss: 0.7754, Val: 0.7242, Test: 0.7174\n",
      "Epoch: 243, Loss: 0.7818, Val: 0.7225, Test: 0.7153\n",
      "Epoch: 244, Loss: 0.7742, Val: 0.7219, Test: 0.7139\n",
      "Epoch: 245, Loss: 0.7820, Val: 0.7234, Test: 0.7151\n",
      "Epoch: 246, Loss: 0.7787, Val: 0.7239, Test: 0.7185\n",
      "Epoch: 247, Loss: 0.7732, Val: 0.7227, Test: 0.7188\n",
      "Epoch: 248, Loss: 0.7848, Val: 0.7224, Test: 0.7171\n",
      "Epoch: 249, Loss: 0.7822, Val: 0.7211, Test: 0.7147\n",
      "Epoch: 250, Loss: 0.7852, Val: 0.7215, Test: 0.7139\n",
      "Epoch: 251, Loss: 0.7743, Val: 0.7236, Test: 0.7165\n",
      "Epoch: 252, Loss: 0.7732, Val: 0.7245, Test: 0.7185\n",
      "Epoch: 253, Loss: 0.7763, Val: 0.7250, Test: 0.7187\n",
      "Epoch: 254, Loss: 0.7743, Val: 0.7226, Test: 0.7172\n",
      "Epoch: 255, Loss: 0.7800, Val: 0.7200, Test: 0.7140\n",
      "Epoch: 256, Loss: 0.7743, Val: 0.7197, Test: 0.7126\n",
      "Epoch: 257, Loss: 0.7813, Val: 0.7230, Test: 0.7168\n",
      "Epoch: 258, Loss: 0.7769, Val: 0.7256, Test: 0.7194\n",
      "Epoch: 259, Loss: 0.7752, Val: 0.7250, Test: 0.7183\n",
      "Epoch: 260, Loss: 0.7768, Val: 0.7215, Test: 0.7128\n",
      "Epoch: 261, Loss: 0.7752, Val: 0.7196, Test: 0.7112\n",
      "Epoch: 262, Loss: 0.7796, Val: 0.7214, Test: 0.7143\n",
      "Epoch: 263, Loss: 0.7754, Val: 0.7238, Test: 0.7185\n",
      "Epoch: 264, Loss: 0.7812, Val: 0.7251, Test: 0.7196\n",
      "Epoch: 265, Loss: 0.7757, Val: 0.7240, Test: 0.7157\n",
      "Epoch: 266, Loss: 0.7673, Val: 0.7218, Test: 0.7141\n",
      "Epoch: 267, Loss: 0.7742, Val: 0.7210, Test: 0.7147\n",
      "Epoch: 268, Loss: 0.7737, Val: 0.7227, Test: 0.7166\n",
      "Epoch: 269, Loss: 0.7862, Val: 0.7216, Test: 0.7168\n",
      "Epoch: 270, Loss: 0.7764, Val: 0.7209, Test: 0.7156\n",
      "Epoch: 271, Loss: 0.7808, Val: 0.7217, Test: 0.7151\n",
      "Epoch: 272, Loss: 0.7742, Val: 0.7241, Test: 0.7186\n",
      "Epoch: 273, Loss: 0.7869, Val: 0.7252, Test: 0.7200\n",
      "Epoch: 274, Loss: 0.7799, Val: 0.7242, Test: 0.7181\n",
      "Epoch: 275, Loss: 0.7763, Val: 0.7215, Test: 0.7145\n",
      "Epoch: 276, Loss: 0.7692, Val: 0.7207, Test: 0.7135\n",
      "Epoch: 277, Loss: 0.7771, Val: 0.7217, Test: 0.7158\n",
      "Epoch: 278, Loss: 0.7740, Val: 0.7240, Test: 0.7185\n",
      "Epoch: 279, Loss: 0.7676, Val: 0.7255, Test: 0.7193\n",
      "Epoch: 280, Loss: 0.7764, Val: 0.7244, Test: 0.7170\n",
      "Epoch: 281, Loss: 0.7815, Val: 0.7225, Test: 0.7160\n",
      "Epoch: 282, Loss: 0.7754, Val: 0.7230, Test: 0.7181\n",
      "Epoch: 283, Loss: 0.7713, Val: 0.7241, Test: 0.7192\n",
      "Epoch: 284, Loss: 0.7725, Val: 0.7237, Test: 0.7178\n",
      "Epoch: 285, Loss: 0.7747, Val: 0.7235, Test: 0.7177\n",
      "Epoch: 286, Loss: 0.7767, Val: 0.7241, Test: 0.7178\n",
      "Epoch: 287, Loss: 0.7791, Val: 0.7244, Test: 0.7193\n",
      "Epoch: 288, Loss: 0.7767, Val: 0.7240, Test: 0.7186\n",
      "Epoch: 289, Loss: 0.7802, Val: 0.7240, Test: 0.7186\n",
      "Epoch: 290, Loss: 0.7792, Val: 0.7230, Test: 0.7165\n",
      "Epoch: 291, Loss: 0.7795, Val: 0.7221, Test: 0.7162\n",
      "Epoch: 292, Loss: 0.7756, Val: 0.7235, Test: 0.7175\n",
      "Epoch: 293, Loss: 0.7702, Val: 0.7234, Test: 0.7172\n",
      "Epoch: 294, Loss: 0.7828, Val: 0.7213, Test: 0.7149\n",
      "Epoch: 295, Loss: 0.7729, Val: 0.7214, Test: 0.7149\n",
      "Epoch: 296, Loss: 0.7670, Val: 0.7223, Test: 0.7167\n",
      "Epoch: 297, Loss: 0.7739, Val: 0.7235, Test: 0.7185\n",
      "Epoch: 298, Loss: 0.7754, Val: 0.7243, Test: 0.7188\n",
      "Epoch: 299, Loss: 0.7732, Val: 0.7238, Test: 0.7187\n",
      "Epoch: 300, Loss: 0.7740, Val: 0.7247, Test: 0.7186\n",
      "0.65\n",
      "Label_rate: 0.65\n",
      "Epoch: 001, Loss: 2.2253, Val: 0.4887, Test: 0.4039\n",
      "Epoch: 002, Loss: 1.7201, Val: 0.5657, Test: 0.5251\n",
      "Epoch: 003, Loss: 1.4412, Val: 0.6098, Test: 0.5863\n",
      "Epoch: 004, Loss: 1.2763, Val: 0.6146, Test: 0.5908\n",
      "Epoch: 005, Loss: 1.2291, Val: 0.6288, Test: 0.6150\n",
      "Epoch: 006, Loss: 1.1910, Val: 0.6484, Test: 0.6431\n",
      "Epoch: 007, Loss: 1.1236, Val: 0.6615, Test: 0.6547\n",
      "Epoch: 008, Loss: 1.0676, Val: 0.6634, Test: 0.6537\n",
      "Epoch: 009, Loss: 1.0468, Val: 0.6691, Test: 0.6619\n",
      "Epoch: 010, Loss: 1.0499, Val: 0.6812, Test: 0.6780\n",
      "Epoch: 011, Loss: 1.0344, Val: 0.6893, Test: 0.6870\n",
      "Epoch: 012, Loss: 1.0277, Val: 0.6946, Test: 0.6910\n",
      "Epoch: 013, Loss: 1.0018, Val: 0.6965, Test: 0.6910\n",
      "Epoch: 014, Loss: 0.9796, Val: 0.6915, Test: 0.6849\n",
      "Epoch: 015, Loss: 0.9788, Val: 0.6917, Test: 0.6827\n",
      "Epoch: 016, Loss: 0.9696, Val: 0.6927, Test: 0.6879\n",
      "Epoch: 017, Loss: 0.9616, Val: 0.6950, Test: 0.6893\n",
      "Epoch: 018, Loss: 0.9358, Val: 0.6944, Test: 0.6881\n",
      "Epoch: 019, Loss: 0.9301, Val: 0.6928, Test: 0.6863\n",
      "Epoch: 020, Loss: 0.9222, Val: 0.6950, Test: 0.6895\n",
      "Epoch: 021, Loss: 0.9340, Val: 0.6977, Test: 0.6921\n",
      "Epoch: 022, Loss: 0.9226, Val: 0.6977, Test: 0.6924\n",
      "Epoch: 023, Loss: 0.9142, Val: 0.6985, Test: 0.6913\n",
      "Epoch: 024, Loss: 0.9227, Val: 0.6977, Test: 0.6896\n",
      "Epoch: 025, Loss: 0.8977, Val: 0.6994, Test: 0.6913\n",
      "Epoch: 026, Loss: 0.8927, Val: 0.7046, Test: 0.6985\n",
      "Epoch: 027, Loss: 0.8999, Val: 0.7060, Test: 0.7031\n",
      "Epoch: 028, Loss: 0.8926, Val: 0.7063, Test: 0.7036\n",
      "Epoch: 029, Loss: 0.8873, Val: 0.7042, Test: 0.6985\n",
      "Epoch: 030, Loss: 0.8896, Val: 0.7021, Test: 0.6931\n",
      "Epoch: 031, Loss: 0.8753, Val: 0.7010, Test: 0.6936\n",
      "Epoch: 032, Loss: 0.8813, Val: 0.7037, Test: 0.6979\n",
      "Epoch: 033, Loss: 0.8745, Val: 0.7053, Test: 0.7007\n",
      "Epoch: 034, Loss: 0.8719, Val: 0.7061, Test: 0.7013\n",
      "Epoch: 035, Loss: 0.8666, Val: 0.7054, Test: 0.7006\n",
      "Epoch: 036, Loss: 0.8633, Val: 0.7048, Test: 0.6997\n",
      "Epoch: 037, Loss: 0.8675, Val: 0.7055, Test: 0.7006\n",
      "Epoch: 038, Loss: 0.8643, Val: 0.7079, Test: 0.7036\n",
      "Epoch: 039, Loss: 0.8519, Val: 0.7103, Test: 0.7053\n",
      "Epoch: 040, Loss: 0.8682, Val: 0.7088, Test: 0.7047\n",
      "Epoch: 041, Loss: 0.8565, Val: 0.7074, Test: 0.7022\n",
      "Epoch: 042, Loss: 0.8641, Val: 0.7085, Test: 0.7037\n",
      "Epoch: 043, Loss: 0.8510, Val: 0.7098, Test: 0.7056\n",
      "Epoch: 044, Loss: 0.8511, Val: 0.7102, Test: 0.7072\n",
      "Epoch: 045, Loss: 0.8561, Val: 0.7104, Test: 0.7068\n",
      "Epoch: 046, Loss: 0.8510, Val: 0.7080, Test: 0.7037\n",
      "Epoch: 047, Loss: 0.8447, Val: 0.7066, Test: 0.7017\n",
      "Epoch: 048, Loss: 0.8414, Val: 0.7086, Test: 0.7046\n",
      "Epoch: 049, Loss: 0.8402, Val: 0.7109, Test: 0.7075\n",
      "Epoch: 050, Loss: 0.8424, Val: 0.7119, Test: 0.7082\n",
      "Epoch: 051, Loss: 0.8531, Val: 0.7106, Test: 0.7066\n",
      "Epoch: 052, Loss: 0.8483, Val: 0.7102, Test: 0.7053\n",
      "Epoch: 053, Loss: 0.8351, Val: 0.7098, Test: 0.7060\n",
      "Epoch: 054, Loss: 0.8388, Val: 0.7122, Test: 0.7082\n",
      "Epoch: 055, Loss: 0.8416, Val: 0.7130, Test: 0.7101\n",
      "Epoch: 056, Loss: 0.8397, Val: 0.7124, Test: 0.7096\n",
      "Epoch: 057, Loss: 0.8303, Val: 0.7110, Test: 0.7078\n",
      "Epoch: 058, Loss: 0.8347, Val: 0.7105, Test: 0.7059\n",
      "Epoch: 059, Loss: 0.8327, Val: 0.7106, Test: 0.7071\n",
      "Epoch: 060, Loss: 0.8368, Val: 0.7124, Test: 0.7091\n",
      "Epoch: 061, Loss: 0.8384, Val: 0.7142, Test: 0.7115\n",
      "Epoch: 062, Loss: 0.8193, Val: 0.7142, Test: 0.7117\n",
      "Epoch: 063, Loss: 0.8301, Val: 0.7118, Test: 0.7091\n",
      "Epoch: 064, Loss: 0.8233, Val: 0.7107, Test: 0.7063\n",
      "Epoch: 065, Loss: 0.8192, Val: 0.7113, Test: 0.7079\n",
      "Epoch: 066, Loss: 0.8299, Val: 0.7132, Test: 0.7103\n",
      "Epoch: 067, Loss: 0.8225, Val: 0.7151, Test: 0.7116\n",
      "Epoch: 068, Loss: 0.8220, Val: 0.7140, Test: 0.7108\n",
      "Epoch: 069, Loss: 0.8248, Val: 0.7131, Test: 0.7083\n",
      "Epoch: 070, Loss: 0.8260, Val: 0.7129, Test: 0.7078\n",
      "Epoch: 071, Loss: 0.8243, Val: 0.7127, Test: 0.7082\n",
      "Epoch: 072, Loss: 0.8312, Val: 0.7132, Test: 0.7090\n",
      "Epoch: 073, Loss: 0.8210, Val: 0.7142, Test: 0.7093\n",
      "Epoch: 074, Loss: 0.8269, Val: 0.7130, Test: 0.7082\n",
      "Epoch: 075, Loss: 0.8318, Val: 0.7133, Test: 0.7077\n",
      "Epoch: 076, Loss: 0.8247, Val: 0.7134, Test: 0.7081\n",
      "Epoch: 077, Loss: 0.8138, Val: 0.7141, Test: 0.7101\n",
      "Epoch: 078, Loss: 0.8229, Val: 0.7154, Test: 0.7107\n",
      "Epoch: 079, Loss: 0.8149, Val: 0.7145, Test: 0.7089\n",
      "Epoch: 080, Loss: 0.8251, Val: 0.7147, Test: 0.7086\n",
      "Epoch: 081, Loss: 0.8179, Val: 0.7148, Test: 0.7102\n",
      "Epoch: 082, Loss: 0.8212, Val: 0.7163, Test: 0.7103\n",
      "Epoch: 083, Loss: 0.8247, Val: 0.7155, Test: 0.7105\n",
      "Epoch: 084, Loss: 0.8147, Val: 0.7161, Test: 0.7120\n",
      "Epoch: 085, Loss: 0.8202, Val: 0.7165, Test: 0.7119\n",
      "Epoch: 086, Loss: 0.8166, Val: 0.7157, Test: 0.7089\n",
      "Epoch: 087, Loss: 0.8078, Val: 0.7132, Test: 0.7076\n",
      "Epoch: 088, Loss: 0.8113, Val: 0.7155, Test: 0.7092\n",
      "Epoch: 089, Loss: 0.8193, Val: 0.7151, Test: 0.7119\n",
      "Epoch: 090, Loss: 0.8143, Val: 0.7166, Test: 0.7138\n",
      "Epoch: 091, Loss: 0.8183, Val: 0.7164, Test: 0.7137\n",
      "Epoch: 092, Loss: 0.8173, Val: 0.7161, Test: 0.7108\n",
      "Epoch: 093, Loss: 0.8144, Val: 0.7159, Test: 0.7097\n",
      "Epoch: 094, Loss: 0.8127, Val: 0.7156, Test: 0.7086\n",
      "Epoch: 095, Loss: 0.8210, Val: 0.7156, Test: 0.7089\n",
      "Epoch: 096, Loss: 0.8061, Val: 0.7150, Test: 0.7094\n",
      "Epoch: 097, Loss: 0.8100, Val: 0.7159, Test: 0.7101\n",
      "Epoch: 098, Loss: 0.8103, Val: 0.7167, Test: 0.7094\n",
      "Epoch: 099, Loss: 0.8139, Val: 0.7167, Test: 0.7089\n",
      "Epoch: 100, Loss: 0.8103, Val: 0.7181, Test: 0.7115\n",
      "Epoch: 101, Loss: 0.8006, Val: 0.7193, Test: 0.7140\n",
      "Epoch: 102, Loss: 0.8029, Val: 0.7191, Test: 0.7137\n",
      "Epoch: 103, Loss: 0.8085, Val: 0.7163, Test: 0.7093\n",
      "Epoch: 104, Loss: 0.7963, Val: 0.7130, Test: 0.7051\n",
      "Epoch: 105, Loss: 0.8069, Val: 0.7128, Test: 0.7052\n",
      "Epoch: 106, Loss: 0.8051, Val: 0.7166, Test: 0.7101\n",
      "Epoch: 107, Loss: 0.8086, Val: 0.7185, Test: 0.7147\n",
      "Epoch: 108, Loss: 0.8078, Val: 0.7193, Test: 0.7156\n",
      "Epoch: 109, Loss: 0.8046, Val: 0.7170, Test: 0.7127\n",
      "Epoch: 110, Loss: 0.8121, Val: 0.7157, Test: 0.7092\n",
      "Epoch: 111, Loss: 0.8110, Val: 0.7160, Test: 0.7095\n",
      "Epoch: 112, Loss: 0.7993, Val: 0.7182, Test: 0.7134\n",
      "Epoch: 113, Loss: 0.8051, Val: 0.7189, Test: 0.7150\n",
      "Epoch: 114, Loss: 0.8042, Val: 0.7180, Test: 0.7131\n",
      "Epoch: 115, Loss: 0.8028, Val: 0.7186, Test: 0.7117\n",
      "Epoch: 116, Loss: 0.7973, Val: 0.7186, Test: 0.7120\n",
      "Epoch: 117, Loss: 0.7969, Val: 0.7191, Test: 0.7145\n",
      "Epoch: 118, Loss: 0.8118, Val: 0.7188, Test: 0.7129\n",
      "Epoch: 119, Loss: 0.7955, Val: 0.7177, Test: 0.7109\n",
      "Epoch: 120, Loss: 0.8006, Val: 0.7172, Test: 0.7100\n",
      "Epoch: 121, Loss: 0.7951, Val: 0.7183, Test: 0.7110\n",
      "Epoch: 122, Loss: 0.7976, Val: 0.7191, Test: 0.7132\n",
      "Epoch: 123, Loss: 0.8074, Val: 0.7194, Test: 0.7132\n",
      "Epoch: 124, Loss: 0.8018, Val: 0.7167, Test: 0.7100\n",
      "Epoch: 125, Loss: 0.8058, Val: 0.7163, Test: 0.7096\n",
      "Epoch: 126, Loss: 0.8015, Val: 0.7171, Test: 0.7123\n",
      "Epoch: 127, Loss: 0.7953, Val: 0.7195, Test: 0.7138\n",
      "Epoch: 128, Loss: 0.7990, Val: 0.7190, Test: 0.7144\n",
      "Epoch: 129, Loss: 0.7967, Val: 0.7173, Test: 0.7129\n",
      "Epoch: 130, Loss: 0.7864, Val: 0.7174, Test: 0.7129\n",
      "Epoch: 131, Loss: 0.7935, Val: 0.7180, Test: 0.7135\n",
      "Epoch: 132, Loss: 0.7940, Val: 0.7187, Test: 0.7137\n",
      "Epoch: 133, Loss: 0.8007, Val: 0.7183, Test: 0.7125\n",
      "Epoch: 134, Loss: 0.7987, Val: 0.7175, Test: 0.7112\n",
      "Epoch: 135, Loss: 0.8021, Val: 0.7176, Test: 0.7102\n",
      "Epoch: 136, Loss: 0.8068, Val: 0.7180, Test: 0.7108\n",
      "Epoch: 137, Loss: 0.8027, Val: 0.7192, Test: 0.7125\n",
      "Epoch: 138, Loss: 0.7910, Val: 0.7208, Test: 0.7142\n",
      "Epoch: 139, Loss: 0.7892, Val: 0.7194, Test: 0.7129\n",
      "Epoch: 140, Loss: 0.7975, Val: 0.7182, Test: 0.7125\n",
      "Epoch: 141, Loss: 0.8005, Val: 0.7175, Test: 0.7124\n",
      "Epoch: 142, Loss: 0.7923, Val: 0.7180, Test: 0.7130\n",
      "Epoch: 143, Loss: 0.7859, Val: 0.7192, Test: 0.7143\n",
      "Epoch: 144, Loss: 0.7898, Val: 0.7200, Test: 0.7152\n",
      "Epoch: 145, Loss: 0.7943, Val: 0.7205, Test: 0.7150\n",
      "Epoch: 146, Loss: 0.7872, Val: 0.7194, Test: 0.7132\n",
      "Epoch: 147, Loss: 0.8004, Val: 0.7173, Test: 0.7111\n",
      "Epoch: 148, Loss: 0.7971, Val: 0.7167, Test: 0.7102\n",
      "Epoch: 149, Loss: 0.7919, Val: 0.7187, Test: 0.7123\n",
      "Epoch: 150, Loss: 0.7976, Val: 0.7203, Test: 0.7138\n",
      "Epoch: 151, Loss: 0.7876, Val: 0.7201, Test: 0.7140\n",
      "Epoch: 152, Loss: 0.7918, Val: 0.7197, Test: 0.7122\n",
      "Epoch: 153, Loss: 0.7972, Val: 0.7172, Test: 0.7088\n",
      "Epoch: 154, Loss: 0.7966, Val: 0.7160, Test: 0.7071\n",
      "Epoch: 155, Loss: 0.7966, Val: 0.7178, Test: 0.7106\n",
      "Epoch: 156, Loss: 0.7892, Val: 0.7198, Test: 0.7149\n",
      "Epoch: 157, Loss: 0.7881, Val: 0.7202, Test: 0.7154\n",
      "Epoch: 158, Loss: 0.7917, Val: 0.7185, Test: 0.7140\n",
      "Epoch: 159, Loss: 0.7921, Val: 0.7179, Test: 0.7114\n",
      "Epoch: 160, Loss: 0.7919, Val: 0.7184, Test: 0.7121\n",
      "Epoch: 161, Loss: 0.7954, Val: 0.7204, Test: 0.7156\n",
      "Epoch: 162, Loss: 0.7806, Val: 0.7193, Test: 0.7151\n",
      "Epoch: 163, Loss: 0.7921, Val: 0.7175, Test: 0.7121\n",
      "Epoch: 164, Loss: 0.7917, Val: 0.7177, Test: 0.7113\n",
      "Epoch: 165, Loss: 0.7950, Val: 0.7175, Test: 0.7126\n",
      "Epoch: 166, Loss: 0.7996, Val: 0.7187, Test: 0.7125\n",
      "Epoch: 167, Loss: 0.7818, Val: 0.7187, Test: 0.7121\n",
      "Epoch: 168, Loss: 0.7936, Val: 0.7194, Test: 0.7125\n",
      "Epoch: 169, Loss: 0.7912, Val: 0.7201, Test: 0.7144\n",
      "Epoch: 170, Loss: 0.7937, Val: 0.7200, Test: 0.7147\n",
      "Epoch: 171, Loss: 0.7872, Val: 0.7187, Test: 0.7116\n",
      "Epoch: 172, Loss: 0.7877, Val: 0.7160, Test: 0.7067\n",
      "Epoch: 173, Loss: 0.7854, Val: 0.7167, Test: 0.7077\n",
      "Epoch: 174, Loss: 0.7860, Val: 0.7206, Test: 0.7135\n",
      "Epoch: 175, Loss: 0.7837, Val: 0.7225, Test: 0.7165\n",
      "Epoch: 176, Loss: 0.7826, Val: 0.7217, Test: 0.7153\n",
      "Epoch: 177, Loss: 0.7864, Val: 0.7175, Test: 0.7096\n",
      "Epoch: 178, Loss: 0.7828, Val: 0.7157, Test: 0.7057\n",
      "Epoch: 179, Loss: 0.7870, Val: 0.7191, Test: 0.7110\n",
      "Epoch: 180, Loss: 0.7860, Val: 0.7219, Test: 0.7157\n",
      "Epoch: 181, Loss: 0.7860, Val: 0.7210, Test: 0.7144\n",
      "Epoch: 182, Loss: 0.7931, Val: 0.7180, Test: 0.7107\n",
      "Epoch: 183, Loss: 0.7768, Val: 0.7183, Test: 0.7103\n",
      "Epoch: 184, Loss: 0.7824, Val: 0.7204, Test: 0.7139\n",
      "Epoch: 185, Loss: 0.7863, Val: 0.7221, Test: 0.7171\n",
      "Epoch: 186, Loss: 0.7891, Val: 0.7223, Test: 0.7167\n",
      "Epoch: 187, Loss: 0.7949, Val: 0.7199, Test: 0.7139\n",
      "Epoch: 188, Loss: 0.7854, Val: 0.7179, Test: 0.7109\n",
      "Epoch: 189, Loss: 0.7902, Val: 0.7183, Test: 0.7116\n",
      "Epoch: 190, Loss: 0.7887, Val: 0.7199, Test: 0.7141\n",
      "Epoch: 191, Loss: 0.7825, Val: 0.7221, Test: 0.7162\n",
      "Epoch: 192, Loss: 0.7840, Val: 0.7221, Test: 0.7167\n",
      "Epoch: 193, Loss: 0.7892, Val: 0.7198, Test: 0.7122\n",
      "Epoch: 194, Loss: 0.7902, Val: 0.7168, Test: 0.7065\n",
      "Epoch: 195, Loss: 0.7918, Val: 0.7185, Test: 0.7087\n",
      "Epoch: 196, Loss: 0.7782, Val: 0.7209, Test: 0.7144\n",
      "Epoch: 197, Loss: 0.7830, Val: 0.7220, Test: 0.7174\n",
      "Epoch: 198, Loss: 0.7749, Val: 0.7210, Test: 0.7151\n",
      "Epoch: 199, Loss: 0.7785, Val: 0.7177, Test: 0.7086\n",
      "Epoch: 200, Loss: 0.7798, Val: 0.7173, Test: 0.7075\n",
      "Epoch: 201, Loss: 0.7830, Val: 0.7211, Test: 0.7138\n",
      "Epoch: 202, Loss: 0.7901, Val: 0.7217, Test: 0.7168\n",
      "Epoch: 203, Loss: 0.7708, Val: 0.7203, Test: 0.7150\n",
      "Epoch: 204, Loss: 0.7745, Val: 0.7208, Test: 0.7145\n",
      "Epoch: 205, Loss: 0.7829, Val: 0.7201, Test: 0.7136\n",
      "Epoch: 206, Loss: 0.7771, Val: 0.7185, Test: 0.7118\n",
      "Epoch: 207, Loss: 0.7848, Val: 0.7183, Test: 0.7114\n",
      "Epoch: 208, Loss: 0.7861, Val: 0.7204, Test: 0.7135\n",
      "Epoch: 209, Loss: 0.7717, Val: 0.7212, Test: 0.7167\n",
      "Epoch: 210, Loss: 0.7736, Val: 0.7210, Test: 0.7160\n",
      "Epoch: 211, Loss: 0.7726, Val: 0.7206, Test: 0.7158\n",
      "Epoch: 212, Loss: 0.7801, Val: 0.7197, Test: 0.7142\n",
      "Epoch: 213, Loss: 0.7704, Val: 0.7201, Test: 0.7134\n",
      "Epoch: 214, Loss: 0.7766, Val: 0.7203, Test: 0.7137\n",
      "Epoch: 215, Loss: 0.7761, Val: 0.7217, Test: 0.7153\n",
      "Epoch: 216, Loss: 0.7770, Val: 0.7216, Test: 0.7162\n",
      "Epoch: 217, Loss: 0.7762, Val: 0.7197, Test: 0.7131\n",
      "Epoch: 218, Loss: 0.7800, Val: 0.7185, Test: 0.7106\n",
      "Epoch: 219, Loss: 0.7865, Val: 0.7182, Test: 0.7094\n",
      "Epoch: 220, Loss: 0.7776, Val: 0.7187, Test: 0.7123\n",
      "Epoch: 221, Loss: 0.7756, Val: 0.7216, Test: 0.7150\n",
      "Epoch: 222, Loss: 0.7735, Val: 0.7212, Test: 0.7152\n",
      "Epoch: 223, Loss: 0.7734, Val: 0.7195, Test: 0.7117\n",
      "Epoch: 224, Loss: 0.7812, Val: 0.7192, Test: 0.7121\n",
      "Epoch: 225, Loss: 0.7879, Val: 0.7201, Test: 0.7138\n",
      "Epoch: 226, Loss: 0.7788, Val: 0.7208, Test: 0.7139\n",
      "Epoch: 227, Loss: 0.7809, Val: 0.7207, Test: 0.7151\n",
      "Epoch: 228, Loss: 0.7676, Val: 0.7217, Test: 0.7159\n",
      "Epoch: 229, Loss: 0.7785, Val: 0.7223, Test: 0.7161\n",
      "Epoch: 230, Loss: 0.7796, Val: 0.7207, Test: 0.7136\n",
      "Epoch: 231, Loss: 0.7713, Val: 0.7203, Test: 0.7133\n",
      "Epoch: 232, Loss: 0.7761, Val: 0.7206, Test: 0.7139\n",
      "Epoch: 233, Loss: 0.7795, Val: 0.7204, Test: 0.7140\n",
      "Epoch: 234, Loss: 0.7740, Val: 0.7206, Test: 0.7151\n",
      "Epoch: 235, Loss: 0.7723, Val: 0.7208, Test: 0.7147\n",
      "Epoch: 236, Loss: 0.7836, Val: 0.7209, Test: 0.7130\n",
      "Epoch: 237, Loss: 0.7706, Val: 0.7195, Test: 0.7116\n",
      "Epoch: 238, Loss: 0.7748, Val: 0.7219, Test: 0.7155\n",
      "Epoch: 239, Loss: 0.7749, Val: 0.7214, Test: 0.7160\n",
      "Epoch: 240, Loss: 0.7692, Val: 0.7201, Test: 0.7145\n",
      "Epoch: 241, Loss: 0.7908, Val: 0.7183, Test: 0.7103\n",
      "Epoch: 242, Loss: 0.7766, Val: 0.7188, Test: 0.7101\n",
      "Epoch: 243, Loss: 0.7874, Val: 0.7212, Test: 0.7145\n",
      "Epoch: 244, Loss: 0.7705, Val: 0.7229, Test: 0.7174\n",
      "Epoch: 245, Loss: 0.7735, Val: 0.7224, Test: 0.7167\n",
      "Epoch: 246, Loss: 0.7676, Val: 0.7185, Test: 0.7116\n",
      "Epoch: 247, Loss: 0.7743, Val: 0.7189, Test: 0.7111\n",
      "Epoch: 248, Loss: 0.7793, Val: 0.7220, Test: 0.7157\n",
      "Epoch: 249, Loss: 0.7737, Val: 0.7226, Test: 0.7187\n",
      "Epoch: 250, Loss: 0.7789, Val: 0.7219, Test: 0.7169\n",
      "Epoch: 251, Loss: 0.7723, Val: 0.7186, Test: 0.7097\n",
      "Epoch: 252, Loss: 0.7719, Val: 0.7183, Test: 0.7102\n",
      "Epoch: 253, Loss: 0.7733, Val: 0.7204, Test: 0.7132\n",
      "Epoch: 254, Loss: 0.7722, Val: 0.7230, Test: 0.7173\n",
      "Epoch: 255, Loss: 0.7614, Val: 0.7232, Test: 0.7174\n",
      "Epoch: 256, Loss: 0.7754, Val: 0.7217, Test: 0.7148\n",
      "Epoch: 257, Loss: 0.7808, Val: 0.7210, Test: 0.7141\n",
      "Epoch: 258, Loss: 0.7808, Val: 0.7208, Test: 0.7137\n",
      "Epoch: 259, Loss: 0.7735, Val: 0.7192, Test: 0.7135\n",
      "Epoch: 260, Loss: 0.7770, Val: 0.7193, Test: 0.7125\n",
      "Epoch: 261, Loss: 0.7741, Val: 0.7210, Test: 0.7147\n",
      "Epoch: 262, Loss: 0.7812, Val: 0.7210, Test: 0.7144\n",
      "Epoch: 263, Loss: 0.7661, Val: 0.7217, Test: 0.7149\n",
      "Epoch: 264, Loss: 0.7755, Val: 0.7223, Test: 0.7151\n",
      "Epoch: 265, Loss: 0.7729, Val: 0.7202, Test: 0.7123\n",
      "Epoch: 266, Loss: 0.7719, Val: 0.7194, Test: 0.7103\n",
      "Epoch: 267, Loss: 0.7682, Val: 0.7208, Test: 0.7116\n",
      "Epoch: 268, Loss: 0.7633, Val: 0.7232, Test: 0.7150\n",
      "Epoch: 269, Loss: 0.7632, Val: 0.7224, Test: 0.7155\n",
      "Epoch: 270, Loss: 0.7723, Val: 0.7199, Test: 0.7114\n",
      "Epoch: 271, Loss: 0.7648, Val: 0.7197, Test: 0.7114\n",
      "Epoch: 272, Loss: 0.7753, Val: 0.7215, Test: 0.7144\n",
      "Epoch: 273, Loss: 0.7675, Val: 0.7232, Test: 0.7169\n",
      "Epoch: 274, Loss: 0.7694, Val: 0.7228, Test: 0.7152\n",
      "Epoch: 275, Loss: 0.7666, Val: 0.7211, Test: 0.7132\n",
      "Epoch: 276, Loss: 0.7707, Val: 0.7217, Test: 0.7149\n",
      "Epoch: 277, Loss: 0.7606, Val: 0.7221, Test: 0.7163\n",
      "Epoch: 278, Loss: 0.7680, Val: 0.7216, Test: 0.7154\n",
      "Epoch: 279, Loss: 0.7661, Val: 0.7199, Test: 0.7132\n",
      "Epoch: 280, Loss: 0.7671, Val: 0.7200, Test: 0.7120\n",
      "Epoch: 281, Loss: 0.7613, Val: 0.7218, Test: 0.7154\n",
      "Epoch: 282, Loss: 0.7643, Val: 0.7225, Test: 0.7166\n",
      "Epoch: 283, Loss: 0.7651, Val: 0.7228, Test: 0.7167\n",
      "Epoch: 284, Loss: 0.7645, Val: 0.7217, Test: 0.7131\n",
      "Epoch: 285, Loss: 0.7685, Val: 0.7181, Test: 0.7092\n",
      "Epoch: 286, Loss: 0.7654, Val: 0.7193, Test: 0.7103\n",
      "Epoch: 287, Loss: 0.7660, Val: 0.7210, Test: 0.7141\n",
      "Epoch: 288, Loss: 0.7700, Val: 0.7210, Test: 0.7150\n",
      "Epoch: 289, Loss: 0.7597, Val: 0.7212, Test: 0.7149\n",
      "Epoch: 290, Loss: 0.7744, Val: 0.7220, Test: 0.7139\n",
      "Epoch: 291, Loss: 0.7650, Val: 0.7218, Test: 0.7136\n",
      "Epoch: 292, Loss: 0.7729, Val: 0.7226, Test: 0.7143\n",
      "Epoch: 293, Loss: 0.7713, Val: 0.7215, Test: 0.7151\n",
      "Epoch: 294, Loss: 0.7767, Val: 0.7215, Test: 0.7132\n",
      "Epoch: 295, Loss: 0.7569, Val: 0.7215, Test: 0.7140\n",
      "Epoch: 296, Loss: 0.7724, Val: 0.7228, Test: 0.7158\n",
      "Epoch: 297, Loss: 0.7657, Val: 0.7221, Test: 0.7149\n",
      "Epoch: 298, Loss: 0.7622, Val: 0.7221, Test: 0.7140\n",
      "Epoch: 299, Loss: 0.7627, Val: 0.7232, Test: 0.7157\n",
      "Epoch: 300, Loss: 0.7667, Val: 0.7237, Test: 0.7185\n",
      "0.7000000000000001\n",
      "Label_rate: 0.7000000000000001\n",
      "Epoch: 001, Loss: 2.5904, Val: 0.4661, Test: 0.4035\n",
      "Epoch: 002, Loss: 1.9872, Val: 0.5482, Test: 0.5271\n",
      "Epoch: 003, Loss: 1.6047, Val: 0.5632, Test: 0.5553\n",
      "Epoch: 004, Loss: 1.4506, Val: 0.5902, Test: 0.5849\n",
      "Epoch: 005, Loss: 1.3120, Val: 0.6305, Test: 0.6248\n",
      "Epoch: 006, Loss: 1.2182, Val: 0.6398, Test: 0.6439\n",
      "Epoch: 007, Loss: 1.1926, Val: 0.6588, Test: 0.6674\n",
      "Epoch: 008, Loss: 1.1397, Val: 0.6634, Test: 0.6745\n",
      "Epoch: 009, Loss: 1.1432, Val: 0.6759, Test: 0.6825\n",
      "Epoch: 010, Loss: 1.0753, Val: 0.6775, Test: 0.6791\n",
      "Epoch: 011, Loss: 1.0589, Val: 0.6780, Test: 0.6786\n",
      "Epoch: 012, Loss: 1.0677, Val: 0.6804, Test: 0.6827\n",
      "Epoch: 013, Loss: 1.0581, Val: 0.6828, Test: 0.6861\n",
      "Epoch: 014, Loss: 1.0226, Val: 0.6848, Test: 0.6860\n",
      "Epoch: 015, Loss: 1.0203, Val: 0.6852, Test: 0.6853\n",
      "Epoch: 016, Loss: 0.9970, Val: 0.6859, Test: 0.6878\n",
      "Epoch: 017, Loss: 0.9945, Val: 0.6871, Test: 0.6906\n",
      "Epoch: 018, Loss: 0.9798, Val: 0.6862, Test: 0.6896\n",
      "Epoch: 019, Loss: 0.9787, Val: 0.6869, Test: 0.6879\n",
      "Epoch: 020, Loss: 0.9800, Val: 0.6873, Test: 0.6881\n",
      "Epoch: 021, Loss: 0.9746, Val: 0.6894, Test: 0.6888\n",
      "Epoch: 022, Loss: 0.9400, Val: 0.6888, Test: 0.6905\n",
      "Epoch: 023, Loss: 0.9548, Val: 0.6903, Test: 0.6926\n",
      "Epoch: 024, Loss: 0.9320, Val: 0.6939, Test: 0.6960\n",
      "Epoch: 025, Loss: 0.9438, Val: 0.6975, Test: 0.7001\n",
      "Epoch: 026, Loss: 0.9449, Val: 0.6979, Test: 0.7008\n",
      "Epoch: 027, Loss: 0.9318, Val: 0.6968, Test: 0.6982\n",
      "Epoch: 028, Loss: 0.9297, Val: 0.6957, Test: 0.6949\n",
      "Epoch: 029, Loss: 0.9169, Val: 0.6950, Test: 0.6952\n",
      "Epoch: 030, Loss: 0.9060, Val: 0.6977, Test: 0.6985\n",
      "Epoch: 031, Loss: 0.9189, Val: 0.7003, Test: 0.7019\n",
      "Epoch: 032, Loss: 0.9050, Val: 0.6999, Test: 0.7009\n",
      "Epoch: 033, Loss: 0.9102, Val: 0.6977, Test: 0.6973\n",
      "Epoch: 034, Loss: 0.9063, Val: 0.6956, Test: 0.6952\n",
      "Epoch: 035, Loss: 0.8989, Val: 0.6953, Test: 0.6949\n",
      "Epoch: 036, Loss: 0.9086, Val: 0.6968, Test: 0.6953\n",
      "Epoch: 037, Loss: 0.9015, Val: 0.6984, Test: 0.6964\n",
      "Epoch: 038, Loss: 0.9007, Val: 0.6996, Test: 0.6984\n",
      "Epoch: 039, Loss: 0.8896, Val: 0.7014, Test: 0.7002\n",
      "Epoch: 040, Loss: 0.9003, Val: 0.7015, Test: 0.7004\n",
      "Epoch: 041, Loss: 0.8794, Val: 0.7013, Test: 0.6996\n",
      "Epoch: 042, Loss: 0.8933, Val: 0.7001, Test: 0.6993\n",
      "Epoch: 043, Loss: 0.8729, Val: 0.7012, Test: 0.7000\n",
      "Epoch: 044, Loss: 0.8769, Val: 0.7026, Test: 0.7022\n",
      "Epoch: 045, Loss: 0.8807, Val: 0.7041, Test: 0.7036\n",
      "Epoch: 046, Loss: 0.8715, Val: 0.7053, Test: 0.7048\n",
      "Epoch: 047, Loss: 0.8701, Val: 0.7047, Test: 0.7036\n",
      "Epoch: 048, Loss: 0.8739, Val: 0.7048, Test: 0.7037\n",
      "Epoch: 049, Loss: 0.8602, Val: 0.7051, Test: 0.7048\n",
      "Epoch: 050, Loss: 0.8674, Val: 0.7055, Test: 0.7051\n",
      "Epoch: 051, Loss: 0.8694, Val: 0.7055, Test: 0.7057\n",
      "Epoch: 052, Loss: 0.8614, Val: 0.7057, Test: 0.7044\n",
      "Epoch: 053, Loss: 0.8711, Val: 0.7034, Test: 0.7025\n",
      "Epoch: 054, Loss: 0.8572, Val: 0.7033, Test: 0.7011\n",
      "Epoch: 055, Loss: 0.8502, Val: 0.7038, Test: 0.7024\n",
      "Epoch: 056, Loss: 0.8601, Val: 0.7060, Test: 0.7047\n",
      "Epoch: 057, Loss: 0.8515, Val: 0.7066, Test: 0.7050\n",
      "Epoch: 058, Loss: 0.8477, Val: 0.7068, Test: 0.7061\n",
      "Epoch: 059, Loss: 0.8430, Val: 0.7068, Test: 0.7058\n",
      "Epoch: 060, Loss: 0.8479, Val: 0.7057, Test: 0.7051\n",
      "Epoch: 061, Loss: 0.8496, Val: 0.7061, Test: 0.7054\n",
      "Epoch: 062, Loss: 0.8414, Val: 0.7071, Test: 0.7074\n",
      "Epoch: 063, Loss: 0.8670, Val: 0.7074, Test: 0.7078\n",
      "Epoch: 064, Loss: 0.8447, Val: 0.7071, Test: 0.7073\n",
      "Epoch: 065, Loss: 0.8435, Val: 0.7059, Test: 0.7040\n",
      "Epoch: 066, Loss: 0.8354, Val: 0.7062, Test: 0.7037\n",
      "Epoch: 067, Loss: 0.8496, Val: 0.7066, Test: 0.7045\n",
      "Epoch: 068, Loss: 0.8349, Val: 0.7079, Test: 0.7060\n",
      "Epoch: 069, Loss: 0.8384, Val: 0.7072, Test: 0.7055\n",
      "Epoch: 070, Loss: 0.8464, Val: 0.7065, Test: 0.7043\n",
      "Epoch: 071, Loss: 0.8445, Val: 0.7074, Test: 0.7041\n",
      "Epoch: 072, Loss: 0.8310, Val: 0.7078, Test: 0.7052\n",
      "Epoch: 073, Loss: 0.8401, Val: 0.7085, Test: 0.7059\n",
      "Epoch: 074, Loss: 0.8477, Val: 0.7075, Test: 0.7055\n",
      "Epoch: 075, Loss: 0.8320, Val: 0.7081, Test: 0.7050\n",
      "Epoch: 076, Loss: 0.8263, Val: 0.7079, Test: 0.7048\n",
      "Epoch: 077, Loss: 0.8295, Val: 0.7095, Test: 0.7070\n",
      "Epoch: 078, Loss: 0.8284, Val: 0.7108, Test: 0.7085\n",
      "Epoch: 079, Loss: 0.8427, Val: 0.7107, Test: 0.7086\n",
      "Epoch: 080, Loss: 0.8457, Val: 0.7102, Test: 0.7074\n",
      "Epoch: 081, Loss: 0.8287, Val: 0.7096, Test: 0.7066\n",
      "Epoch: 082, Loss: 0.8274, Val: 0.7082, Test: 0.7053\n",
      "Epoch: 083, Loss: 0.8319, Val: 0.7088, Test: 0.7063\n",
      "Epoch: 084, Loss: 0.8221, Val: 0.7106, Test: 0.7084\n",
      "Epoch: 085, Loss: 0.8326, Val: 0.7110, Test: 0.7100\n",
      "Epoch: 086, Loss: 0.8250, Val: 0.7112, Test: 0.7101\n",
      "Epoch: 087, Loss: 0.8274, Val: 0.7099, Test: 0.7070\n",
      "Epoch: 088, Loss: 0.8284, Val: 0.7079, Test: 0.7057\n",
      "Epoch: 089, Loss: 0.8171, Val: 0.7101, Test: 0.7079\n",
      "Epoch: 090, Loss: 0.8214, Val: 0.7121, Test: 0.7097\n",
      "Epoch: 091, Loss: 0.8202, Val: 0.7128, Test: 0.7119\n",
      "Epoch: 092, Loss: 0.8175, Val: 0.7135, Test: 0.7125\n",
      "Epoch: 093, Loss: 0.8275, Val: 0.7122, Test: 0.7093\n",
      "Epoch: 094, Loss: 0.8225, Val: 0.7097, Test: 0.7065\n",
      "Epoch: 095, Loss: 0.8313, Val: 0.7104, Test: 0.7077\n",
      "Epoch: 096, Loss: 0.8236, Val: 0.7131, Test: 0.7091\n",
      "Epoch: 097, Loss: 0.8170, Val: 0.7135, Test: 0.7108\n",
      "Epoch: 098, Loss: 0.8226, Val: 0.7124, Test: 0.7103\n",
      "Epoch: 099, Loss: 0.8220, Val: 0.7123, Test: 0.7094\n",
      "Epoch: 100, Loss: 0.8227, Val: 0.7122, Test: 0.7099\n",
      "Epoch: 101, Loss: 0.8131, Val: 0.7129, Test: 0.7102\n",
      "Epoch: 102, Loss: 0.8232, Val: 0.7127, Test: 0.7106\n",
      "Epoch: 103, Loss: 0.8109, Val: 0.7132, Test: 0.7116\n",
      "Epoch: 104, Loss: 0.8128, Val: 0.7135, Test: 0.7103\n",
      "Epoch: 105, Loss: 0.8162, Val: 0.7142, Test: 0.7106\n",
      "Epoch: 106, Loss: 0.8200, Val: 0.7143, Test: 0.7107\n",
      "Epoch: 107, Loss: 0.8220, Val: 0.7141, Test: 0.7109\n",
      "Epoch: 108, Loss: 0.8108, Val: 0.7135, Test: 0.7114\n",
      "Epoch: 109, Loss: 0.8128, Val: 0.7134, Test: 0.7109\n",
      "Epoch: 110, Loss: 0.8158, Val: 0.7141, Test: 0.7106\n",
      "Epoch: 111, Loss: 0.8318, Val: 0.7132, Test: 0.7105\n",
      "Epoch: 112, Loss: 0.7972, Val: 0.7139, Test: 0.7114\n",
      "Epoch: 113, Loss: 0.8144, Val: 0.7145, Test: 0.7126\n",
      "Epoch: 114, Loss: 0.7983, Val: 0.7135, Test: 0.7128\n",
      "Epoch: 115, Loss: 0.8066, Val: 0.7132, Test: 0.7120\n",
      "Epoch: 116, Loss: 0.8124, Val: 0.7132, Test: 0.7111\n",
      "Epoch: 117, Loss: 0.8219, Val: 0.7155, Test: 0.7126\n",
      "Epoch: 118, Loss: 0.8028, Val: 0.7161, Test: 0.7132\n",
      "Epoch: 119, Loss: 0.7953, Val: 0.7161, Test: 0.7128\n",
      "Epoch: 120, Loss: 0.8028, Val: 0.7131, Test: 0.7098\n",
      "Epoch: 121, Loss: 0.8062, Val: 0.7121, Test: 0.7081\n",
      "Epoch: 122, Loss: 0.8107, Val: 0.7126, Test: 0.7094\n",
      "Epoch: 123, Loss: 0.8119, Val: 0.7147, Test: 0.7125\n",
      "Epoch: 124, Loss: 0.8146, Val: 0.7160, Test: 0.7144\n",
      "Epoch: 125, Loss: 0.8086, Val: 0.7164, Test: 0.7148\n",
      "Epoch: 126, Loss: 0.8037, Val: 0.7150, Test: 0.7129\n",
      "Epoch: 127, Loss: 0.8026, Val: 0.7134, Test: 0.7093\n",
      "Epoch: 128, Loss: 0.8040, Val: 0.7138, Test: 0.7096\n",
      "Epoch: 129, Loss: 0.8074, Val: 0.7152, Test: 0.7127\n",
      "Epoch: 130, Loss: 0.8075, Val: 0.7163, Test: 0.7151\n",
      "Epoch: 131, Loss: 0.8066, Val: 0.7165, Test: 0.7144\n",
      "Epoch: 132, Loss: 0.8126, Val: 0.7151, Test: 0.7128\n",
      "Epoch: 133, Loss: 0.8004, Val: 0.7158, Test: 0.7117\n",
      "Epoch: 134, Loss: 0.8077, Val: 0.7157, Test: 0.7116\n",
      "Epoch: 135, Loss: 0.8152, Val: 0.7154, Test: 0.7111\n",
      "Epoch: 136, Loss: 0.8071, Val: 0.7154, Test: 0.7109\n",
      "Epoch: 137, Loss: 0.8023, Val: 0.7160, Test: 0.7116\n",
      "Epoch: 138, Loss: 0.8064, Val: 0.7155, Test: 0.7123\n",
      "Epoch: 139, Loss: 0.7998, Val: 0.7152, Test: 0.7127\n",
      "Epoch: 140, Loss: 0.7960, Val: 0.7150, Test: 0.7111\n",
      "Epoch: 141, Loss: 0.8008, Val: 0.7143, Test: 0.7112\n",
      "Epoch: 142, Loss: 0.8052, Val: 0.7152, Test: 0.7112\n",
      "Epoch: 143, Loss: 0.7902, Val: 0.7156, Test: 0.7111\n",
      "Epoch: 144, Loss: 0.7999, Val: 0.7151, Test: 0.7104\n",
      "Epoch: 145, Loss: 0.8015, Val: 0.7147, Test: 0.7110\n",
      "Epoch: 146, Loss: 0.7838, Val: 0.7148, Test: 0.7123\n",
      "Epoch: 147, Loss: 0.8005, Val: 0.7142, Test: 0.7122\n",
      "Epoch: 148, Loss: 0.7955, Val: 0.7151, Test: 0.7126\n",
      "Epoch: 149, Loss: 0.7948, Val: 0.7164, Test: 0.7137\n",
      "Epoch: 150, Loss: 0.7974, Val: 0.7159, Test: 0.7126\n",
      "Epoch: 151, Loss: 0.7920, Val: 0.7151, Test: 0.7110\n",
      "Epoch: 152, Loss: 0.8104, Val: 0.7159, Test: 0.7113\n",
      "Epoch: 153, Loss: 0.7899, Val: 0.7152, Test: 0.7119\n",
      "Epoch: 154, Loss: 0.7893, Val: 0.7154, Test: 0.7121\n",
      "Epoch: 155, Loss: 0.7964, Val: 0.7156, Test: 0.7116\n",
      "Epoch: 156, Loss: 0.7941, Val: 0.7157, Test: 0.7121\n",
      "Epoch: 157, Loss: 0.8043, Val: 0.7157, Test: 0.7101\n",
      "Epoch: 158, Loss: 0.7864, Val: 0.7158, Test: 0.7088\n",
      "Epoch: 159, Loss: 0.8010, Val: 0.7156, Test: 0.7091\n",
      "Epoch: 160, Loss: 0.7864, Val: 0.7169, Test: 0.7127\n",
      "Epoch: 161, Loss: 0.7951, Val: 0.7187, Test: 0.7151\n",
      "Epoch: 162, Loss: 0.7948, Val: 0.7187, Test: 0.7150\n",
      "Epoch: 163, Loss: 0.7953, Val: 0.7177, Test: 0.7128\n",
      "Epoch: 164, Loss: 0.8002, Val: 0.7169, Test: 0.7124\n",
      "Epoch: 165, Loss: 0.7964, Val: 0.7155, Test: 0.7123\n",
      "Epoch: 166, Loss: 0.7980, Val: 0.7156, Test: 0.7136\n",
      "Epoch: 167, Loss: 0.7923, Val: 0.7170, Test: 0.7137\n",
      "Epoch: 168, Loss: 0.7962, Val: 0.7175, Test: 0.7124\n",
      "Epoch: 169, Loss: 0.7889, Val: 0.7179, Test: 0.7113\n",
      "Epoch: 170, Loss: 0.7957, Val: 0.7179, Test: 0.7125\n",
      "Epoch: 171, Loss: 0.7833, Val: 0.7168, Test: 0.7135\n",
      "Epoch: 172, Loss: 0.7882, Val: 0.7172, Test: 0.7137\n",
      "Epoch: 173, Loss: 0.7867, Val: 0.7163, Test: 0.7130\n",
      "Epoch: 174, Loss: 0.7933, Val: 0.7161, Test: 0.7121\n",
      "Epoch: 175, Loss: 0.7947, Val: 0.7168, Test: 0.7120\n",
      "Epoch: 176, Loss: 0.8030, Val: 0.7174, Test: 0.7128\n",
      "Epoch: 177, Loss: 0.7862, Val: 0.7168, Test: 0.7137\n",
      "Epoch: 178, Loss: 0.7918, Val: 0.7179, Test: 0.7142\n",
      "Epoch: 179, Loss: 0.7755, Val: 0.7182, Test: 0.7143\n",
      "Epoch: 180, Loss: 0.7759, Val: 0.7179, Test: 0.7145\n",
      "Epoch: 181, Loss: 0.7825, Val: 0.7183, Test: 0.7146\n",
      "Epoch: 182, Loss: 0.7889, Val: 0.7183, Test: 0.7153\n",
      "Epoch: 183, Loss: 0.7935, Val: 0.7183, Test: 0.7151\n",
      "Epoch: 184, Loss: 0.7867, Val: 0.7166, Test: 0.7148\n",
      "Epoch: 185, Loss: 0.7778, Val: 0.7170, Test: 0.7152\n",
      "Epoch: 186, Loss: 0.7833, Val: 0.7188, Test: 0.7165\n",
      "Epoch: 187, Loss: 0.7863, Val: 0.7194, Test: 0.7166\n",
      "Epoch: 188, Loss: 0.7852, Val: 0.7190, Test: 0.7137\n",
      "Epoch: 189, Loss: 0.7934, Val: 0.7182, Test: 0.7114\n",
      "Epoch: 190, Loss: 0.7926, Val: 0.7169, Test: 0.7114\n",
      "Epoch: 191, Loss: 0.7907, Val: 0.7186, Test: 0.7148\n",
      "Epoch: 192, Loss: 0.7858, Val: 0.7188, Test: 0.7168\n",
      "Epoch: 193, Loss: 0.7825, Val: 0.7201, Test: 0.7163\n",
      "Epoch: 194, Loss: 0.7879, Val: 0.7186, Test: 0.7145\n",
      "Epoch: 195, Loss: 0.7831, Val: 0.7192, Test: 0.7132\n",
      "Epoch: 196, Loss: 0.7786, Val: 0.7191, Test: 0.7134\n",
      "Epoch: 197, Loss: 0.7918, Val: 0.7177, Test: 0.7136\n",
      "Epoch: 198, Loss: 0.7796, Val: 0.7175, Test: 0.7157\n",
      "Epoch: 199, Loss: 0.7719, Val: 0.7192, Test: 0.7172\n",
      "Epoch: 200, Loss: 0.7853, Val: 0.7198, Test: 0.7168\n",
      "Epoch: 201, Loss: 0.7802, Val: 0.7187, Test: 0.7137\n",
      "Epoch: 202, Loss: 0.7788, Val: 0.7181, Test: 0.7130\n",
      "Epoch: 203, Loss: 0.7864, Val: 0.7196, Test: 0.7159\n",
      "Epoch: 204, Loss: 0.7842, Val: 0.7199, Test: 0.7175\n",
      "Epoch: 205, Loss: 0.7782, Val: 0.7191, Test: 0.7161\n",
      "Epoch: 206, Loss: 0.7876, Val: 0.7186, Test: 0.7137\n",
      "Epoch: 207, Loss: 0.7739, Val: 0.7181, Test: 0.7120\n",
      "Epoch: 208, Loss: 0.7742, Val: 0.7178, Test: 0.7121\n",
      "Epoch: 209, Loss: 0.7759, Val: 0.7190, Test: 0.7143\n",
      "Epoch: 210, Loss: 0.7733, Val: 0.7198, Test: 0.7159\n",
      "Epoch: 211, Loss: 0.7799, Val: 0.7204, Test: 0.7148\n",
      "Epoch: 212, Loss: 0.7705, Val: 0.7195, Test: 0.7145\n",
      "Epoch: 213, Loss: 0.7801, Val: 0.7205, Test: 0.7171\n",
      "Epoch: 214, Loss: 0.7911, Val: 0.7204, Test: 0.7163\n",
      "Epoch: 215, Loss: 0.7855, Val: 0.7191, Test: 0.7147\n",
      "Epoch: 216, Loss: 0.7872, Val: 0.7177, Test: 0.7112\n",
      "Epoch: 217, Loss: 0.7699, Val: 0.7173, Test: 0.7099\n",
      "Epoch: 218, Loss: 0.7860, Val: 0.7192, Test: 0.7142\n",
      "Epoch: 219, Loss: 0.7850, Val: 0.7201, Test: 0.7175\n",
      "Epoch: 220, Loss: 0.7844, Val: 0.7196, Test: 0.7153\n",
      "Epoch: 221, Loss: 0.7829, Val: 0.7188, Test: 0.7120\n",
      "Epoch: 222, Loss: 0.7674, Val: 0.7190, Test: 0.7129\n",
      "Epoch: 223, Loss: 0.7728, Val: 0.7195, Test: 0.7153\n",
      "Epoch: 224, Loss: 0.7896, Val: 0.7192, Test: 0.7161\n",
      "Epoch: 225, Loss: 0.7822, Val: 0.7187, Test: 0.7151\n",
      "Epoch: 226, Loss: 0.7752, Val: 0.7170, Test: 0.7128\n",
      "Epoch: 227, Loss: 0.7812, Val: 0.7171, Test: 0.7133\n",
      "Epoch: 228, Loss: 0.7625, Val: 0.7202, Test: 0.7169\n",
      "Epoch: 229, Loss: 0.7869, Val: 0.7222, Test: 0.7195\n",
      "Epoch: 230, Loss: 0.7783, Val: 0.7223, Test: 0.7192\n",
      "Epoch: 231, Loss: 0.7713, Val: 0.7210, Test: 0.7167\n",
      "Epoch: 232, Loss: 0.7733, Val: 0.7179, Test: 0.7121\n",
      "Epoch: 233, Loss: 0.7719, Val: 0.7178, Test: 0.7126\n",
      "Epoch: 234, Loss: 0.7693, Val: 0.7185, Test: 0.7156\n",
      "Epoch: 235, Loss: 0.7674, Val: 0.7190, Test: 0.7189\n",
      "Epoch: 236, Loss: 0.7747, Val: 0.7193, Test: 0.7177\n",
      "Epoch: 237, Loss: 0.7691, Val: 0.7198, Test: 0.7161\n",
      "Epoch: 238, Loss: 0.7749, Val: 0.7199, Test: 0.7147\n",
      "Epoch: 239, Loss: 0.7710, Val: 0.7212, Test: 0.7168\n",
      "Epoch: 240, Loss: 0.7698, Val: 0.7209, Test: 0.7178\n",
      "Epoch: 241, Loss: 0.7704, Val: 0.7210, Test: 0.7174\n",
      "Epoch: 242, Loss: 0.7755, Val: 0.7198, Test: 0.7145\n",
      "Epoch: 243, Loss: 0.7749, Val: 0.7178, Test: 0.7117\n",
      "Epoch: 244, Loss: 0.7737, Val: 0.7175, Test: 0.7134\n",
      "Epoch: 245, Loss: 0.7722, Val: 0.7195, Test: 0.7159\n",
      "Epoch: 246, Loss: 0.7777, Val: 0.7210, Test: 0.7171\n",
      "Epoch: 247, Loss: 0.7751, Val: 0.7217, Test: 0.7171\n",
      "Epoch: 248, Loss: 0.7653, Val: 0.7215, Test: 0.7161\n",
      "Epoch: 249, Loss: 0.7704, Val: 0.7203, Test: 0.7158\n",
      "Epoch: 250, Loss: 0.7675, Val: 0.7194, Test: 0.7167\n",
      "Epoch: 251, Loss: 0.7700, Val: 0.7189, Test: 0.7160\n",
      "Epoch: 252, Loss: 0.7700, Val: 0.7186, Test: 0.7155\n",
      "Epoch: 253, Loss: 0.7850, Val: 0.7201, Test: 0.7156\n",
      "Epoch: 254, Loss: 0.7834, Val: 0.7213, Test: 0.7177\n",
      "Epoch: 255, Loss: 0.7654, Val: 0.7212, Test: 0.7186\n",
      "Epoch: 256, Loss: 0.7689, Val: 0.7206, Test: 0.7169\n",
      "Epoch: 257, Loss: 0.7683, Val: 0.7194, Test: 0.7138\n",
      "Epoch: 258, Loss: 0.7744, Val: 0.7191, Test: 0.7133\n",
      "Epoch: 259, Loss: 0.7709, Val: 0.7194, Test: 0.7159\n",
      "Epoch: 260, Loss: 0.7768, Val: 0.7214, Test: 0.7193\n",
      "Epoch: 261, Loss: 0.7739, Val: 0.7217, Test: 0.7196\n",
      "Epoch: 262, Loss: 0.7830, Val: 0.7208, Test: 0.7179\n",
      "Epoch: 263, Loss: 0.7672, Val: 0.7193, Test: 0.7143\n",
      "Epoch: 264, Loss: 0.7785, Val: 0.7179, Test: 0.7131\n",
      "Epoch: 265, Loss: 0.7747, Val: 0.7198, Test: 0.7164\n",
      "Epoch: 266, Loss: 0.7712, Val: 0.7205, Test: 0.7185\n",
      "Epoch: 267, Loss: 0.7688, Val: 0.7198, Test: 0.7178\n",
      "Epoch: 268, Loss: 0.7649, Val: 0.7199, Test: 0.7162\n",
      "Epoch: 269, Loss: 0.7654, Val: 0.7195, Test: 0.7143\n",
      "Epoch: 270, Loss: 0.7758, Val: 0.7204, Test: 0.7162\n",
      "Epoch: 271, Loss: 0.7709, Val: 0.7201, Test: 0.7174\n",
      "Epoch: 272, Loss: 0.7665, Val: 0.7215, Test: 0.7191\n",
      "Epoch: 273, Loss: 0.7592, Val: 0.7209, Test: 0.7188\n",
      "Epoch: 274, Loss: 0.7752, Val: 0.7199, Test: 0.7162\n",
      "Epoch: 275, Loss: 0.7785, Val: 0.7183, Test: 0.7125\n",
      "Epoch: 276, Loss: 0.7726, Val: 0.7194, Test: 0.7142\n",
      "Epoch: 277, Loss: 0.7748, Val: 0.7213, Test: 0.7174\n",
      "Epoch: 278, Loss: 0.7581, Val: 0.7213, Test: 0.7183\n",
      "Epoch: 279, Loss: 0.7635, Val: 0.7212, Test: 0.7174\n",
      "Epoch: 280, Loss: 0.7706, Val: 0.7197, Test: 0.7166\n",
      "Epoch: 281, Loss: 0.7731, Val: 0.7202, Test: 0.7162\n",
      "Epoch: 282, Loss: 0.7702, Val: 0.7212, Test: 0.7173\n",
      "Epoch: 283, Loss: 0.7644, Val: 0.7211, Test: 0.7165\n",
      "Epoch: 284, Loss: 0.7573, Val: 0.7202, Test: 0.7155\n",
      "Epoch: 285, Loss: 0.7671, Val: 0.7206, Test: 0.7153\n",
      "Epoch: 286, Loss: 0.7669, Val: 0.7210, Test: 0.7160\n",
      "Epoch: 287, Loss: 0.7635, Val: 0.7219, Test: 0.7177\n",
      "Epoch: 288, Loss: 0.7666, Val: 0.7222, Test: 0.7179\n",
      "Epoch: 289, Loss: 0.7657, Val: 0.7203, Test: 0.7173\n",
      "Epoch: 290, Loss: 0.7566, Val: 0.7199, Test: 0.7157\n",
      "Epoch: 291, Loss: 0.7591, Val: 0.7196, Test: 0.7160\n",
      "Epoch: 292, Loss: 0.7679, Val: 0.7201, Test: 0.7168\n",
      "Epoch: 293, Loss: 0.7625, Val: 0.7209, Test: 0.7193\n",
      "Epoch: 294, Loss: 0.7661, Val: 0.7211, Test: 0.7200\n",
      "Epoch: 295, Loss: 0.7641, Val: 0.7214, Test: 0.7184\n",
      "Epoch: 296, Loss: 0.7642, Val: 0.7210, Test: 0.7179\n",
      "Epoch: 297, Loss: 0.7684, Val: 0.7205, Test: 0.7180\n",
      "Epoch: 298, Loss: 0.7623, Val: 0.7212, Test: 0.7183\n",
      "Epoch: 299, Loss: 0.7620, Val: 0.7205, Test: 0.7170\n",
      "Epoch: 300, Loss: 0.7728, Val: 0.7201, Test: 0.7150\n",
      "0.75\n",
      "Label_rate: 0.75\n",
      "Epoch: 001, Loss: 2.2082, Val: 0.5021, Test: 0.4487\n",
      "Epoch: 002, Loss: 1.7508, Val: 0.5702, Test: 0.5291\n",
      "Epoch: 003, Loss: 1.4632, Val: 0.6019, Test: 0.5934\n",
      "Epoch: 004, Loss: 1.3431, Val: 0.6248, Test: 0.6307\n",
      "Epoch: 005, Loss: 1.2578, Val: 0.6493, Test: 0.6526\n",
      "Epoch: 006, Loss: 1.1779, Val: 0.6519, Test: 0.6490\n",
      "Epoch: 007, Loss: 1.1495, Val: 0.6521, Test: 0.6482\n",
      "Epoch: 008, Loss: 1.1197, Val: 0.6588, Test: 0.6633\n",
      "Epoch: 009, Loss: 1.0737, Val: 0.6784, Test: 0.6816\n",
      "Epoch: 010, Loss: 1.0563, Val: 0.6857, Test: 0.6896\n",
      "Epoch: 011, Loss: 1.0387, Val: 0.6840, Test: 0.6862\n",
      "Epoch: 012, Loss: 1.0356, Val: 0.6810, Test: 0.6795\n",
      "Epoch: 013, Loss: 1.0263, Val: 0.6846, Test: 0.6865\n",
      "Epoch: 014, Loss: 1.0124, Val: 0.6941, Test: 0.6991\n",
      "Epoch: 015, Loss: 1.0043, Val: 0.6975, Test: 0.7017\n",
      "Epoch: 016, Loss: 0.9695, Val: 0.6967, Test: 0.6994\n",
      "Epoch: 017, Loss: 0.9873, Val: 0.6942, Test: 0.6934\n",
      "Epoch: 018, Loss: 0.9600, Val: 0.6906, Test: 0.6894\n",
      "Epoch: 019, Loss: 0.9706, Val: 0.6907, Test: 0.6900\n",
      "Epoch: 020, Loss: 0.9404, Val: 0.6952, Test: 0.6935\n",
      "Epoch: 021, Loss: 0.9468, Val: 0.6998, Test: 0.6992\n",
      "Epoch: 022, Loss: 0.9296, Val: 0.7021, Test: 0.7016\n",
      "Epoch: 023, Loss: 0.9448, Val: 0.7011, Test: 0.7027\n",
      "Epoch: 024, Loss: 0.9191, Val: 0.7007, Test: 0.7004\n",
      "Epoch: 025, Loss: 0.9180, Val: 0.6999, Test: 0.6977\n",
      "Epoch: 026, Loss: 0.8991, Val: 0.7017, Test: 0.6965\n",
      "Epoch: 027, Loss: 0.9004, Val: 0.7041, Test: 0.7000\n",
      "Epoch: 028, Loss: 0.9017, Val: 0.7057, Test: 0.7031\n",
      "Epoch: 029, Loss: 0.9000, Val: 0.7051, Test: 0.7034\n",
      "Epoch: 030, Loss: 0.8872, Val: 0.7044, Test: 0.7018\n",
      "Epoch: 031, Loss: 0.8959, Val: 0.7027, Test: 0.7004\n",
      "Epoch: 032, Loss: 0.8863, Val: 0.7036, Test: 0.7002\n",
      "Epoch: 033, Loss: 0.8760, Val: 0.7049, Test: 0.7020\n",
      "Epoch: 034, Loss: 0.8716, Val: 0.7069, Test: 0.7042\n",
      "Epoch: 035, Loss: 0.8753, Val: 0.7083, Test: 0.7060\n",
      "Epoch: 036, Loss: 0.8781, Val: 0.7089, Test: 0.7062\n",
      "Epoch: 037, Loss: 0.8804, Val: 0.7090, Test: 0.7049\n",
      "Epoch: 038, Loss: 0.8652, Val: 0.7094, Test: 0.7053\n",
      "Epoch: 039, Loss: 0.8551, Val: 0.7092, Test: 0.7062\n",
      "Epoch: 040, Loss: 0.8647, Val: 0.7098, Test: 0.7083\n",
      "Epoch: 041, Loss: 0.8612, Val: 0.7106, Test: 0.7088\n",
      "Epoch: 042, Loss: 0.8553, Val: 0.7106, Test: 0.7089\n",
      "Epoch: 043, Loss: 0.8572, Val: 0.7102, Test: 0.7064\n",
      "Epoch: 044, Loss: 0.8598, Val: 0.7088, Test: 0.7037\n",
      "Epoch: 045, Loss: 0.8639, Val: 0.7070, Test: 0.7021\n",
      "Epoch: 046, Loss: 0.8633, Val: 0.7082, Test: 0.7038\n",
      "Epoch: 047, Loss: 0.8547, Val: 0.7097, Test: 0.7069\n",
      "Epoch: 048, Loss: 0.8465, Val: 0.7111, Test: 0.7103\n",
      "Epoch: 049, Loss: 0.8525, Val: 0.7123, Test: 0.7102\n",
      "Epoch: 050, Loss: 0.8479, Val: 0.7117, Test: 0.7090\n",
      "Epoch: 051, Loss: 0.8437, Val: 0.7116, Test: 0.7085\n",
      "Epoch: 052, Loss: 0.8403, Val: 0.7133, Test: 0.7102\n",
      "Epoch: 053, Loss: 0.8356, Val: 0.7134, Test: 0.7118\n",
      "Epoch: 054, Loss: 0.8664, Val: 0.7130, Test: 0.7114\n",
      "Epoch: 055, Loss: 0.8238, Val: 0.7132, Test: 0.7090\n",
      "Epoch: 056, Loss: 0.8450, Val: 0.7126, Test: 0.7074\n",
      "Epoch: 057, Loss: 0.8320, Val: 0.7125, Test: 0.7080\n",
      "Epoch: 058, Loss: 0.8395, Val: 0.7143, Test: 0.7096\n",
      "Epoch: 059, Loss: 0.8402, Val: 0.7155, Test: 0.7112\n",
      "Epoch: 060, Loss: 0.8390, Val: 0.7155, Test: 0.7103\n",
      "Epoch: 061, Loss: 0.8492, Val: 0.7145, Test: 0.7096\n",
      "Epoch: 062, Loss: 0.8379, Val: 0.7135, Test: 0.7088\n",
      "Epoch: 063, Loss: 0.8429, Val: 0.7134, Test: 0.7089\n",
      "Epoch: 064, Loss: 0.8194, Val: 0.7137, Test: 0.7088\n",
      "Epoch: 065, Loss: 0.8228, Val: 0.7140, Test: 0.7086\n",
      "Epoch: 066, Loss: 0.8310, Val: 0.7130, Test: 0.7094\n",
      "Epoch: 067, Loss: 0.8312, Val: 0.7134, Test: 0.7100\n",
      "Epoch: 068, Loss: 0.8344, Val: 0.7146, Test: 0.7119\n",
      "Epoch: 069, Loss: 0.8296, Val: 0.7158, Test: 0.7130\n",
      "Epoch: 070, Loss: 0.8223, Val: 0.7156, Test: 0.7125\n",
      "Epoch: 071, Loss: 0.8208, Val: 0.7143, Test: 0.7105\n",
      "Epoch: 072, Loss: 0.8286, Val: 0.7129, Test: 0.7093\n",
      "Epoch: 073, Loss: 0.8217, Val: 0.7143, Test: 0.7106\n",
      "Epoch: 074, Loss: 0.8218, Val: 0.7155, Test: 0.7122\n",
      "Epoch: 075, Loss: 0.8222, Val: 0.7156, Test: 0.7129\n",
      "Epoch: 076, Loss: 0.8311, Val: 0.7164, Test: 0.7135\n",
      "Epoch: 077, Loss: 0.8370, Val: 0.7161, Test: 0.7138\n",
      "Epoch: 078, Loss: 0.8244, Val: 0.7131, Test: 0.7097\n",
      "Epoch: 079, Loss: 0.8251, Val: 0.7124, Test: 0.7077\n",
      "Epoch: 080, Loss: 0.8237, Val: 0.7143, Test: 0.7100\n",
      "Epoch: 081, Loss: 0.8177, Val: 0.7164, Test: 0.7122\n",
      "Epoch: 082, Loss: 0.8155, Val: 0.7167, Test: 0.7131\n",
      "Epoch: 083, Loss: 0.8283, Val: 0.7163, Test: 0.7131\n",
      "Epoch: 084, Loss: 0.8190, Val: 0.7153, Test: 0.7116\n",
      "Epoch: 085, Loss: 0.8143, Val: 0.7164, Test: 0.7121\n",
      "Epoch: 086, Loss: 0.8280, Val: 0.7165, Test: 0.7132\n",
      "Epoch: 087, Loss: 0.8194, Val: 0.7175, Test: 0.7139\n",
      "Epoch: 088, Loss: 0.8174, Val: 0.7180, Test: 0.7142\n",
      "Epoch: 089, Loss: 0.8106, Val: 0.7173, Test: 0.7134\n",
      "Epoch: 090, Loss: 0.7956, Val: 0.7170, Test: 0.7131\n",
      "Epoch: 091, Loss: 0.8054, Val: 0.7167, Test: 0.7125\n",
      "Epoch: 092, Loss: 0.8042, Val: 0.7175, Test: 0.7120\n",
      "Epoch: 093, Loss: 0.8138, Val: 0.7178, Test: 0.7127\n",
      "Epoch: 094, Loss: 0.8206, Val: 0.7179, Test: 0.7134\n",
      "Epoch: 095, Loss: 0.8128, Val: 0.7166, Test: 0.7133\n",
      "Epoch: 096, Loss: 0.8117, Val: 0.7168, Test: 0.7123\n",
      "Epoch: 097, Loss: 0.8137, Val: 0.7155, Test: 0.7108\n",
      "Epoch: 098, Loss: 0.8085, Val: 0.7152, Test: 0.7096\n",
      "Epoch: 099, Loss: 0.8053, Val: 0.7167, Test: 0.7113\n",
      "Epoch: 100, Loss: 0.7949, Val: 0.7190, Test: 0.7147\n",
      "Epoch: 101, Loss: 0.8022, Val: 0.7198, Test: 0.7170\n",
      "Epoch: 102, Loss: 0.8141, Val: 0.7193, Test: 0.7153\n",
      "Epoch: 103, Loss: 0.8020, Val: 0.7185, Test: 0.7126\n",
      "Epoch: 104, Loss: 0.8249, Val: 0.7161, Test: 0.7106\n",
      "Epoch: 105, Loss: 0.8034, Val: 0.7156, Test: 0.7110\n",
      "Epoch: 106, Loss: 0.8098, Val: 0.7179, Test: 0.7137\n",
      "Epoch: 107, Loss: 0.8068, Val: 0.7190, Test: 0.7164\n",
      "Epoch: 108, Loss: 0.8118, Val: 0.7189, Test: 0.7158\n",
      "Epoch: 109, Loss: 0.7982, Val: 0.7177, Test: 0.7148\n",
      "Epoch: 110, Loss: 0.8092, Val: 0.7167, Test: 0.7117\n",
      "Epoch: 111, Loss: 0.8007, Val: 0.7173, Test: 0.7122\n",
      "Epoch: 112, Loss: 0.8046, Val: 0.7178, Test: 0.7143\n",
      "Epoch: 113, Loss: 0.8074, Val: 0.7182, Test: 0.7157\n",
      "Epoch: 114, Loss: 0.8006, Val: 0.7176, Test: 0.7153\n",
      "Epoch: 115, Loss: 0.8122, Val: 0.7171, Test: 0.7126\n",
      "Epoch: 116, Loss: 0.8114, Val: 0.7178, Test: 0.7131\n",
      "Epoch: 117, Loss: 0.7957, Val: 0.7183, Test: 0.7142\n",
      "Epoch: 118, Loss: 0.7997, Val: 0.7193, Test: 0.7149\n",
      "Epoch: 119, Loss: 0.8041, Val: 0.7186, Test: 0.7137\n",
      "Epoch: 120, Loss: 0.7875, Val: 0.7177, Test: 0.7126\n",
      "Epoch: 121, Loss: 0.8080, Val: 0.7181, Test: 0.7120\n",
      "Epoch: 122, Loss: 0.8043, Val: 0.7194, Test: 0.7139\n",
      "Epoch: 123, Loss: 0.7879, Val: 0.7191, Test: 0.7153\n",
      "Epoch: 124, Loss: 0.7928, Val: 0.7189, Test: 0.7159\n",
      "Epoch: 125, Loss: 0.7946, Val: 0.7184, Test: 0.7158\n",
      "Epoch: 126, Loss: 0.7902, Val: 0.7187, Test: 0.7148\n",
      "Epoch: 127, Loss: 0.7914, Val: 0.7192, Test: 0.7145\n",
      "Epoch: 128, Loss: 0.7980, Val: 0.7186, Test: 0.7148\n",
      "Epoch: 129, Loss: 0.7982, Val: 0.7185, Test: 0.7165\n",
      "Epoch: 130, Loss: 0.7972, Val: 0.7192, Test: 0.7167\n",
      "Epoch: 131, Loss: 0.8066, Val: 0.7193, Test: 0.7159\n",
      "Epoch: 132, Loss: 0.7941, Val: 0.7175, Test: 0.7129\n",
      "Epoch: 133, Loss: 0.7932, Val: 0.7172, Test: 0.7108\n",
      "Epoch: 134, Loss: 0.8013, Val: 0.7169, Test: 0.7106\n",
      "Epoch: 135, Loss: 0.7828, Val: 0.7184, Test: 0.7130\n",
      "Epoch: 136, Loss: 0.7938, Val: 0.7180, Test: 0.7150\n",
      "Epoch: 137, Loss: 0.7961, Val: 0.7191, Test: 0.7168\n",
      "Epoch: 138, Loss: 0.7981, Val: 0.7203, Test: 0.7173\n",
      "Epoch: 139, Loss: 0.7981, Val: 0.7202, Test: 0.7161\n",
      "Epoch: 140, Loss: 0.8024, Val: 0.7193, Test: 0.7152\n",
      "Epoch: 141, Loss: 0.8030, Val: 0.7193, Test: 0.7148\n",
      "Epoch: 142, Loss: 0.8041, Val: 0.7190, Test: 0.7151\n",
      "Epoch: 143, Loss: 0.7927, Val: 0.7194, Test: 0.7159\n",
      "Epoch: 144, Loss: 0.7825, Val: 0.7185, Test: 0.7154\n",
      "Epoch: 145, Loss: 0.7882, Val: 0.7166, Test: 0.7126\n",
      "Epoch: 146, Loss: 0.7942, Val: 0.7165, Test: 0.7108\n",
      "Epoch: 147, Loss: 0.7808, Val: 0.7169, Test: 0.7109\n",
      "Epoch: 148, Loss: 0.7902, Val: 0.7182, Test: 0.7130\n",
      "Epoch: 149, Loss: 0.7975, Val: 0.7193, Test: 0.7159\n",
      "Epoch: 150, Loss: 0.7869, Val: 0.7201, Test: 0.7175\n",
      "Epoch: 151, Loss: 0.7874, Val: 0.7203, Test: 0.7170\n",
      "Epoch: 152, Loss: 0.8081, Val: 0.7191, Test: 0.7144\n",
      "Epoch: 153, Loss: 0.7916, Val: 0.7176, Test: 0.7118\n",
      "Epoch: 154, Loss: 0.7864, Val: 0.7187, Test: 0.7141\n",
      "Epoch: 155, Loss: 0.7940, Val: 0.7202, Test: 0.7163\n",
      "Epoch: 156, Loss: 0.7970, Val: 0.7208, Test: 0.7173\n",
      "Epoch: 157, Loss: 0.7821, Val: 0.7196, Test: 0.7168\n",
      "Epoch: 158, Loss: 0.7811, Val: 0.7172, Test: 0.7150\n",
      "Epoch: 159, Loss: 0.7954, Val: 0.7173, Test: 0.7142\n",
      "Epoch: 160, Loss: 0.7976, Val: 0.7188, Test: 0.7150\n",
      "Epoch: 161, Loss: 0.7958, Val: 0.7197, Test: 0.7157\n",
      "Epoch: 162, Loss: 0.7820, Val: 0.7202, Test: 0.7156\n",
      "Epoch: 163, Loss: 0.7857, Val: 0.7203, Test: 0.7152\n",
      "Epoch: 164, Loss: 0.7755, Val: 0.7190, Test: 0.7153\n",
      "Epoch: 165, Loss: 0.7919, Val: 0.7192, Test: 0.7142\n",
      "Epoch: 166, Loss: 0.7839, Val: 0.7186, Test: 0.7144\n",
      "Epoch: 167, Loss: 0.7705, Val: 0.7184, Test: 0.7148\n",
      "Epoch: 168, Loss: 0.7929, Val: 0.7188, Test: 0.7142\n",
      "Epoch: 169, Loss: 0.7867, Val: 0.7183, Test: 0.7134\n",
      "Epoch: 170, Loss: 0.7885, Val: 0.7176, Test: 0.7140\n",
      "Epoch: 171, Loss: 0.7990, Val: 0.7176, Test: 0.7136\n",
      "Epoch: 172, Loss: 0.7814, Val: 0.7192, Test: 0.7150\n",
      "Epoch: 173, Loss: 0.7688, Val: 0.7201, Test: 0.7161\n",
      "Epoch: 174, Loss: 0.7899, Val: 0.7192, Test: 0.7151\n",
      "Epoch: 175, Loss: 0.7831, Val: 0.7187, Test: 0.7135\n",
      "Epoch: 176, Loss: 0.7705, Val: 0.7187, Test: 0.7141\n",
      "Epoch: 177, Loss: 0.7978, Val: 0.7197, Test: 0.7163\n",
      "Epoch: 178, Loss: 0.7856, Val: 0.7204, Test: 0.7174\n",
      "Epoch: 179, Loss: 0.7855, Val: 0.7202, Test: 0.7178\n",
      "Epoch: 180, Loss: 0.7842, Val: 0.7191, Test: 0.7161\n",
      "Epoch: 181, Loss: 0.7783, Val: 0.7193, Test: 0.7151\n",
      "Epoch: 182, Loss: 0.7768, Val: 0.7186, Test: 0.7143\n",
      "Epoch: 183, Loss: 0.7788, Val: 0.7196, Test: 0.7150\n",
      "Epoch: 184, Loss: 0.7879, Val: 0.7193, Test: 0.7154\n",
      "Epoch: 185, Loss: 0.7910, Val: 0.7192, Test: 0.7144\n",
      "Epoch: 186, Loss: 0.7846, Val: 0.7193, Test: 0.7143\n",
      "Epoch: 187, Loss: 0.7741, Val: 0.7202, Test: 0.7155\n",
      "Epoch: 188, Loss: 0.7891, Val: 0.7198, Test: 0.7165\n",
      "Epoch: 189, Loss: 0.7920, Val: 0.7196, Test: 0.7157\n",
      "Epoch: 190, Loss: 0.7804, Val: 0.7197, Test: 0.7160\n",
      "Epoch: 191, Loss: 0.7856, Val: 0.7196, Test: 0.7162\n",
      "Epoch: 192, Loss: 0.7738, Val: 0.7193, Test: 0.7145\n",
      "Epoch: 193, Loss: 0.7913, Val: 0.7185, Test: 0.7141\n",
      "Epoch: 194, Loss: 0.7762, Val: 0.7198, Test: 0.7150\n",
      "Epoch: 195, Loss: 0.7804, Val: 0.7210, Test: 0.7169\n",
      "Epoch: 196, Loss: 0.7715, Val: 0.7204, Test: 0.7168\n",
      "Epoch: 197, Loss: 0.7850, Val: 0.7193, Test: 0.7153\n",
      "Epoch: 198, Loss: 0.7787, Val: 0.7187, Test: 0.7132\n",
      "Epoch: 199, Loss: 0.7748, Val: 0.7188, Test: 0.7131\n",
      "Epoch: 200, Loss: 0.7676, Val: 0.7191, Test: 0.7152\n",
      "Epoch: 201, Loss: 0.7834, Val: 0.7197, Test: 0.7155\n",
      "Epoch: 202, Loss: 0.7836, Val: 0.7196, Test: 0.7149\n",
      "Epoch: 203, Loss: 0.7969, Val: 0.7184, Test: 0.7138\n",
      "Epoch: 204, Loss: 0.7843, Val: 0.7197, Test: 0.7144\n",
      "Epoch: 205, Loss: 0.7892, Val: 0.7192, Test: 0.7148\n",
      "Epoch: 206, Loss: 0.7710, Val: 0.7192, Test: 0.7142\n",
      "Epoch: 207, Loss: 0.7734, Val: 0.7195, Test: 0.7142\n",
      "Epoch: 208, Loss: 0.7797, Val: 0.7187, Test: 0.7133\n",
      "Epoch: 209, Loss: 0.7727, Val: 0.7181, Test: 0.7127\n",
      "Epoch: 210, Loss: 0.7762, Val: 0.7184, Test: 0.7141\n",
      "Epoch: 211, Loss: 0.7786, Val: 0.7195, Test: 0.7161\n",
      "Epoch: 212, Loss: 0.7842, Val: 0.7201, Test: 0.7172\n",
      "Epoch: 213, Loss: 0.7799, Val: 0.7190, Test: 0.7153\n",
      "Epoch: 214, Loss: 0.7746, Val: 0.7172, Test: 0.7127\n",
      "Epoch: 215, Loss: 0.7705, Val: 0.7173, Test: 0.7127\n",
      "Epoch: 216, Loss: 0.7829, Val: 0.7203, Test: 0.7162\n",
      "Epoch: 217, Loss: 0.7791, Val: 0.7207, Test: 0.7172\n",
      "Epoch: 218, Loss: 0.7825, Val: 0.7204, Test: 0.7178\n",
      "Epoch: 219, Loss: 0.7740, Val: 0.7196, Test: 0.7151\n",
      "Epoch: 220, Loss: 0.7780, Val: 0.7193, Test: 0.7140\n",
      "Epoch: 221, Loss: 0.7674, Val: 0.7196, Test: 0.7143\n",
      "Epoch: 222, Loss: 0.7750, Val: 0.7201, Test: 0.7152\n",
      "Epoch: 223, Loss: 0.7799, Val: 0.7194, Test: 0.7140\n",
      "Epoch: 224, Loss: 0.7809, Val: 0.7195, Test: 0.7143\n",
      "Epoch: 225, Loss: 0.7761, Val: 0.7191, Test: 0.7142\n",
      "Epoch: 226, Loss: 0.7771, Val: 0.7183, Test: 0.7133\n",
      "Epoch: 227, Loss: 0.7753, Val: 0.7179, Test: 0.7129\n",
      "Epoch: 228, Loss: 0.7718, Val: 0.7185, Test: 0.7119\n",
      "Epoch: 229, Loss: 0.7786, Val: 0.7193, Test: 0.7144\n",
      "Epoch: 230, Loss: 0.7685, Val: 0.7202, Test: 0.7166\n",
      "Epoch: 231, Loss: 0.7844, Val: 0.7192, Test: 0.7164\n",
      "Epoch: 232, Loss: 0.7845, Val: 0.7190, Test: 0.7151\n",
      "Epoch: 233, Loss: 0.7747, Val: 0.7194, Test: 0.7161\n",
      "Epoch: 234, Loss: 0.7701, Val: 0.7206, Test: 0.7170\n",
      "Epoch: 235, Loss: 0.7763, Val: 0.7197, Test: 0.7159\n",
      "Epoch: 236, Loss: 0.7872, Val: 0.7191, Test: 0.7138\n",
      "Epoch: 237, Loss: 0.7732, Val: 0.7186, Test: 0.7137\n",
      "Epoch: 238, Loss: 0.7699, Val: 0.7189, Test: 0.7155\n",
      "Epoch: 239, Loss: 0.7773, Val: 0.7196, Test: 0.7175\n",
      "Epoch: 240, Loss: 0.7667, Val: 0.7202, Test: 0.7180\n",
      "Epoch: 241, Loss: 0.7662, Val: 0.7202, Test: 0.7168\n",
      "Epoch: 242, Loss: 0.7790, Val: 0.7205, Test: 0.7157\n",
      "Epoch: 243, Loss: 0.7756, Val: 0.7208, Test: 0.7166\n",
      "Epoch: 244, Loss: 0.7701, Val: 0.7215, Test: 0.7181\n",
      "Epoch: 245, Loss: 0.7687, Val: 0.7203, Test: 0.7174\n",
      "Epoch: 246, Loss: 0.7703, Val: 0.7177, Test: 0.7152\n",
      "Epoch: 247, Loss: 0.7654, Val: 0.7174, Test: 0.7133\n",
      "Epoch: 248, Loss: 0.7567, Val: 0.7187, Test: 0.7159\n",
      "Epoch: 249, Loss: 0.7676, Val: 0.7216, Test: 0.7187\n",
      "Epoch: 250, Loss: 0.7701, Val: 0.7215, Test: 0.7188\n",
      "Epoch: 251, Loss: 0.7694, Val: 0.7204, Test: 0.7144\n",
      "Epoch: 252, Loss: 0.7528, Val: 0.7180, Test: 0.7120\n",
      "Epoch: 253, Loss: 0.7831, Val: 0.7189, Test: 0.7139\n",
      "Epoch: 254, Loss: 0.7738, Val: 0.7204, Test: 0.7170\n",
      "Epoch: 255, Loss: 0.7755, Val: 0.7209, Test: 0.7194\n",
      "Epoch: 256, Loss: 0.7740, Val: 0.7195, Test: 0.7173\n",
      "Epoch: 257, Loss: 0.7639, Val: 0.7174, Test: 0.7143\n",
      "Epoch: 258, Loss: 0.7802, Val: 0.7164, Test: 0.7116\n",
      "Epoch: 259, Loss: 0.7697, Val: 0.7195, Test: 0.7165\n",
      "Epoch: 260, Loss: 0.7688, Val: 0.7208, Test: 0.7191\n",
      "Epoch: 261, Loss: 0.7657, Val: 0.7190, Test: 0.7159\n",
      "Epoch: 262, Loss: 0.7709, Val: 0.7172, Test: 0.7128\n",
      "Epoch: 263, Loss: 0.7710, Val: 0.7167, Test: 0.7120\n",
      "Epoch: 264, Loss: 0.7627, Val: 0.7189, Test: 0.7144\n",
      "Epoch: 265, Loss: 0.7660, Val: 0.7213, Test: 0.7175\n",
      "Epoch: 266, Loss: 0.7590, Val: 0.7230, Test: 0.7194\n",
      "Epoch: 267, Loss: 0.7774, Val: 0.7208, Test: 0.7161\n",
      "Epoch: 268, Loss: 0.7693, Val: 0.7192, Test: 0.7142\n",
      "Epoch: 269, Loss: 0.7735, Val: 0.7193, Test: 0.7147\n",
      "Epoch: 270, Loss: 0.7689, Val: 0.7194, Test: 0.7165\n",
      "Epoch: 271, Loss: 0.7790, Val: 0.7203, Test: 0.7172\n",
      "Epoch: 272, Loss: 0.7749, Val: 0.7183, Test: 0.7148\n",
      "Epoch: 273, Loss: 0.7653, Val: 0.7170, Test: 0.7101\n",
      "Epoch: 274, Loss: 0.7678, Val: 0.7181, Test: 0.7139\n",
      "Epoch: 275, Loss: 0.7743, Val: 0.7197, Test: 0.7174\n",
      "Epoch: 276, Loss: 0.7628, Val: 0.7201, Test: 0.7186\n",
      "Epoch: 277, Loss: 0.7792, Val: 0.7197, Test: 0.7167\n",
      "Epoch: 278, Loss: 0.7615, Val: 0.7198, Test: 0.7155\n",
      "Epoch: 279, Loss: 0.7721, Val: 0.7186, Test: 0.7153\n",
      "Epoch: 280, Loss: 0.7666, Val: 0.7197, Test: 0.7173\n",
      "Epoch: 281, Loss: 0.7745, Val: 0.7197, Test: 0.7178\n",
      "Epoch: 282, Loss: 0.7653, Val: 0.7198, Test: 0.7171\n",
      "Epoch: 283, Loss: 0.7676, Val: 0.7197, Test: 0.7154\n",
      "Epoch: 284, Loss: 0.7596, Val: 0.7202, Test: 0.7150\n",
      "Epoch: 285, Loss: 0.7797, Val: 0.7186, Test: 0.7142\n",
      "Epoch: 286, Loss: 0.7581, Val: 0.7193, Test: 0.7155\n",
      "Epoch: 287, Loss: 0.7517, Val: 0.7202, Test: 0.7165\n",
      "Epoch: 288, Loss: 0.7588, Val: 0.7204, Test: 0.7170\n",
      "Epoch: 289, Loss: 0.7598, Val: 0.7204, Test: 0.7161\n",
      "Epoch: 290, Loss: 0.7662, Val: 0.7197, Test: 0.7161\n",
      "Epoch: 291, Loss: 0.7601, Val: 0.7193, Test: 0.7159\n",
      "Epoch: 292, Loss: 0.7506, Val: 0.7191, Test: 0.7150\n",
      "Epoch: 293, Loss: 0.7631, Val: 0.7194, Test: 0.7152\n",
      "Epoch: 294, Loss: 0.7608, Val: 0.7201, Test: 0.7162\n",
      "Epoch: 295, Loss: 0.7556, Val: 0.7200, Test: 0.7174\n",
      "Epoch: 296, Loss: 0.7708, Val: 0.7192, Test: 0.7158\n",
      "Epoch: 297, Loss: 0.7580, Val: 0.7196, Test: 0.7167\n",
      "Epoch: 298, Loss: 0.7641, Val: 0.7201, Test: 0.7172\n",
      "Epoch: 299, Loss: 0.7613, Val: 0.7197, Test: 0.7170\n",
      "Epoch: 300, Loss: 0.7665, Val: 0.7192, Test: 0.7158\n",
      "0.8\n",
      "Label_rate: 0.8\n",
      "Epoch: 001, Loss: 1.9976, Val: 0.5676, Test: 0.5231\n",
      "Epoch: 002, Loss: 1.5892, Val: 0.5952, Test: 0.5583\n",
      "Epoch: 003, Loss: 1.3957, Val: 0.6152, Test: 0.5893\n",
      "Epoch: 004, Loss: 1.2805, Val: 0.6286, Test: 0.6183\n",
      "Epoch: 005, Loss: 1.2077, Val: 0.6551, Test: 0.6525\n",
      "Epoch: 006, Loss: 1.1366, Val: 0.6640, Test: 0.6584\n",
      "Epoch: 007, Loss: 1.0828, Val: 0.6712, Test: 0.6656\n",
      "Epoch: 008, Loss: 1.0538, Val: 0.6768, Test: 0.6790\n",
      "Epoch: 009, Loss: 1.0302, Val: 0.6825, Test: 0.6890\n",
      "Epoch: 010, Loss: 1.0171, Val: 0.6949, Test: 0.6973\n",
      "Epoch: 011, Loss: 1.0034, Val: 0.6986, Test: 0.6977\n",
      "Epoch: 012, Loss: 0.9983, Val: 0.6986, Test: 0.6954\n",
      "Epoch: 013, Loss: 0.9815, Val: 0.6982, Test: 0.6954\n",
      "Epoch: 014, Loss: 0.9575, Val: 0.6980, Test: 0.6940\n",
      "Epoch: 015, Loss: 0.9573, Val: 0.6994, Test: 0.6954\n",
      "Epoch: 016, Loss: 0.9529, Val: 0.7003, Test: 0.6966\n",
      "Epoch: 017, Loss: 0.9438, Val: 0.6995, Test: 0.6971\n",
      "Epoch: 018, Loss: 0.9312, Val: 0.7003, Test: 0.6978\n",
      "Epoch: 019, Loss: 0.9239, Val: 0.7007, Test: 0.6997\n",
      "Epoch: 020, Loss: 0.9089, Val: 0.7037, Test: 0.7023\n",
      "Epoch: 021, Loss: 0.9048, Val: 0.7050, Test: 0.7032\n",
      "Epoch: 022, Loss: 0.9036, Val: 0.7054, Test: 0.7020\n",
      "Epoch: 023, Loss: 0.8878, Val: 0.7038, Test: 0.7006\n",
      "Epoch: 024, Loss: 0.8826, Val: 0.7045, Test: 0.7012\n",
      "Epoch: 025, Loss: 0.8965, Val: 0.7054, Test: 0.7024\n",
      "Epoch: 026, Loss: 0.8654, Val: 0.7076, Test: 0.7059\n",
      "Epoch: 027, Loss: 0.8649, Val: 0.7089, Test: 0.7078\n",
      "Epoch: 028, Loss: 0.8709, Val: 0.7094, Test: 0.7074\n",
      "Epoch: 029, Loss: 0.8622, Val: 0.7098, Test: 0.7066\n",
      "Epoch: 030, Loss: 0.8809, Val: 0.7100, Test: 0.7063\n",
      "Epoch: 031, Loss: 0.8546, Val: 0.7100, Test: 0.7082\n",
      "Epoch: 032, Loss: 0.8540, Val: 0.7100, Test: 0.7101\n",
      "Epoch: 033, Loss: 0.8602, Val: 0.7113, Test: 0.7113\n",
      "Epoch: 034, Loss: 0.8553, Val: 0.7121, Test: 0.7122\n",
      "Epoch: 035, Loss: 0.8372, Val: 0.7113, Test: 0.7115\n",
      "Epoch: 036, Loss: 0.8490, Val: 0.7100, Test: 0.7103\n",
      "Epoch: 037, Loss: 0.8483, Val: 0.7098, Test: 0.7099\n",
      "Epoch: 038, Loss: 0.8389, Val: 0.7088, Test: 0.7100\n",
      "Epoch: 039, Loss: 0.8435, Val: 0.7097, Test: 0.7098\n",
      "Epoch: 040, Loss: 0.8371, Val: 0.7106, Test: 0.7109\n",
      "Epoch: 041, Loss: 0.8388, Val: 0.7131, Test: 0.7116\n",
      "Epoch: 042, Loss: 0.8440, Val: 0.7135, Test: 0.7117\n",
      "Epoch: 043, Loss: 0.8294, Val: 0.7148, Test: 0.7128\n",
      "Epoch: 044, Loss: 0.8290, Val: 0.7149, Test: 0.7132\n",
      "Epoch: 045, Loss: 0.8326, Val: 0.7129, Test: 0.7121\n",
      "Epoch: 046, Loss: 0.8315, Val: 0.7130, Test: 0.7128\n",
      "Epoch: 047, Loss: 0.8173, Val: 0.7138, Test: 0.7140\n",
      "Epoch: 048, Loss: 0.8300, Val: 0.7148, Test: 0.7143\n",
      "Epoch: 049, Loss: 0.8139, Val: 0.7159, Test: 0.7147\n",
      "Epoch: 050, Loss: 0.8357, Val: 0.7156, Test: 0.7148\n",
      "Epoch: 051, Loss: 0.8312, Val: 0.7161, Test: 0.7147\n",
      "Epoch: 052, Loss: 0.8166, Val: 0.7153, Test: 0.7142\n",
      "Epoch: 053, Loss: 0.8276, Val: 0.7148, Test: 0.7140\n",
      "Epoch: 054, Loss: 0.8064, Val: 0.7146, Test: 0.7137\n",
      "Epoch: 055, Loss: 0.8107, Val: 0.7145, Test: 0.7133\n",
      "Epoch: 056, Loss: 0.8343, Val: 0.7159, Test: 0.7145\n",
      "Epoch: 057, Loss: 0.8375, Val: 0.7158, Test: 0.7143\n",
      "Epoch: 058, Loss: 0.8278, Val: 0.7150, Test: 0.7134\n",
      "Epoch: 059, Loss: 0.8092, Val: 0.7151, Test: 0.7141\n",
      "Epoch: 060, Loss: 0.8243, Val: 0.7153, Test: 0.7149\n",
      "Epoch: 061, Loss: 0.8028, Val: 0.7158, Test: 0.7153\n",
      "Epoch: 062, Loss: 0.8109, Val: 0.7173, Test: 0.7170\n",
      "Epoch: 063, Loss: 0.8144, Val: 0.7182, Test: 0.7178\n",
      "Epoch: 064, Loss: 0.7964, Val: 0.7188, Test: 0.7193\n",
      "Epoch: 065, Loss: 0.8144, Val: 0.7175, Test: 0.7178\n",
      "Epoch: 066, Loss: 0.8174, Val: 0.7161, Test: 0.7153\n",
      "Epoch: 067, Loss: 0.8127, Val: 0.7154, Test: 0.7142\n",
      "Epoch: 068, Loss: 0.8028, Val: 0.7155, Test: 0.7164\n",
      "Epoch: 069, Loss: 0.8007, Val: 0.7186, Test: 0.7196\n",
      "Epoch: 070, Loss: 0.7951, Val: 0.7191, Test: 0.7204\n",
      "Epoch: 071, Loss: 0.8054, Val: 0.7187, Test: 0.7195\n",
      "Epoch: 072, Loss: 0.8020, Val: 0.7173, Test: 0.7178\n",
      "Epoch: 073, Loss: 0.8025, Val: 0.7167, Test: 0.7161\n",
      "Epoch: 074, Loss: 0.8041, Val: 0.7178, Test: 0.7179\n",
      "Epoch: 075, Loss: 0.7934, Val: 0.7180, Test: 0.7194\n",
      "Epoch: 076, Loss: 0.7927, Val: 0.7194, Test: 0.7201\n",
      "Epoch: 077, Loss: 0.7886, Val: 0.7191, Test: 0.7179\n",
      "Epoch: 078, Loss: 0.7843, Val: 0.7182, Test: 0.7146\n",
      "Epoch: 079, Loss: 0.8030, Val: 0.7168, Test: 0.7142\n",
      "Epoch: 080, Loss: 0.8097, Val: 0.7172, Test: 0.7158\n",
      "Epoch: 081, Loss: 0.7994, Val: 0.7194, Test: 0.7195\n",
      "Epoch: 082, Loss: 0.7966, Val: 0.7197, Test: 0.7204\n",
      "Epoch: 083, Loss: 0.7930, Val: 0.7198, Test: 0.7204\n",
      "Epoch: 084, Loss: 0.8014, Val: 0.7176, Test: 0.7165\n",
      "Epoch: 085, Loss: 0.7994, Val: 0.7169, Test: 0.7135\n",
      "Epoch: 086, Loss: 0.7852, Val: 0.7179, Test: 0.7162\n",
      "Epoch: 087, Loss: 0.7854, Val: 0.7198, Test: 0.7201\n",
      "Epoch: 088, Loss: 0.7998, Val: 0.7204, Test: 0.7224\n",
      "Epoch: 089, Loss: 0.7882, Val: 0.7194, Test: 0.7215\n",
      "Epoch: 090, Loss: 0.7833, Val: 0.7185, Test: 0.7197\n",
      "Epoch: 091, Loss: 0.7968, Val: 0.7180, Test: 0.7167\n",
      "Epoch: 092, Loss: 0.7847, Val: 0.7183, Test: 0.7165\n",
      "Epoch: 093, Loss: 0.7835, Val: 0.7192, Test: 0.7184\n",
      "Epoch: 094, Loss: 0.7897, Val: 0.7193, Test: 0.7193\n",
      "Epoch: 095, Loss: 0.7960, Val: 0.7190, Test: 0.7200\n",
      "Epoch: 096, Loss: 0.7835, Val: 0.7197, Test: 0.7209\n",
      "Epoch: 097, Loss: 0.7882, Val: 0.7192, Test: 0.7207\n",
      "Epoch: 098, Loss: 0.7929, Val: 0.7197, Test: 0.7199\n",
      "Epoch: 099, Loss: 0.7881, Val: 0.7189, Test: 0.7199\n",
      "Epoch: 100, Loss: 0.7781, Val: 0.7188, Test: 0.7196\n",
      "Epoch: 101, Loss: 0.7891, Val: 0.7187, Test: 0.7188\n",
      "Epoch: 102, Loss: 0.7779, Val: 0.7194, Test: 0.7208\n",
      "Epoch: 103, Loss: 0.7780, Val: 0.7201, Test: 0.7212\n",
      "Epoch: 104, Loss: 0.7905, Val: 0.7209, Test: 0.7207\n",
      "Epoch: 105, Loss: 0.7860, Val: 0.7193, Test: 0.7188\n",
      "Epoch: 106, Loss: 0.7889, Val: 0.7177, Test: 0.7139\n",
      "Epoch: 107, Loss: 0.7784, Val: 0.7188, Test: 0.7144\n",
      "Epoch: 108, Loss: 0.7766, Val: 0.7200, Test: 0.7184\n",
      "Epoch: 109, Loss: 0.7796, Val: 0.7217, Test: 0.7215\n",
      "Epoch: 110, Loss: 0.7887, Val: 0.7216, Test: 0.7220\n",
      "Epoch: 111, Loss: 0.7972, Val: 0.7208, Test: 0.7215\n",
      "Epoch: 112, Loss: 0.7852, Val: 0.7196, Test: 0.7203\n",
      "Epoch: 113, Loss: 0.7780, Val: 0.7203, Test: 0.7208\n",
      "Epoch: 114, Loss: 0.7860, Val: 0.7216, Test: 0.7228\n",
      "Epoch: 115, Loss: 0.7844, Val: 0.7211, Test: 0.7218\n",
      "Epoch: 116, Loss: 0.7888, Val: 0.7202, Test: 0.7210\n",
      "Epoch: 117, Loss: 0.7777, Val: 0.7194, Test: 0.7193\n",
      "Epoch: 118, Loss: 0.7930, Val: 0.7197, Test: 0.7188\n",
      "Epoch: 119, Loss: 0.7783, Val: 0.7200, Test: 0.7188\n",
      "Epoch: 120, Loss: 0.7781, Val: 0.7199, Test: 0.7194\n",
      "Epoch: 121, Loss: 0.7768, Val: 0.7204, Test: 0.7203\n",
      "Epoch: 122, Loss: 0.7887, Val: 0.7208, Test: 0.7208\n",
      "Epoch: 123, Loss: 0.7786, Val: 0.7205, Test: 0.7201\n",
      "Epoch: 124, Loss: 0.7748, Val: 0.7205, Test: 0.7191\n",
      "Epoch: 125, Loss: 0.7796, Val: 0.7203, Test: 0.7189\n",
      "Epoch: 126, Loss: 0.7852, Val: 0.7212, Test: 0.7206\n",
      "Epoch: 127, Loss: 0.7680, Val: 0.7215, Test: 0.7218\n",
      "Epoch: 128, Loss: 0.7815, Val: 0.7224, Test: 0.7220\n",
      "Epoch: 129, Loss: 0.7775, Val: 0.7211, Test: 0.7203\n",
      "Epoch: 130, Loss: 0.7781, Val: 0.7196, Test: 0.7177\n",
      "Epoch: 131, Loss: 0.7817, Val: 0.7190, Test: 0.7160\n",
      "Epoch: 132, Loss: 0.7690, Val: 0.7218, Test: 0.7190\n",
      "Epoch: 133, Loss: 0.7841, Val: 0.7220, Test: 0.7221\n",
      "Epoch: 134, Loss: 0.7782, Val: 0.7213, Test: 0.7228\n",
      "Epoch: 135, Loss: 0.7834, Val: 0.7197, Test: 0.7208\n",
      "Epoch: 136, Loss: 0.7803, Val: 0.7185, Test: 0.7161\n",
      "Epoch: 137, Loss: 0.7745, Val: 0.7195, Test: 0.7177\n",
      "Epoch: 138, Loss: 0.7562, Val: 0.7192, Test: 0.7180\n",
      "Epoch: 139, Loss: 0.7840, Val: 0.7213, Test: 0.7216\n",
      "Epoch: 140, Loss: 0.7658, Val: 0.7211, Test: 0.7207\n",
      "Epoch: 141, Loss: 0.7776, Val: 0.7201, Test: 0.7181\n",
      "Epoch: 142, Loss: 0.7682, Val: 0.7190, Test: 0.7172\n",
      "Epoch: 143, Loss: 0.7767, Val: 0.7202, Test: 0.7191\n",
      "Epoch: 144, Loss: 0.7697, Val: 0.7221, Test: 0.7232\n",
      "Epoch: 145, Loss: 0.7648, Val: 0.7238, Test: 0.7237\n",
      "Epoch: 146, Loss: 0.7655, Val: 0.7225, Test: 0.7225\n",
      "Epoch: 147, Loss: 0.7838, Val: 0.7206, Test: 0.7197\n",
      "Epoch: 148, Loss: 0.7746, Val: 0.7207, Test: 0.7195\n",
      "Epoch: 149, Loss: 0.7696, Val: 0.7227, Test: 0.7224\n",
      "Epoch: 150, Loss: 0.7702, Val: 0.7228, Test: 0.7236\n",
      "Epoch: 151, Loss: 0.7795, Val: 0.7223, Test: 0.7216\n",
      "Epoch: 152, Loss: 0.7586, Val: 0.7206, Test: 0.7184\n",
      "Epoch: 153, Loss: 0.7799, Val: 0.7199, Test: 0.7179\n",
      "Epoch: 154, Loss: 0.7752, Val: 0.7215, Test: 0.7193\n",
      "Epoch: 155, Loss: 0.7629, Val: 0.7204, Test: 0.7209\n",
      "Epoch: 156, Loss: 0.7662, Val: 0.7201, Test: 0.7202\n",
      "Epoch: 157, Loss: 0.7510, Val: 0.7208, Test: 0.7202\n",
      "Epoch: 158, Loss: 0.7547, Val: 0.7229, Test: 0.7226\n",
      "Epoch: 159, Loss: 0.7574, Val: 0.7231, Test: 0.7216\n",
      "Epoch: 160, Loss: 0.7743, Val: 0.7225, Test: 0.7220\n",
      "Epoch: 161, Loss: 0.7681, Val: 0.7233, Test: 0.7236\n",
      "Epoch: 162, Loss: 0.7829, Val: 0.7231, Test: 0.7234\n",
      "Epoch: 163, Loss: 0.7568, Val: 0.7213, Test: 0.7208\n",
      "Epoch: 164, Loss: 0.7640, Val: 0.7201, Test: 0.7202\n",
      "Epoch: 165, Loss: 0.7716, Val: 0.7213, Test: 0.7216\n",
      "Epoch: 166, Loss: 0.7673, Val: 0.7230, Test: 0.7229\n",
      "Epoch: 167, Loss: 0.7670, Val: 0.7227, Test: 0.7222\n",
      "Epoch: 168, Loss: 0.7677, Val: 0.7216, Test: 0.7202\n",
      "Epoch: 169, Loss: 0.7677, Val: 0.7215, Test: 0.7209\n",
      "Epoch: 170, Loss: 0.7677, Val: 0.7219, Test: 0.7219\n",
      "Epoch: 171, Loss: 0.7632, Val: 0.7207, Test: 0.7207\n",
      "Epoch: 172, Loss: 0.7659, Val: 0.7190, Test: 0.7185\n",
      "Epoch: 173, Loss: 0.7754, Val: 0.7203, Test: 0.7196\n",
      "Epoch: 174, Loss: 0.7607, Val: 0.7218, Test: 0.7217\n",
      "Epoch: 175, Loss: 0.7710, Val: 0.7225, Test: 0.7230\n",
      "Epoch: 176, Loss: 0.7678, Val: 0.7220, Test: 0.7222\n",
      "Epoch: 177, Loss: 0.7699, Val: 0.7203, Test: 0.7193\n",
      "Epoch: 178, Loss: 0.7554, Val: 0.7195, Test: 0.7166\n",
      "Epoch: 179, Loss: 0.7729, Val: 0.7203, Test: 0.7193\n",
      "Epoch: 180, Loss: 0.7598, Val: 0.7209, Test: 0.7219\n",
      "Epoch: 181, Loss: 0.7684, Val: 0.7214, Test: 0.7226\n",
      "Epoch: 182, Loss: 0.7633, Val: 0.7205, Test: 0.7207\n",
      "Epoch: 183, Loss: 0.7824, Val: 0.7199, Test: 0.7179\n",
      "Epoch: 184, Loss: 0.7620, Val: 0.7219, Test: 0.7209\n",
      "Epoch: 185, Loss: 0.7692, Val: 0.7226, Test: 0.7224\n",
      "Epoch: 186, Loss: 0.7635, Val: 0.7227, Test: 0.7225\n",
      "Epoch: 187, Loss: 0.7562, Val: 0.7231, Test: 0.7214\n",
      "Epoch: 188, Loss: 0.7483, Val: 0.7215, Test: 0.7200\n",
      "Epoch: 189, Loss: 0.7456, Val: 0.7223, Test: 0.7206\n",
      "Epoch: 190, Loss: 0.7702, Val: 0.7235, Test: 0.7224\n",
      "Epoch: 191, Loss: 0.7686, Val: 0.7227, Test: 0.7233\n",
      "Epoch: 192, Loss: 0.7635, Val: 0.7232, Test: 0.7226\n",
      "Epoch: 193, Loss: 0.7704, Val: 0.7223, Test: 0.7199\n",
      "Epoch: 194, Loss: 0.7639, Val: 0.7220, Test: 0.7200\n",
      "Epoch: 195, Loss: 0.7609, Val: 0.7236, Test: 0.7216\n",
      "Epoch: 196, Loss: 0.7604, Val: 0.7237, Test: 0.7225\n",
      "Epoch: 197, Loss: 0.7642, Val: 0.7234, Test: 0.7233\n",
      "Epoch: 198, Loss: 0.7650, Val: 0.7230, Test: 0.7228\n",
      "Epoch: 199, Loss: 0.7516, Val: 0.7225, Test: 0.7233\n",
      "Epoch: 200, Loss: 0.7654, Val: 0.7234, Test: 0.7244\n",
      "Epoch: 201, Loss: 0.7366, Val: 0.7244, Test: 0.7249\n",
      "Epoch: 202, Loss: 0.7672, Val: 0.7242, Test: 0.7249\n",
      "Epoch: 203, Loss: 0.7624, Val: 0.7229, Test: 0.7221\n",
      "Epoch: 204, Loss: 0.7666, Val: 0.7221, Test: 0.7208\n",
      "Epoch: 205, Loss: 0.7564, Val: 0.7229, Test: 0.7206\n",
      "Epoch: 206, Loss: 0.7617, Val: 0.7226, Test: 0.7213\n",
      "Epoch: 207, Loss: 0.7502, Val: 0.7218, Test: 0.7209\n",
      "Epoch: 208, Loss: 0.7642, Val: 0.7217, Test: 0.7214\n",
      "Epoch: 209, Loss: 0.7686, Val: 0.7221, Test: 0.7220\n",
      "Epoch: 210, Loss: 0.7460, Val: 0.7209, Test: 0.7215\n",
      "Epoch: 211, Loss: 0.7674, Val: 0.7213, Test: 0.7213\n",
      "Epoch: 212, Loss: 0.7568, Val: 0.7227, Test: 0.7218\n",
      "Epoch: 213, Loss: 0.7545, Val: 0.7210, Test: 0.7209\n",
      "Epoch: 214, Loss: 0.7630, Val: 0.7206, Test: 0.7208\n",
      "Epoch: 215, Loss: 0.7657, Val: 0.7217, Test: 0.7223\n",
      "Epoch: 216, Loss: 0.7578, Val: 0.7231, Test: 0.7226\n",
      "Epoch: 217, Loss: 0.7645, Val: 0.7231, Test: 0.7209\n",
      "Epoch: 218, Loss: 0.7592, Val: 0.7218, Test: 0.7201\n",
      "Epoch: 219, Loss: 0.7578, Val: 0.7204, Test: 0.7190\n",
      "Epoch: 220, Loss: 0.7629, Val: 0.7215, Test: 0.7200\n",
      "Epoch: 221, Loss: 0.7451, Val: 0.7231, Test: 0.7242\n",
      "Epoch: 222, Loss: 0.7507, Val: 0.7227, Test: 0.7249\n",
      "Epoch: 223, Loss: 0.7601, Val: 0.7219, Test: 0.7232\n",
      "Epoch: 224, Loss: 0.7538, Val: 0.7210, Test: 0.7221\n",
      "Epoch: 225, Loss: 0.7440, Val: 0.7220, Test: 0.7214\n",
      "Epoch: 226, Loss: 0.7567, Val: 0.7222, Test: 0.7225\n",
      "Epoch: 227, Loss: 0.7461, Val: 0.7238, Test: 0.7244\n",
      "Epoch: 228, Loss: 0.7518, Val: 0.7227, Test: 0.7227\n",
      "Epoch: 229, Loss: 0.7542, Val: 0.7210, Test: 0.7187\n",
      "Epoch: 230, Loss: 0.7451, Val: 0.7211, Test: 0.7183\n",
      "Epoch: 231, Loss: 0.7533, Val: 0.7239, Test: 0.7241\n",
      "Epoch: 232, Loss: 0.7521, Val: 0.7241, Test: 0.7238\n",
      "Epoch: 233, Loss: 0.7499, Val: 0.7227, Test: 0.7202\n"
     ]
    }
   ],
   "source": [
    "best_val_acc_lst, best_val_epoch_lst, best_test_acc_lst, best_test_epoch_lst = [], [], [], []\n",
    "for lr in label_rate_arr:\n",
    "    print(lr)\n",
    "    lost_lst, val_acc_lst, test_acc_lst = optimization_loop(lr)\n",
    "    \n",
    "    best_val_epoch_lst.append(torch.Tensor(val_acc_lst).argmax())\n",
    "    best_val_acc_lst.append(torch.Tensor(val_acc_lst).max())\n",
    "    best_test_epoch_lst.append(torch.Tensor(test_acc_lst).argmax())\n",
    "    best_test_acc_lst.append(torch.Tensor(test_acc_lst).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "marker_size = 3\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].plot(best_val_epoch_lst, \"b.\", markersize=marker_size)\n",
    "axs[0, 0].set_title(\"best val epoch\")\n",
    "\n",
    "\n",
    "axs[0, 1].plot(best_val_acc_lst, \"b.\",markersize=marker_size)\n",
    "axs[0, 1].set_title(\"best val acc\")\n",
    "\n",
    "axs[1, 0].plot(best_test_epoch_lst, \"b.\",markersize=marker_size)\n",
    "axs[1, 0].set_title(\"best test epoch\")\n",
    "\n",
    "axs[1, 1].plot(best_test_acc_lst, \"b.\",markersize=marker_size)\n",
    "axs[1, 1].set_title(\"best test acc\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to plot the ROC curve\n",
    "y-axis: true positive rate = test_acc  \n",
    "x-axis: false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m roc_curve(\u001b[43my_train_5\u001b[49m, y_scores)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_roc_curve\u001b[39m(fpr, tpr, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(fpr, tpr, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39mlabel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_5' is not defined"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') \n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "sklearn.metrics.roc_curve and other functions built on top of it (such as https://pytorch.org/ignite/generated/ignite.contrib.metrics.RocCurve.html) only work for binary classification tasks. How to modify it/this code to use those functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "unimp_arxiv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
